{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the sketch we want to try to implemnt:\n",
    "- For each document we need to compute: semantic embedding using sbert (all-distilroberta-v1), style embedding using luar-mud, genre embedding using xml-roberta\n",
    "- Construct each batch such that:\n",
    "\t1. Each pair of anchor and positive have semantic similarity below threshold-x (0.4)\n",
    "\t2. Each pair of anchor and positive have genre similarity below threshold-z (0.7)\n",
    "\t3. Inside each batch, we have XX% of authors such that their style similarity is higher than threshold-y (0.3)\n",
    "- Ideas on how to construct these batches:\n",
    "  - 20% of the instances in the batch are from condition 1\n",
    "  - 20% of the instances in the batch are from condition 2\n",
    "  - 20% of the instances would follow condition 3\n",
    "  - 30% randomly selected instances\n",
    "  OR\n",
    "  In each batch having few examples that are hard positives in terms of semantic and genre, and few that contain hard negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sadiri_luarsbert_emb_train_path = \"/data/araghavan/HIATUS/datadreamer-ta2/data/ta2_jan_2025_trian_data/train_sadiri_processed_with_luarsbertembeddings_wo_ao3_filtered.jsonl\"\n",
    "df_luarsbert_emb_train = pd.read_json(sadiri_luarsbert_emb_train_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_luarsbert_emb_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Create Anchor-Positive Pairs (Within Author)\n",
    "# -----------------------------\n",
    "df_anchorpos_pairs_raw = df_luarsbert_emb_train.merge(df_luarsbert_emb_train, how=\"outer\", on=\"authorID\", suffixes=[\"_anchor\", \"_positive\"])\n",
    "df_anchorpos_pairs_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Filter out self-pairs and same-genre pairs\n",
    "# -----------------------------\n",
    "df_anchorpos_pairs_prep = df_anchorpos_pairs_raw.loc[\n",
    "    (df_anchorpos_pairs_raw[\"documentID_anchor\"] != df_anchorpos_pairs_raw[\"documentID_positive\"]) &\n",
    "    (df_anchorpos_pairs_raw[\"doc_xrbmtgc_genre_anchor\"] != df_anchorpos_pairs_raw[\"doc_xrbmtgc_genre_positive\"])\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Deduplicate Unordered Pairs per Author\n",
    "# -----------------------------\n",
    "doc_min = np.minimum(df_anchorpos_pairs_prep[\"documentID_anchor\"], df_anchorpos_pairs_prep[\"documentID_positive\"])\n",
    "doc_max = np.maximum(df_anchorpos_pairs_prep[\"documentID_anchor\"], df_anchorpos_pairs_prep[\"documentID_positive\"])\n",
    "df_anchorpos_pairs_prep.loc[:, \"pair_key\"] = (\n",
    "    df_anchorpos_pairs_prep[\"authorID\"].astype(str) + \"__\" +\n",
    "    doc_min.astype(str) + \"__\" +\n",
    "    doc_max.astype(str)\n",
    ")\n",
    "df_anchorpos_pairs_prep = df_anchorpos_pairs_prep.drop_duplicates(subset=\"pair_key\").drop(columns=\"pair_key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Compute Cosine Similarity (Vectorized)\n",
    "# -----------------------------\n",
    "def batched_cosine_similarity(a_embeddings, b_embeddings, batch_size=10000):\n",
    "    scores = []\n",
    "    for i in tqdm(range(0, len(a_embeddings), batch_size), desc=\"Cosine Similarity\"):\n",
    "        a_batch = a_embeddings[i:i+batch_size]\n",
    "        b_batch = b_embeddings[i:i+batch_size]\n",
    "        sim = np.sum(a_batch * b_batch, axis=1) / (\n",
    "            np.linalg.norm(a_batch, axis=1) * np.linalg.norm(b_batch, axis=1)\n",
    "        )\n",
    "        scores.extend(sim)\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_embeddings_luar = np.stack(df_anchorpos_pairs_prep[\"doc_luarmud_embedding_anchor\"].values)\n",
    "positive_embeddings_luar = np.stack(df_anchorpos_pairs_prep[\"doc_luarmud_embedding_positive\"].values)\n",
    "df_anchorpos_pairs_prep[\"luarmud_embedding_similarity_score\"] = batched_cosine_similarity(anchor_embeddings_luar, positive_embeddings_luar, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_embeddings_sbert = np.stack(df_anchorpos_pairs_prep[\"doc_sbertamllv2_embedding_anchor\"].values)\n",
    "positive_embeddings_sbert = np.stack(df_anchorpos_pairs_prep[\"doc_sbertamllv2_embedding_positive\"].values)\n",
    "df_anchorpos_pairs_prep[\"sbertamllv2_embedding_similarity_score\"] = batched_cosine_similarity(anchor_embeddings_sbert, positive_embeddings_sbert, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anchorpos_pairs_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anchorpos_pairs_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anchorpos_pairs_prep.to_json(\"../data/ta2_jan_2025_trian_data/anchor_pos_train_sadiri_luarmudsbertamllv2.jsonl\", orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

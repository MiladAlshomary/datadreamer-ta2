{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## File Paths\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/dev_candidates.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/dev_queries.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/test_candidates.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/test_queries.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/train_candidates.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/train_queries.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/bookcorpus/dev_candidates.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/bookcorpus/dev_queries.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/bookcorpus/test_candidates.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/bookcorpus/test_queries.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/bookcorpus/train_candidates.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/bookcorpus/train_queries.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/gmane/dev_candidates.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/gmane/dev_queries.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/gmane/test_candidates.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/gmane/test_queries.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/gmane/train_candidates.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/gmane/train_queries.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/realnews/dev_candidates.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/realnews/dev_queries.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/realnews/test_candidates.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/realnews/test_queries.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/realnews/train_candidates.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/realnews/train_queries.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/reddit/dev_candidates.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/reddit/dev_queries.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/reddit/test_candidates.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/reddit/test_queries.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/reddit/train_candidates.jsonl\"\n",
    "# \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/reddit/train_queries.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Biber Ready Raw Data\n",
    "### For more details, refer: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/all_data_raw_metadata.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This path contains all the biber ready data that is created from the respective sources with the below transformations:\n",
    "# Refer to /data/araghavan/HIATUS/datadreamer-ta2/src/biber-pipeline.ipynb for the code to create these\n",
    "\n",
    "# Source:\n",
    "# Sadiri - combined \n",
    "#     - (dev + test + train): realnews, gmane\n",
    "#     - (dev + test): ao3 only\n",
    "#     - corpus file: reddit, bookcorpus\n",
    "# blogs - downloaded from kaggle: back\n",
    "\n",
    "# Preprocessing:\n",
    "# 1. Columns Added:\n",
    "#     a. fullText_space_lengthWords\n",
    "#     b. authorID\n",
    "#     [c. author_docs_count: after step 2]\n",
    "\n",
    "# 2. Created two sets of files for each source:\n",
    "#     a. gte70 - atleast 70 words in every document\n",
    "#     b. gte250 - atleast 250 words in every document\n",
    "\n",
    "# *** Note: Since author_docs_count was added AFTER the filtering based on words count\n",
    "# We can observe the same author to have different author_docs_count in gte70 vs gte250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_raw_folder = \"all_data_raw\"\n",
    "all_data_prep_folder = \"all_data_prep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# ## ao3\n",
    "# output_file_json_gte70 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/ao3_gte70.jsonl\"\n",
    "# output_file_json_gte250 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/ao3_gte250.jsonl\"\n",
    "# dev_can_df_ao3 = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/dev_candidates.jsonl\", lines=True)\n",
    "# dev_que_df_ao3 = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/dev_queries.jsonl\", lines=True)\n",
    "# tes_can_df_ao3 = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/test_candidates.jsonl\", lines=True)\n",
    "# tes_que_df_ao3 = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/test_queries.jsonl\", lines=True)\n",
    "# tra_can_df_ao3 = pd.DataFrame() # pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/train_candidates.jsonl\", lines=True)\n",
    "# tra_que_df_ao3 = pd.DataFrame() # pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/train_queries.jsonl\", lines=True)\n",
    "# print(\"ao3\")\n",
    "# print(f\"Dev Can Shape: {dev_can_df_ao3.shape}, Dev Que Shape: {dev_que_df_ao3.shape}, Tes Can Shape: {tes_can_df_ao3.shape}, Tes Que Shape: {tes_que_df_ao3.shape}, Tra Can Shape: {tra_can_df_ao3.shape}, Tra Que Shape: {tra_que_df_ao3.shape}\")\n",
    "\n",
    "# combined_df_ao3 = pd.concat([dev_can_df_ao3, dev_que_df_ao3, tes_can_df_ao3, tes_que_df_ao3, tra_can_df_ao3, tra_que_df_ao3])\n",
    "# print(f\"Combined Shape: {combined_df_ao3.shape}\")\n",
    "\n",
    "# combined_df_ao3[\"authorID\"] = combined_df_ao3[\"authorIDs\"].apply(lambda x: x[0])\n",
    "# combined_df_ao3['fullText_space_lengthWords'] = combined_df_ao3['fullText'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# ## Greater than 70\n",
    "# combined_df_ao3_gte70 = combined_df_ao3[combined_df_ao3['fullText_space_lengthWords'] >= 70].copy()\n",
    "# print(f\"Combined Shape for gte 70: {combined_df_ao3_gte70.shape}\")\n",
    "# combined_df_ao3_gte70['author_docs_count'] = combined_df_ao3_gte70.groupby('authorID')['documentID'].transform('count')\n",
    "# print(f\"Author Count for gte 70: {combined_df_ao3_gte70.authorID.nunique()}\")\n",
    "# combined_df_ao3_gte70.to_json(output_file_json_gte70, orient='records', lines=True, force_ascii=False)\n",
    "# print(f\"Output File for gte 70: {output_file_json_gte70}\")\n",
    "\n",
    "# del combined_df_ao3_gte70\n",
    "\n",
    "# ## Greater than 250\n",
    "# combined_df_ao3_gte250 = combined_df_ao3[combined_df_ao3['fullText_space_lengthWords'] >= 250].copy()\n",
    "# print(f\"Combined Shape for gte 250: {combined_df_ao3_gte250.shape}\")\n",
    "# combined_df_ao3_gte250['author_docs_count'] = combined_df_ao3_gte250.groupby('authorID')['documentID'].transform('count')\n",
    "# print(f\"Author Count for gte 250: {combined_df_ao3_gte250.authorID.nunique()}\")\n",
    "# combined_df_ao3_gte250.to_json(output_file_json_gte250, orient='records', lines=True, force_ascii=False)\n",
    "# print(f\"Output File for gte 250: {output_file_json_gte250}\")\n",
    "\n",
    "# del dev_can_df_ao3, dev_que_df_ao3, tes_can_df_ao3, tes_que_df_ao3, tra_can_df_ao3, tra_que_df_ao3, combined_df_ao3, combined_df_ao3_gte250 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Define output file paths (ensure all_data_raw_folder is defined)\n",
    "# output_file_json_gte70 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/ao3_gte70.jsonl\"\n",
    "# output_file_json_gte250 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/ao3_gte250.jsonl\"\n",
    "\n",
    "# # Read input JSONL files\n",
    "# dev_can_df_ao3 = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/dev_candidates.jsonl\", lines=True)\n",
    "# dev_que_df_ao3 = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/dev_queries.jsonl\", lines=True)\n",
    "# tes_can_df_ao3 = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/test_candidates.jsonl\", lines=True)\n",
    "# tes_que_df_ao3 = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/test_queries.jsonl\", lines=True)\n",
    "# # Use empty DataFrames if training files are not available\n",
    "# tra_can_df_ao3 = pd.DataFrame()\n",
    "# tra_que_df_ao3 = pd.DataFrame()\n",
    "\n",
    "# print(\"ao3\")\n",
    "# print(f\"Dev Can Shape: {dev_can_df_ao3.shape}, \"\n",
    "#       f\"Dev Que Shape: {dev_que_df_ao3.shape}, \"\n",
    "#       f\"Tes Can Shape: {tes_can_df_ao3.shape}, \"\n",
    "#       f\"Tes Que Shape: {tes_que_df_ao3.shape}, \"\n",
    "#       f\"Tra Can Shape: {tra_can_df_ao3.shape}, \"\n",
    "#       f\"Tra Que Shape: {tra_que_df_ao3.shape}\")\n",
    "\n",
    "# # Concatenate all DataFrames (ignore index for a fresh integer index)\n",
    "# combined_df_ao3 = pd.concat([\n",
    "#     dev_can_df_ao3, dev_que_df_ao3,\n",
    "#     tes_can_df_ao3, tes_que_df_ao3,\n",
    "#     tra_can_df_ao3, tra_que_df_ao3\n",
    "# ], ignore_index=True)\n",
    "# print(f\"Combined Shape: {combined_df_ao3.shape}\")\n",
    "\n",
    "# # Create a new column \"authorID\" by taking the first element of the \"authorIDs\" list.\n",
    "# combined_df_ao3[\"authorID\"] = combined_df_ao3[\"authorIDs\"].str[0]\n",
    "\n",
    "# # Compute word count in fullText using vectorized string operations\n",
    "# combined_df_ao3['fullText_space_lengthWords'] = combined_df_ao3['fullText'].str.split().str.len()\n",
    "\n",
    "# ### For texts with at least 70 words ###\n",
    "# mask70 = combined_df_ao3['fullText_space_lengthWords'] >= 70\n",
    "# print(f\"Combined Shape for gte 70: {mask70.sum()} rows\")\n",
    "\n",
    "# # Compute document count per author for rows with at least 70 words\n",
    "# author_counts_70 = combined_df_ao3.loc[mask70].groupby('authorID')['documentID'].count()\n",
    "# # Use .loc to update only the filtered rows without making an extra copy\n",
    "# combined_df_ao3.loc[mask70, 'author_docs_count'] = combined_df_ao3.loc[mask70, 'authorID'].map(author_counts_70)\n",
    "# print(f\"Author Count for gte 70: {combined_df_ao3.loc[mask70, 'authorID'].nunique()}\")\n",
    "\n",
    "# # Save rows with >=70 words to JSONL\n",
    "# combined_df_ao3.loc[mask70].to_json(output_file_json_gte70, orient='records', lines=True, force_ascii=False)\n",
    "# print(f\"Output File for gte 70: {output_file_json_gte70}\")\n",
    "\n",
    "# ### For texts with at least 250 words ###\n",
    "# mask250 = combined_df_ao3['fullText_space_lengthWords'] >= 250\n",
    "# print(f\"Combined Shape for gte 250: {mask250.sum()} rows\")\n",
    "\n",
    "# author_counts_250 = combined_df_ao3.loc[mask250].groupby('authorID')['documentID'].count()\n",
    "# combined_df_ao3.loc[mask250, 'author_docs_count'] = combined_df_ao3.loc[mask250, 'authorID'].map(author_counts_250)\n",
    "# print(f\"Author Count for gte 250: {combined_df_ao3.loc[mask250, 'authorID'].nunique()}\")\n",
    "\n",
    "# combined_df_ao3.loc[mask250].to_json(output_file_json_gte250, orient='records', lines=True, force_ascii=False)\n",
    "# print(f\"Output File for gte 250: {output_file_json_gte250}\")\n",
    "\n",
    "# # Clean up (delete unused DataFrames)\n",
    "# del dev_can_df_ao3, dev_que_df_ao3, tes_can_df_ao3, tes_que_df_ao3, tra_can_df_ao3, tra_que_df_ao3, combined_df_ao3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ao3\n",
      "Dev Can Shape: (3076827, 14), Dev Que Shape: (37327, 14), Tes Can Shape: (6182027, 14), Tes Que Shape: (74653, 14), Tra Can Shape: (0, 0), Tra Que Shape: (0, 0)\n",
      "Combined Shape: (9370834, 14)\n",
      "Combined Shape for gte 70: 9370255 rows\n",
      "Author Count for gte 70: 111977\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Define output file paths (ensure all_data_raw_folder is defined)\n",
    "# output_file_json_gte70 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/ao3_gte70.jsonl\"\n",
    "# output_file_json_gte250 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/ao3_gte250.jsonl\"\n",
    "\n",
    "# # Read input JSONL files\n",
    "# dev_can_df_ao3 = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/dev_candidates.jsonl\", lines=True)\n",
    "# dev_que_df_ao3 = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/dev_queries.jsonl\", lines=True)\n",
    "# tes_can_df_ao3 = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/test_candidates.jsonl\", lines=True)\n",
    "# tes_que_df_ao3 = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/ao3/test_queries.jsonl\", lines=True)\n",
    "# # Use empty DataFrames if training files are not available\n",
    "# tra_can_df_ao3 = pd.DataFrame()\n",
    "# tra_que_df_ao3 = pd.DataFrame()\n",
    "\n",
    "# print(\"ao3\")\n",
    "# print(f\"Dev Can Shape: {dev_can_df_ao3.shape}, \"\n",
    "#       f\"Dev Que Shape: {dev_que_df_ao3.shape}, \"\n",
    "#       f\"Tes Can Shape: {tes_can_df_ao3.shape}, \"\n",
    "#       f\"Tes Que Shape: {tes_que_df_ao3.shape}, \"\n",
    "#       f\"Tra Can Shape: {tra_can_df_ao3.shape}, \"\n",
    "#       f\"Tra Que Shape: {tra_que_df_ao3.shape}\")\n",
    "\n",
    "# # Concatenate all DataFrames (ignore index for a fresh integer index)\n",
    "# combined_df_ao3 = pd.concat([\n",
    "#     dev_can_df_ao3, dev_que_df_ao3,\n",
    "#     tes_can_df_ao3, tes_que_df_ao3,\n",
    "#     tra_can_df_ao3, tra_que_df_ao3\n",
    "# ], ignore_index=True)\n",
    "# print(f\"Combined Shape: {combined_df_ao3.shape}\")\n",
    "\n",
    "# # Create a new column \"authorID\" by taking the first element of the \"authorIDs\" list.\n",
    "# combined_df_ao3[\"authorID\"] = combined_df_ao3[\"authorIDs\"].str[0]\n",
    "\n",
    "# # Compute word count in fullText using vectorized string operations\n",
    "# combined_df_ao3['fullText_space_lengthWords'] = combined_df_ao3['fullText'].str.split().str.len()\n",
    "\n",
    "# ########################\n",
    "# # For texts with ≥70 words\n",
    "# ########################\n",
    "# df_gte70 = combined_df_ao3[combined_df_ao3['fullText_space_lengthWords'] >= 70]\n",
    "# print(f\"Combined Shape for gte 70: {df_gte70.shape[0]} rows\")\n",
    "\n",
    "# # Compute document counts per author in the filtered subset\n",
    "# author_counts_70 = df_gte70.groupby('authorID', as_index=False)['documentID'].count()\n",
    "# author_counts_70.rename(columns={'documentID': 'author_docs_count'}, inplace=True)\n",
    "\n",
    "# # Merge the counts back with the filtered DataFrame\n",
    "# df_gte70 = df_gte70.merge(author_counts_70, on='authorID', how='left')\n",
    "# print(f\"Author Count for gte 70: {df_gte70['authorID'].nunique()}\")\n",
    "\n",
    "# # Save to JSONL\n",
    "# df_gte70.to_json(output_file_json_gte70, orient='records', lines=True, force_ascii=False)\n",
    "# print(f\"Output File for gte 70: {output_file_json_gte70}\")\n",
    "\n",
    "# ########################\n",
    "# # For texts with ≥250 words\n",
    "# ########################\n",
    "# df_gte250 = combined_df_ao3[combined_df_ao3['fullText_space_lengthWords'] >= 250]\n",
    "# print(f\"Combined Shape for gte 250: {df_gte250.shape[0]} rows\")\n",
    "\n",
    "# author_counts_250 = df_gte250.groupby('authorID', as_index=False)['documentID'].count()\n",
    "# author_counts_250.rename(columns={'documentID': 'author_docs_count'}, inplace=True)\n",
    "\n",
    "# df_gte250 = df_gte250.merge(author_counts_250, on='authorID', how='left')\n",
    "# print(f\"Author Count for gte 250: {df_gte250['authorID'].nunique()}\")\n",
    "\n",
    "# df_gte250.to_json(output_file_json_gte250, orient='records', lines=True, force_ascii=False)\n",
    "# print(f\"Output File for gte 250: {output_file_json_gte250}\")\n",
    "\n",
    "# # Clean up (delete unused DataFrames)\n",
    "# del dev_can_df_ao3, dev_que_df_ao3, tes_can_df_ao3, tes_que_df_ao3, tra_can_df_ao3, tra_que_df_ao3, combined_df_ao3, df_gte70, df_gte250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# ## bookcorpus\n",
    "# output_file_json_gte70 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/bookcorpus_gte70.jsonl\"\n",
    "# output_file_json_gte250 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/bookcorpus_gte250.jsonl\"\n",
    "# dev_can_df_bookcorpus = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/bookcorpus/dev_candidates.jsonl\", lines=True)\n",
    "# dev_que_df_bookcorpus = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/bookcorpus/dev_queries.jsonl\", lines=True)\n",
    "# tes_can_df_bookcorpus = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/bookcorpus/test_candidates.jsonl\", lines=True)\n",
    "# tes_que_df_bookcorpus = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/bookcorpus/test_queries.jsonl\", lines=True)\n",
    "# tra_can_df_bookcorpus = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/bookcorpus/train_candidates.jsonl\", lines=True)\n",
    "# tra_que_df_bookcorpus = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/bookcorpus/train_queries.jsonl\", lines=True)\n",
    "# print(\"bookcorpus\")\n",
    "# print(f\"Dev Can Shape: {dev_can_df_bookcorpus.shape}, Dev Que Shape: {dev_que_df_bookcorpus.shape}, Tes Can Shape: {tes_can_df_bookcorpus.shape}, Tes Que Shape: {tes_que_df_bookcorpus.shape}, Tra Can Shape: {tra_can_df_bookcorpus.shape}, Tra Que Shape: {tra_que_df_bookcorpus.shape}\")\n",
    "\n",
    "# combined_df_bookcorpus = pd.concat([dev_can_df_bookcorpus, dev_que_df_bookcorpus, tes_can_df_bookcorpus, tes_que_df_bookcorpus, tra_can_df_bookcorpus, tra_que_df_bookcorpus])\n",
    "# print(f\"Combined Shape: {combined_df_bookcorpus.shape}\")\n",
    "\n",
    "# combined_df_bookcorpus[\"authorID\"] = combined_df_bookcorpus[\"authorIDs\"].apply(lambda x: x[0])\n",
    "# combined_df_bookcorpus['fullText_space_lengthWords'] = combined_df_bookcorpus['fullText'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# combined_df_bookcorpus_gte70 = combined_df_bookcorpus[combined_df_bookcorpus['fullText_space_lengthWords'] >= 70]\n",
    "# combined_df_bookcorpus_gte250 = combined_df_bookcorpus[combined_df_bookcorpus['fullText_space_lengthWords'] >= 250]\n",
    "# ## Greater than 70\n",
    "# print(f\"Combined Shape for gte 70: {combined_df_bookcorpus_gte70.shape}\")\n",
    "# author_count_bookcorpus_gte70 = combined_df_bookcorpus_gte70.groupby('authorID').size().reset_index(name='author_docs_count')\n",
    "# print(f\"Author Count Shape for gte 70: {author_count_bookcorpus_gte70.shape}\")\n",
    "# combined_df_bookcorpus_gte70 = pd.merge(combined_df_bookcorpus_gte70, author_count_bookcorpus_gte70, on='authorID')\n",
    "# print(\"Meged Author Docs Count for gte 70\")\n",
    "# combined_df_bookcorpus_gte70.to_json(output_file_json_gte70, orient='records', lines=True, ensure_ascii=False)\n",
    "# print(f\"Output File for gte 70: {output_file_json_gte70}\")\n",
    "\n",
    "# ## Greater than 250\n",
    "# print(f\"Combined Shape for gte 250: {combined_df_bookcorpus_gte250.shape}\")\n",
    "# author_count_bookcorpus_gte250 = combined_df_bookcorpus_gte250.groupby('authorID').size().reset_index(name='author_docs_count')\n",
    "# print(f\"Author Count Shape for gte 250: {author_count_bookcorpus_gte250.shape}\")\n",
    "# combined_df_bookcorpus_gte250 = pd.merge(combined_df_bookcorpus_gte250, author_count_bookcorpus_gte250, on='authorID')\n",
    "# print(\"Meged Author Docs Count for gte 250\")\n",
    "# combined_df_bookcorpus_gte250.to_json(output_file_json_gte250, orient='records', lines=True, ensure_ascii=False)\n",
    "# print(f\"Output File for gte 250: {output_file_json_gte250}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gmane\n",
      "Dev Can Shape: (33538, 14), Dev Que Shape: (33538, 14), Tes Can Shape: (65102, 14), Tes Que Shape: (65103, 14), Tra Can Shape: (558926, 14), Tra Que Shape: (558918, 14)\n",
      "Combined Shape: (1315125, 14)\n",
      "Combined Shape for gte 70: (1038808, 16)\n",
      "Author Count Shape for gte 70: 617087\n",
      "Output File for gte 70: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/gmane_gte70.jsonl\n",
      "Combined Shape for gte 250: (92482, 16)\n",
      "Author Count Shape for gte 250: 82342\n",
      "Output File for gte 250: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/gmane_gte250.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "## gmane\n",
    "output_file_json_gte70 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/gmane_gte70.jsonl\"\n",
    "output_file_json_gte250 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/gmane_gte250.jsonl\"\n",
    "dev_can_df_gmane = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/gmane/dev_candidates.jsonl\", lines=True)\n",
    "dev_que_df_gmane = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/gmane/dev_queries.jsonl\", lines=True)\n",
    "tes_can_df_gmane = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/gmane/test_candidates.jsonl\", lines=True)\n",
    "tes_que_df_gmane = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/gmane/test_queries.jsonl\", lines=True)\n",
    "tra_can_df_gmane = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/gmane/train_candidates.jsonl\", lines=True)\n",
    "tra_que_df_gmane = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/gmane/train_queries.jsonl\", lines=True)\n",
    "print(\"gmane\")\n",
    "print(f\"Dev Can Shape: {dev_can_df_gmane.shape}, Dev Que Shape: {dev_que_df_gmane.shape}, Tes Can Shape: {tes_can_df_gmane.shape}, Tes Que Shape: {tes_que_df_gmane.shape}, Tra Can Shape: {tra_can_df_gmane.shape}, Tra Que Shape: {tra_que_df_gmane.shape}\")\n",
    "\n",
    "combined_df_gmane = pd.concat([dev_can_df_gmane, dev_que_df_gmane, tes_can_df_gmane, tes_que_df_gmane, tra_can_df_gmane, tra_que_df_gmane])\n",
    "print(f\"Combined Shape: {combined_df_gmane.shape}\")\n",
    "\n",
    "combined_df_gmane[\"authorID\"] = combined_df_gmane[\"authorIDs\"].apply(lambda x: x[0])\n",
    "combined_df_gmane['fullText_space_lengthWords'] = combined_df_gmane['fullText'].apply(lambda x: len(x.split()))\n",
    "\n",
    "## Greater than 70\n",
    "combined_df_gmane_gte70 = combined_df_gmane[combined_df_gmane['fullText_space_lengthWords'] >= 70].copy()\n",
    "print(f\"Combined Shape for gte 70: {combined_df_gmane_gte70.shape}\")\n",
    "combined_df_gmane_gte70['author_docs_count'] = combined_df_gmane_gte70.groupby('authorID')['documentID'].transform('count')\n",
    "print(f\"Author Count Shape for gte 70: {combined_df_gmane_gte70.authorID.nunique()}\")\n",
    "combined_df_gmane_gte70.to_json(output_file_json_gte70, orient='records', lines=True, force_ascii=False)\n",
    "print(f\"Output File for gte 70: {output_file_json_gte70}\")\n",
    "\n",
    "## Greater than 250\n",
    "combined_df_gmane_gte250 = combined_df_gmane[combined_df_gmane['fullText_space_lengthWords'] >= 250].copy()\n",
    "print(f\"Combined Shape for gte 250: {combined_df_gmane_gte250.shape}\")\n",
    "combined_df_gmane_gte250['author_docs_count'] = combined_df_gmane_gte250.groupby('authorID')['documentID'].transform('count')\n",
    "print(f\"Author Count Shape for gte 250: {combined_df_gmane_gte250.authorID.nunique()}\")\n",
    "combined_df_gmane_gte250.to_json(output_file_json_gte250, orient='records', lines=True, force_ascii=False)\n",
    "print(f\"Output File for gte 250: {output_file_json_gte250}\")\n",
    "\n",
    "del dev_can_df_gmane, dev_que_df_gmane, tes_can_df_gmane, tes_que_df_gmane, tra_can_df_gmane, tra_que_df_gmane, combined_df_gmane, combined_df_gmane_gte70, combined_df_gmane_gte250 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realnews\n",
      "Dev Can Shape: (10861, 14), Dev Que Shape: (10861, 14), Tes Can Shape: (21085, 14), Tes Que Shape: (21085, 14), Tra Can Shape: (181024, 14), Tra Que Shape: (181024, 14)\n",
      "Combined Shape: (425940, 14)\n",
      "Combined Shape for gte 70: (425938, 16)\n",
      "Author Count for gte 70: 212969\n",
      "Output File for gte 70: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/realnews_gte70.jsonl\n",
      "Combined Shape for gte 250: (385191, 16)\n",
      "Author Count for gte 250: 205640\n",
      "Output File for gte 250: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/realnews_gte250.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "## realnews\n",
    "output_file_json_gte70 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/realnews_gte70.jsonl\"\n",
    "output_file_json_gte250 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/realnews_gte250.jsonl\"\n",
    "dev_can_df_realnews = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/realnews/dev_candidates.jsonl\", lines=True)\n",
    "dev_que_df_realnews = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/realnews/dev_queries.jsonl\", lines=True)\n",
    "tes_can_df_realnews = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/realnews/test_candidates.jsonl\", lines=True)\n",
    "tes_que_df_realnews = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/realnews/test_queries.jsonl\", lines=True)\n",
    "tra_can_df_realnews = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/realnews/train_candidates.jsonl\", lines=True)\n",
    "tra_que_df_realnews = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/realnews/train_queries.jsonl\", lines=True)\n",
    "print(\"realnews\")\n",
    "print(f\"Dev Can Shape: {dev_can_df_realnews.shape}, Dev Que Shape: {dev_que_df_realnews.shape}, Tes Can Shape: {tes_can_df_realnews.shape}, Tes Que Shape: {tes_que_df_realnews.shape}, Tra Can Shape: {tra_can_df_realnews.shape}, Tra Que Shape: {tra_que_df_realnews.shape}\")\n",
    "\n",
    "combined_df_realnews = pd.concat([dev_can_df_realnews, dev_que_df_realnews, tes_can_df_realnews, tes_que_df_realnews, tra_can_df_realnews, tra_que_df_realnews])\n",
    "print(f\"Combined Shape: {combined_df_realnews.shape}\")\n",
    "\n",
    "combined_df_realnews[\"authorID\"] = combined_df_realnews[\"authorIDs\"].apply(lambda x: x[0])\n",
    "combined_df_realnews['fullText_space_lengthWords'] = combined_df_realnews['fullText'].apply(lambda x: len(x.split()))\n",
    "\n",
    "## Greater than 70\n",
    "combined_df_realnews_gte70 = combined_df_realnews[combined_df_realnews['fullText_space_lengthWords'] >= 70].copy()\n",
    "print(f\"Combined Shape for gte 70: {combined_df_realnews_gte70.shape}\")\n",
    "combined_df_realnews_gte70['author_docs_count'] = combined_df_realnews_gte70.groupby('authorID')['documentID'].transform('count')\n",
    "print(f\"Author Count for gte 70: {combined_df_realnews_gte70.authorID.nunique()}\")\n",
    "combined_df_realnews_gte70.to_json(output_file_json_gte70, orient='records', lines=True, force_ascii=False)\n",
    "print(f\"Output File for gte 70: {output_file_json_gte70}\")\n",
    "\n",
    "## Greater than 250\n",
    "combined_df_realnews_gte250 = combined_df_realnews[combined_df_realnews['fullText_space_lengthWords'] >= 250].copy()\n",
    "print(f\"Combined Shape for gte 250: {combined_df_realnews_gte250.shape}\")\n",
    "combined_df_realnews_gte250['author_docs_count'] = combined_df_realnews_gte250.groupby('authorID')['documentID'].transform('count')\n",
    "print(f\"Author Count for gte 250: {combined_df_realnews_gte250.authorID.nunique()}\")\n",
    "combined_df_realnews_gte250.to_json(output_file_json_gte250, orient='records', lines=True, force_ascii=False)\n",
    "print(f\"Output File for gte 250: {output_file_json_gte250}\")\n",
    "\n",
    "del dev_can_df_realnews, dev_que_df_realnews, tes_can_df_realnews, tes_que_df_realnews, tra_can_df_realnews, tra_que_df_realnews, combined_df_realnews, combined_df_realnews_gte70, combined_df_realnews_gte250 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# ## reddit\n",
    "# output_file_json_gte70 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/reddit_gte70.jsonl\"\n",
    "# output_file_json_gte250 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/reddit_gte250.jsonl\"\n",
    "# dev_can_df_reddit = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/reddit/dev_candidates.jsonl\", lines=True)\n",
    "# dev_que_df_reddit = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/reddit/dev_queries.jsonl\", lines=True)\n",
    "# tes_can_df_reddit = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/reddit/test_candidates.jsonl\", lines=True)\n",
    "# tes_que_df_reddit = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/reddit/test_queries.jsonl\", lines=True)\n",
    "# tra_can_df_reddit = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/reddit/train_candidates.jsonl\", lines=True)\n",
    "# tra_que_df_reddit = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/reddit/train_queries.jsonl\", lines=True)\n",
    "# print(\"reddit\")\n",
    "# print(f\"Dev Can Shape: {dev_can_df_reddit.shape}, Dev Que Shape: {dev_que_df_reddit.shape}, Tes Can Shape: {tes_can_df_reddit.shape}, Tes Que Shape: {tes_que_df_reddit.shape}, Tra Can Shape: {tra_can_df_reddit.shape}, Tra Que Shape: {tra_que_df_reddit.shape}\")\n",
    "\n",
    "# combined_df_reddit = pd.concat([dev_can_df_reddit, dev_que_df_reddit, tes_can_df_reddit, tes_que_df_reddit, tra_can_df_reddit, tra_que_df_reddit])\n",
    "# print(f\"Combined Shape: {combined_df_reddit.shape}\")\n",
    "\n",
    "# combined_df_reddit[\"authorID\"] = combined_df_reddit[\"authorIDs\"].apply(lambda x: x[0])\n",
    "# combined_df_reddit['fullText_space_lengthWords'] = combined_df_reddit['fullText'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# combined_df_reddit_gte70 = combined_df_reddit[combined_df_reddit['fullText_space_lengthWords'] >= 70]\n",
    "# combined_df_reddit_gte250 = combined_df_reddit[combined_df_reddit['fullText_space_lengthWords'] >= 250]\n",
    "# ## Greater than 70\n",
    "# print(f\"Combined Shape for gte 70: {combined_df_reddit_gte70.shape}\")\n",
    "# author_count_reddit_gte70 = combined_df_reddit_gte70.groupby('authorID').size().reset_index(name='author_docs_count')\n",
    "# print(f\"Author Count Shape for gte 70: {author_count_reddit_gte70.shape}\")\n",
    "# combined_df_reddit_gte70 = pd.merge(combined_df_reddit_gte70, author_count_reddit_gte70, on='authorID')\n",
    "# print(\"Meged Author Docs Count for gte 70\")\n",
    "# combined_df_reddit_gte70.to_json(output_file_json_gte70, orient='records', lines=True, ensure_ascii=False)\n",
    "# print(f\"Output File for gte 70: {output_file_json_gte70}\")\n",
    "\n",
    "# ## Greater than 250\n",
    "# print(f\"Combined Shape for gte 250: {combined_df_reddit_gte250.shape}\")\n",
    "# author_count_reddit_gte250 = combined_df_reddit_gte250.groupby('authorID').size().reset_index(name='author_docs_count')\n",
    "# print(f\"Author Count Shape for gte 250: {author_count_reddit_gte250.shape}\")\n",
    "# combined_df_reddit_gte250 = pd.merge(combined_df_reddit_gte250, author_count_reddit_gte250, on='authorID')\n",
    "# print(\"Meged Author Docs Count for gte 250\")\n",
    "# combined_df_reddit_gte250.to_json(output_file_json_gte250, orient='records', lines=True, ensure_ascii=False)\n",
    "# print(f\"Output File for gte 250: {output_file_json_gte250}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            Student       Leo   14,May,2004   \n",
       "1  2059027   male   15            Student       Leo   13,May,2004   \n",
       "2  2059027   male   15            Student       Leo   12,May,2004   \n",
       "3  2059027   male   15            Student       Leo   12,May,2004   \n",
       "4  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "\n",
       "                                                text  \n",
       "0             Info has been found (+/- 100 pages,...  \n",
       "1             These are the team members:   Drewe...  \n",
       "2             In het kader van kernfusie op aarde...  \n",
       "3                   testing!!!  testing!!!            \n",
       "4               Thanks to Yahoo!'s Toolbar I can ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(681284, 7)\n",
      "Combined Shape for gte 70: (425612, 9)\n",
      "Author Count Shape for gte 70: 19259\n",
      "Output File for gte 70: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/back_gte70.jsonl\n",
      "Combined Shape for gte 250: (174977, 9)\n",
      "Author Count Shape for gte 70: 16528\n",
      "Output File for gte 250: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/back_gte250.jsonl\n"
     ]
    }
   ],
   "source": [
    "## Blog Authorship Corpus Pipeline - https://www.kaggle.com/datasets/rtatman/blog-authorship-corpus/data\n",
    "import pandas as pd\n",
    "import uuid\n",
    "\n",
    "blog_authorship_corpus_kaggle_file = \"/data/araghavan/HIATUS/datadreamer-ta2/data/blog_authorship_corpus_kaggle/blogtext_kaggle.csv\"\n",
    "back_df = pd.read_csv(blog_authorship_corpus_kaggle_file)\n",
    "display(back_df.head())\n",
    "print(back_df.shape)\n",
    "\n",
    "output_file_json_gte70 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/back_gte70.jsonl\"\n",
    "output_file_json_gte250 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/back_gte250.jsonl\"\n",
    "\n",
    "back_df.rename(columns={\"id\": \"authorID\", \"text\":\"fullText\"}, inplace=True)\n",
    "back_df['fullText_space_lengthWords'] = back_df['fullText'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Add a unique documentID column using uuid4\n",
    "back_df['documentID'] = [str(uuid.uuid4()) for _ in range(len(back_df))]\n",
    "\n",
    "## Greater than 70\n",
    "back_df_gte70 = back_df[back_df['fullText_space_lengthWords'] >= 70].copy()\n",
    "print(f\"Combined Shape for gte 70: {back_df_gte70.shape}\")\n",
    "back_df_gte70['author_docs_count'] = back_df_gte70.groupby('authorID')['documentID'].transform('count')\n",
    "print(f\"Author Count Shape for gte 70: {back_df_gte70.authorID.nunique()}\")\n",
    "back_df_gte70.to_json(output_file_json_gte70, orient='records', lines=True, force_ascii=False)\n",
    "print(f\"Output File for gte 70: {output_file_json_gte70}\")\n",
    "\n",
    "## Greater than 250\n",
    "back_df_gte250 = back_df[back_df['fullText_space_lengthWords'] >= 250].copy()\n",
    "print(f\"Combined Shape for gte 250: {back_df_gte250.shape}\")\n",
    "back_df_gte250['author_docs_count'] = back_df_gte250.groupby('authorID')['documentID'].transform('count')\n",
    "print(f\"Author Count Shape for gte 70: {back_df_gte250.authorID.nunique()}\")\n",
    "back_df_gte250.to_json(output_file_json_gte250, orient='records', lines=True, force_ascii=False)\n",
    "print(f\"Output File for gte 250: {output_file_json_gte250}\")\n",
    "\n",
    "del back_df, back_df_gte250, back_df_gte70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit_corpus\n",
      "reddit_corpus Shape:(12290058, 17)\n",
      "Combined Shape for gte 70: (7552986, 17)\n",
      "Author Count Shape for gte 70: 2417978\n",
      "Output File for gte 70: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/reddit_corpus_gte70.jsonl\n",
      "Combined Shape for gte 250: (712381, 17)\n",
      "Author Count Shape for gte 250: 570267\n",
      "Output File for gte 250: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/reddit_corpus_gte250.jsonl\n"
     ]
    }
   ],
   "source": [
    "## Reddit read corpus.jsonl instead of the dev, test, train que & docs\n",
    "import pandas as pd\n",
    "print(\"reddit_corpus\")\n",
    "reddit_corpus_file = \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/reddit/corpus.jsonl\"\n",
    "output_file_json_gte70 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/reddit_corpus_gte70.jsonl\"\n",
    "output_file_json_gte250 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/reddit_corpus_gte250.jsonl\"\n",
    "\n",
    "redcor_df = pd.read_json(reddit_corpus_file, lines=True)\n",
    "redcor_df['authorID'] = redcor_df['authorIDs'].apply(lambda x: x[0])\n",
    "redcor_df['fullText_space_lengthWords'] = redcor_df['fullText'].apply(lambda x: len(x.split()))\n",
    "print(f\"reddit_corpus Shape:{redcor_df.shape}\")\n",
    "\n",
    "## Greater than 70\n",
    "redcor_df_gte70 = redcor_df[redcor_df['fullText_space_lengthWords'] >= 70].copy()\n",
    "print(f\"Combined Shape for gte 70: {redcor_df_gte70.shape}\")\n",
    "redcor_df_gte70['author_docs_count'] = redcor_df_gte70.groupby('authorID')['documentID'].transform('count')\n",
    "print(f\"Author Count Shape for gte 70: {redcor_df_gte70.authorID.nunique()}\")\n",
    "redcor_df_gte70.to_json(output_file_json_gte70, orient='records', lines=True, force_ascii=False)\n",
    "print(f\"Output File for gte 70: {output_file_json_gte70}\")\n",
    "\n",
    "## Greater than 250\n",
    "redcor_df_gte250 = redcor_df[redcor_df['fullText_space_lengthWords'] >= 250].copy()\n",
    "print(f\"Combined Shape for gte 250: {redcor_df_gte250.shape}\")\n",
    "redcor_df_gte250['author_docs_count'] = redcor_df_gte250.groupby('authorID')['documentID'].transform('count')\n",
    "print(f\"Author Count Shape for gte 250: {redcor_df_gte250.authorID.nunique()}\")\n",
    "redcor_df_gte250.to_json(output_file_json_gte250, orient='records', lines=True, force_ascii=False)\n",
    "print(f\"Output File for gte 250: {output_file_json_gte250}\")\n",
    "\n",
    "del redcor_df, redcor_df_gte70, redcor_df_gte250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bookcorpus_corpus\n",
      "bookcorpus_corpus Shape: (1276515, 17)\n",
      "Combined Shape for gte 70: (1276444, 17)\n",
      "Author Count Shape for gte 70: 66385\n",
      "Output File for gte 70: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/bookcorpus_corpus_gte70.jsonl\n",
      "Combined Shape for gte 250: (1275157, 17)\n",
      "Author Count Shape for gte 250: 66383\n",
      "Output File for gte 250: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/bookcorpus_corpus_gte250.jsonl\n"
     ]
    }
   ],
   "source": [
    "## bookcorpus read corpus.jsonl instead of the dev, test, train que & docs\n",
    "import pandas as pd\n",
    "print(\"bookcorpus_corpus\")\n",
    "bookcorpus_corpus_file = \"/data/araghavan/HIATUS/datadreamer-ta2/data/sadiri/bookcorpus/corpus.jsonl\"\n",
    "output_file_json_gte70 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/bookcorpus_corpus_gte70.jsonl\"\n",
    "output_file_json_gte250 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/bookcorpus_corpus_gte250.jsonl\"\n",
    "\n",
    "bookcorpus_corpus_df = pd.read_json(bookcorpus_corpus_file, lines=True)\n",
    "bookcorpus_corpus_df['authorID'] = bookcorpus_corpus_df['authorIDs'].apply(lambda x: x[0])\n",
    "bookcorpus_corpus_df['fullText_space_lengthWords'] = bookcorpus_corpus_df['fullText'].apply(lambda x: len(x.split()))\n",
    "print(f\"bookcorpus_corpus Shape: {bookcorpus_corpus_df.shape}\")\n",
    "\n",
    "## Greater than 70\n",
    "bookcorpus_corpus_df_gte70 = bookcorpus_corpus_df[bookcorpus_corpus_df['fullText_space_lengthWords'] >= 70].copy()\n",
    "print(f\"Combined Shape for gte 70: {bookcorpus_corpus_df_gte70.shape}\")\n",
    "bookcorpus_corpus_df_gte70['author_docs_count'] = bookcorpus_corpus_df_gte70.groupby('authorID')['documentID'].transform('count')\n",
    "print(f\"Author Count Shape for gte 70: {bookcorpus_corpus_df_gte70.authorID.nunique()}\")\n",
    "bookcorpus_corpus_df_gte70.to_json(output_file_json_gte70, orient='records', lines=True, force_ascii=False)\n",
    "print(f\"Output File for gte 70: {output_file_json_gte70}\")\n",
    "\n",
    "## Greater than 250\n",
    "bookcorpus_corpus_df_gte250 = bookcorpus_corpus_df[bookcorpus_corpus_df['fullText_space_lengthWords'] >= 250].copy()\n",
    "print(f\"Combined Shape for gte 250: {bookcorpus_corpus_df_gte250.shape}\")\n",
    "bookcorpus_corpus_df_gte250['author_docs_count'] = bookcorpus_corpus_df_gte250.groupby('authorID')['documentID'].transform('count')\n",
    "print(f\"Author Count Shape for gte 250: {bookcorpus_corpus_df_gte250.authorID.nunique()}\")\n",
    "bookcorpus_corpus_df_gte250.to_json(output_file_json_gte250, orient='records', lines=True, force_ascii=False)\n",
    "print(f\"Output File for gte 250: {output_file_json_gte250}\")\n",
    "\n",
    "del bookcorpus_corpus_df , bookcorpus_corpus_df_gte70, bookcorpus_corpus_df_gte250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Refer to biber-pipeline-stackexchange for stackexchange\n",
    "# import pandas as pd\n",
    "# stex_input_file = \"/data/araghavan/HIATUS/datadreamer-ta2/data/stackexchange/stackexchange_final_data.csv\"\n",
    "# output_file_json_gte70 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/stackexchange_gte70.jsonl\"\n",
    "# output_file_json_gte250 = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_raw_folder}/stackexchange_gte250.jsonl\"\n",
    "\n",
    "# dfse = pd.read_csv(stex_input_file, low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Null Count\n",
      "content_id                      0\n",
      "question_id              31045292\n",
      "parent_question_id       10470113\n",
      "no_of_answers            31045292\n",
      "answer_id                10470113\n",
      "user_id                         0\n",
      "user_id_domain                  0\n",
      "user_name                10471821\n",
      "text                         5142\n",
      "answer_author_profile    10470113\n",
      "answer_pm_score          10470113\n",
      "answer_selected          10470113\n",
      "question_date                   0\n",
      "metadata                        0\n",
      "domain                          0\n",
      "answer_domain            10470113\n"
     ]
    }
   ],
   "source": [
    "# print(dfse.isna().sum().to_frame(name=\"Null Count\").to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Questions did not have author info, they could be retrieved from the user_name from the answers they provided within that domain\n",
    "# user_id_to_name = dfse[['user_id_domain', 'user_name']].dropna().drop_duplicates()\n",
    "# user_id_to_name_dict = dict(zip(user_id_to_name['user_id_domain'], user_id_to_name['user_name']))\n",
    "\n",
    "# # Step 2: Fill missing user_name using this mapping\n",
    "# dfse['user_name_'] = dfse.apply(\n",
    "#     lambda row: user_id_to_name_dict.get(row['user_id_domain'], row['user_name']) if pd.isna(row['user_name']) else row['user_name'],\n",
    "#     axis=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>parent_question_id</th>\n",
       "      <th>no_of_answers</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_id_domain</th>\n",
       "      <th>user_name</th>\n",
       "      <th>text</th>\n",
       "      <th>answer_author_profile</th>\n",
       "      <th>answer_pm_score</th>\n",
       "      <th>answer_selected</th>\n",
       "      <th>question_date</th>\n",
       "      <th>metadata</th>\n",
       "      <th>domain</th>\n",
       "      <th>answer_domain</th>\n",
       "      <th>user_name_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>30_3dprinting.meta.stackexchange</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I have been wanting to learn about 3D printing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-12 00:00:00</td>\n",
       "      <td>['https://3dprinting.meta.stackexchange.com/qu...</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Salmorejo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>30</td>\n",
       "      <td>30_3dprinting.meta.stackexchange</td>\n",
       "      <td>Salmorejo</td>\n",
       "      <td>I would propose something as [business] or [co...</td>\n",
       "      <td>https://3dprinting.meta.stackexchange.com/user...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2016-01-13 00:00:00</td>\n",
       "      <td>['https://3dprinting.meta.stackexchange.com/qu...</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>Salmorejo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    content_id  question_id  parent_question_id  no_of_answers  answer_id  \\\n",
       "0            1          1.0                 NaN            3.0        NaN   \n",
       "49          44          NaN                37.0            NaN       44.0   \n",
       "\n",
       "    user_id                    user_id_domain  user_name  \\\n",
       "0        30  30_3dprinting.meta.stackexchange        NaN   \n",
       "49       30  30_3dprinting.meta.stackexchange  Salmorejo   \n",
       "\n",
       "                                                 text  \\\n",
       "0   I have been wanting to learn about 3D printing...   \n",
       "49  I would propose something as [business] or [co...   \n",
       "\n",
       "                                answer_author_profile  answer_pm_score  \\\n",
       "0                                                 NaN              NaN   \n",
       "49  https://3dprinting.meta.stackexchange.com/user...             -1.0   \n",
       "\n",
       "   answer_selected        question_date  \\\n",
       "0              NaN  2016-01-12 00:00:00   \n",
       "49           False  2016-01-13 00:00:00   \n",
       "\n",
       "                                             metadata  \\\n",
       "0   ['https://3dprinting.meta.stackexchange.com/qu...   \n",
       "49  ['https://3dprinting.meta.stackexchange.com/qu...   \n",
       "\n",
       "                           domain                  answer_domain user_name_  \n",
       "0   3dprinting.meta.stackexchange                            NaN  Salmorejo  \n",
       "49  3dprinting.meta.stackexchange  3dprinting.meta.stackexchange  Salmorejo  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfse[dfse['user_id_domain'] == \"30_3dprinting.meta.stackexchange\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Null Count\n",
      "content_id                      0\n",
      "question_id              31045292\n",
      "parent_question_id       10470113\n",
      "no_of_answers            31045292\n",
      "answer_id                10470113\n",
      "user_id                         0\n",
      "user_id_domain                  0\n",
      "user_name                10471821\n",
      "text                         5142\n",
      "answer_author_profile    10470113\n",
      "answer_pm_score          10470113\n",
      "answer_selected          10470113\n",
      "question_date                   0\n",
      "metadata                        0\n",
      "domain                          0\n",
      "answer_domain            10470113\n",
      "user_name_                3987141\n"
     ]
    }
   ],
   "source": [
    "# print(dfse.isna().sum().to_frame(name=\"Null Count\").to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41515405, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>parent_question_id</th>\n",
       "      <th>no_of_answers</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_id_domain</th>\n",
       "      <th>user_name</th>\n",
       "      <th>fullText</th>\n",
       "      <th>answer_author_profile</th>\n",
       "      <th>answer_pm_score</th>\n",
       "      <th>answer_selected</th>\n",
       "      <th>question_date</th>\n",
       "      <th>metadata</th>\n",
       "      <th>domain</th>\n",
       "      <th>answer_domain</th>\n",
       "      <th>user_name_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>30_3dprinting.meta.stackexchange</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I have been wanting to learn about 3D printing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-12 00:00:00</td>\n",
       "      <td>['https://3dprinting.meta.stackexchange.com/qu...</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Salmorejo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>43</td>\n",
       "      <td>43_3dprinting.meta.stackexchange</td>\n",
       "      <td>Eric Johnson</td>\n",
       "      <td>I would suggest doing a bit of basic research ...</td>\n",
       "      <td>https://3dprinting.meta.stackexchange.com/user...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2016-01-12 00:00:00</td>\n",
       "      <td>['https://3dprinting.meta.stackexchange.com/qu...</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>Eric Johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20</td>\n",
       "      <td>20_3dprinting.meta.stackexchange</td>\n",
       "      <td>kenorb</td>\n",
       "      <td>That's the goal of the site, learn, research a...</td>\n",
       "      <td>https://3dprinting.meta.stackexchange.com/user...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2016-01-12 00:00:00</td>\n",
       "      <td>['https://3dprinting.meta.stackexchange.com/qu...</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>kenorb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.0</td>\n",
       "      <td>138</td>\n",
       "      <td>138_3dprinting.meta.stackexchange</td>\n",
       "      <td>Zizouz212</td>\n",
       "      <td>Vote!Private Betas love, love,lovevotes. Witho...</td>\n",
       "      <td>https://3dprinting.meta.stackexchange.com/user...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2016-01-12 00:00:00</td>\n",
       "      <td>['https://3dprinting.meta.stackexchange.com/qu...</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>Zizouz212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>10_3dprinting.meta.stackexchange</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There are many different printing technologies...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-12 00:00:00</td>\n",
       "      <td>['https://3dprinting.meta.stackexchange.com/qu...</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the third dimension</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   content_id  question_id  parent_question_id  no_of_answers  answer_id  \\\n",
       "0           1          1.0                 NaN            3.0        NaN   \n",
       "1          14          NaN                 1.0            NaN       14.0   \n",
       "2          15          NaN                 1.0            NaN       15.0   \n",
       "3          41          NaN                 1.0            NaN       41.0   \n",
       "4           2          2.0                 NaN            3.0        NaN   \n",
       "\n",
       "   user_id                     user_id_domain     user_name  \\\n",
       "0       30   30_3dprinting.meta.stackexchange           NaN   \n",
       "1       43   43_3dprinting.meta.stackexchange  Eric Johnson   \n",
       "2       20   20_3dprinting.meta.stackexchange        kenorb   \n",
       "3      138  138_3dprinting.meta.stackexchange     Zizouz212   \n",
       "4       10   10_3dprinting.meta.stackexchange           NaN   \n",
       "\n",
       "                                            fullText  \\\n",
       "0  I have been wanting to learn about 3D printing...   \n",
       "1  I would suggest doing a bit of basic research ...   \n",
       "2  That's the goal of the site, learn, research a...   \n",
       "3  Vote!Private Betas love, love,lovevotes. Witho...   \n",
       "4  There are many different printing technologies...   \n",
       "\n",
       "                               answer_author_profile  answer_pm_score  \\\n",
       "0                                                NaN              NaN   \n",
       "1  https://3dprinting.meta.stackexchange.com/user...              2.0   \n",
       "2  https://3dprinting.meta.stackexchange.com/user...              2.0   \n",
       "3  https://3dprinting.meta.stackexchange.com/user...              4.0   \n",
       "4                                                NaN              NaN   \n",
       "\n",
       "  answer_selected        question_date  \\\n",
       "0             NaN  2016-01-12 00:00:00   \n",
       "1           False  2016-01-12 00:00:00   \n",
       "2           False  2016-01-12 00:00:00   \n",
       "3           False  2016-01-12 00:00:00   \n",
       "4             NaN  2016-01-12 00:00:00   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  ['https://3dprinting.meta.stackexchange.com/qu...   \n",
       "1  ['https://3dprinting.meta.stackexchange.com/qu...   \n",
       "2  ['https://3dprinting.meta.stackexchange.com/qu...   \n",
       "3  ['https://3dprinting.meta.stackexchange.com/qu...   \n",
       "4  ['https://3dprinting.meta.stackexchange.com/qu...   \n",
       "\n",
       "                          domain                  answer_domain  \\\n",
       "0  3dprinting.meta.stackexchange                            NaN   \n",
       "1  3dprinting.meta.stackexchange  3dprinting.meta.stackexchange   \n",
       "2  3dprinting.meta.stackexchange  3dprinting.meta.stackexchange   \n",
       "3  3dprinting.meta.stackexchange  3dprinting.meta.stackexchange   \n",
       "4  3dprinting.meta.stackexchange                            NaN   \n",
       "\n",
       "            user_name_  \n",
       "0            Salmorejo  \n",
       "1         Eric Johnson  \n",
       "2               kenorb  \n",
       "3            Zizouz212  \n",
       "4  the third dimension  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37523135, 17)\n"
     ]
    }
   ],
   "source": [
    "# dfse.rename(columns={'text':'fullText'}, inplace=True)\n",
    "# print(dfse.shape)\n",
    "# display(dfse.head())\n",
    "# dfse.dropna(subset=['fullText', 'user_name_'], inplace=True)\n",
    "# print(dfse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hashlib\n",
    "# import pandas as pd\n",
    "\n",
    "# def hash_user(user_name):\n",
    "#     if pd.isna(user_name):\n",
    "#         return None\n",
    "#     hash_val = hashlib.sha256(user_name.encode('utf-8')).hexdigest()\n",
    "#     return f\"stex_id_{hash_val}\"\n",
    "\n",
    "# dfse['authorID'] = dfse['user_name_'].apply(hash_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfse['documentID'] = dfse.index.map(lambda x: f\"stex_did_{hashlib.sha256(str(x).encode('utf-8')).hexdigest()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>domain_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>math.stackexchange</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>superuser</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>askubuntu</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>serverfault</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               domain domain_category\n",
       "0       stackoverflow      Technology\n",
       "1  math.stackexchange         Science\n",
       "2           superuser      Technology\n",
       "3           askubuntu      Technology\n",
       "4         serverfault      Technology"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# category_mapping_file = \"/data/araghavan/HIATUS/datadreamer-ta2/data/stackexchange/domain_category_mapping.csv\"\n",
    "# dfse_category_mapping = pd.read_csv(category_mapping_file)\n",
    "# dfse_category_mapping.rename(columns={'category':'domain_category'}, inplace=True)\n",
    "# dfse_category_mapping = dfse_category_mapping.loc[(~dfse_category_mapping['domain_category'].str.contains('meta')) & (dfse_category_mapping['domain_category'] != \"Business\")]\n",
    "# dfse_category_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Technology           69\n",
       "Culturerecreation    45\n",
       "Lifearts             26\n",
       "Science              19\n",
       "Professional          6\n",
       "Name: domain_category, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfse_category_mapping['domain_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>parent_question_id</th>\n",
       "      <th>no_of_answers</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_id_domain</th>\n",
       "      <th>user_name</th>\n",
       "      <th>fullText</th>\n",
       "      <th>answer_author_profile</th>\n",
       "      <th>answer_pm_score</th>\n",
       "      <th>answer_selected</th>\n",
       "      <th>question_date</th>\n",
       "      <th>metadata</th>\n",
       "      <th>domain</th>\n",
       "      <th>answer_domain</th>\n",
       "      <th>user_name_</th>\n",
       "      <th>authorID</th>\n",
       "      <th>documentID</th>\n",
       "      <th>domain_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>16_3dprinting.stackexchange</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When I've printed an object I've had to choose...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-12 00:00:00</td>\n",
       "      <td>['https://3dprinting.stackexchange.com/questio...</td>\n",
       "      <td>3dprinting.stackexchange</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam Davis</td>\n",
       "      <td>stex_id_9896303b2a454d748c0faefeeee2f9fc486d6b...</td>\n",
       "      <td>stex_did_8d23cf6c86e834a7aa6eded54c26ce2bb2e74...</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.0</td>\n",
       "      <td>53</td>\n",
       "      <td>53_3dprinting.stackexchange</td>\n",
       "      <td>hroncok</td>\n",
       "      <td>You could experiment with slicing. For example...</td>\n",
       "      <td>https://3dprinting.stackexchange.com/users/53</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2016-01-12 00:00:00</td>\n",
       "      <td>['https://3dprinting.stackexchange.com/questio...</td>\n",
       "      <td>3dprinting.stackexchange</td>\n",
       "      <td>3dprinting.stackexchange</td>\n",
       "      <td>hroncok</td>\n",
       "      <td>stex_id_a94e685bf4dfe985ff85dc7feb3fc4dc3d7e3b...</td>\n",
       "      <td>stex_did_f10d91a7596bf5a6773579ff1306afdc363b0...</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4853.0</td>\n",
       "      <td>4639</td>\n",
       "      <td>4639_3dprinting.stackexchange</td>\n",
       "      <td>plaintoothpaste</td>\n",
       "      <td>For FDM technologies in general with a single ...</td>\n",
       "      <td>https://3dprinting.stackexchange.com/users/4639</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2016-01-12 00:00:00</td>\n",
       "      <td>['https://3dprinting.stackexchange.com/questio...</td>\n",
       "      <td>3dprinting.stackexchange</td>\n",
       "      <td>3dprinting.stackexchange</td>\n",
       "      <td>plaintoothpaste</td>\n",
       "      <td>stex_id_36e0bc2c1812f2e7c651f1d5fd48609bb29e68...</td>\n",
       "      <td>stex_did_3949ac1596ec77106a709a618bf5adcb19b77...</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>20_3dprinting.stackexchange</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I would like to buy a 3D printer, but I'm conc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-12 00:00:00</td>\n",
       "      <td>['https://3dprinting.stackexchange.com/questio...</td>\n",
       "      <td>3dprinting.stackexchange</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kenorb</td>\n",
       "      <td>stex_id_8e55c4e12c67bcc98e4869c2a5fc40ac01f174...</td>\n",
       "      <td>stex_did_1038e0b72d98745fac0fb015fd9c56704862a...</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16</td>\n",
       "      <td>16_3dprinting.stackexchange</td>\n",
       "      <td>Adam Davis</td>\n",
       "      <td>Almost all 3D printers have issues that could ...</td>\n",
       "      <td>https://3dprinting.stackexchange.com/users/16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2016-01-12 00:00:00</td>\n",
       "      <td>['https://3dprinting.stackexchange.com/questio...</td>\n",
       "      <td>3dprinting.stackexchange</td>\n",
       "      <td>3dprinting.stackexchange</td>\n",
       "      <td>Adam Davis</td>\n",
       "      <td>stex_id_9896303b2a454d748c0faefeeee2f9fc486d6b...</td>\n",
       "      <td>stex_did_9e11c362bc3d3572970b973d5cd86c073da35...</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8856492</th>\n",
       "      <td>65687</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64686.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65687.0</td>\n",
       "      <td>44281</td>\n",
       "      <td>44281_writers.stackexchange</td>\n",
       "      <td>Mary</td>\n",
       "      <td>Revision.Go back and decide which red herrings...</td>\n",
       "      <td>https://writers.stackexchange.com/users/44281</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-03-02 00:00:00</td>\n",
       "      <td>['https://writers.stackexchange.com/questions/...</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>Mary</td>\n",
       "      <td>stex_id_aebac53c46bbeff10fdd26ca0e2196a9bfc1d1...</td>\n",
       "      <td>stex_did_280de443242adf9e8f773414be2e543a793c5...</td>\n",
       "      <td>Professional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8856493</th>\n",
       "      <td>65697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64686.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65697.0</td>\n",
       "      <td>55858</td>\n",
       "      <td>55858_writers.stackexchange</td>\n",
       "      <td>user52445</td>\n",
       "      <td>The common advice is to write detective puzzle...</td>\n",
       "      <td>https://writers.stackexchange.com/users/55858</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-03-02 00:00:00</td>\n",
       "      <td>['https://writers.stackexchange.com/questions/...</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>user52445</td>\n",
       "      <td>stex_id_7fa0da4d2f9e55f49b3733a0c7aa6a79a9433e...</td>\n",
       "      <td>stex_did_a556648f941b7782bc43ab73a565d534d98f4...</td>\n",
       "      <td>Professional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8856494</th>\n",
       "      <td>65708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64686.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65708.0</td>\n",
       "      <td>28298</td>\n",
       "      <td>28298_writers.stackexchange</td>\n",
       "      <td>nick012000</td>\n",
       "      <td>Have a fight scene.I remember some writing adv...</td>\n",
       "      <td>https://writers.stackexchange.com/users/28298</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-03-02 00:00:00</td>\n",
       "      <td>['https://writers.stackexchange.com/questions/...</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>nick012000</td>\n",
       "      <td>stex_id_12edbabc1cf50371b0066242cf8ce11d0482fd...</td>\n",
       "      <td>stex_did_8e5c0b52e96126e2faa506ff62e04e5814f63...</td>\n",
       "      <td>Professional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8856495</th>\n",
       "      <td>65696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65695.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65696.0</td>\n",
       "      <td>55858</td>\n",
       "      <td>55858_writers.stackexchange</td>\n",
       "      <td>user52445</td>\n",
       "      <td>I feel your question is missing one of the mos...</td>\n",
       "      <td>https://writers.stackexchange.com/users/55858</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-03-02 00:00:00</td>\n",
       "      <td>['https://writers.stackexchange.com/questions/...</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>user52445</td>\n",
       "      <td>stex_id_7fa0da4d2f9e55f49b3733a0c7aa6a79a9433e...</td>\n",
       "      <td>stex_did_52b9f36918d05eb528f94ab95621c19f24967...</td>\n",
       "      <td>Professional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8856496</th>\n",
       "      <td>65700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65695.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65700.0</td>\n",
       "      <td>55584</td>\n",
       "      <td>55584_writers.stackexchange</td>\n",
       "      <td>Krišjānis Liepiņš</td>\n",
       "      <td>From my experience, formal settings are varied...</td>\n",
       "      <td>https://writers.stackexchange.com/users/55584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-03-02 00:00:00</td>\n",
       "      <td>['https://writers.stackexchange.com/questions/...</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>Krišjānis Liepiņš</td>\n",
       "      <td>stex_id_916c7754bf18705d7b0be06824504a279dd543...</td>\n",
       "      <td>stex_did_09a5c6e2daf2668f0e10c9af7589f10c0cade...</td>\n",
       "      <td>Professional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8856497 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         content_id  question_id  parent_question_id  no_of_answers  \\\n",
       "0                 1          1.0                 NaN            2.0   \n",
       "1                51          NaN                 1.0            NaN   \n",
       "2              4853          NaN                 1.0            NaN   \n",
       "3                 2          2.0                 NaN            4.0   \n",
       "4                 9          NaN                 2.0            NaN   \n",
       "...             ...          ...                 ...            ...   \n",
       "8856492       65687          NaN             64686.0            NaN   \n",
       "8856493       65697          NaN             64686.0            NaN   \n",
       "8856494       65708          NaN             64686.0            NaN   \n",
       "8856495       65696          NaN             65695.0            NaN   \n",
       "8856496       65700          NaN             65695.0            NaN   \n",
       "\n",
       "         answer_id  user_id                 user_id_domain          user_name  \\\n",
       "0              NaN       16    16_3dprinting.stackexchange                NaN   \n",
       "1             51.0       53    53_3dprinting.stackexchange            hroncok   \n",
       "2           4853.0     4639  4639_3dprinting.stackexchange    plaintoothpaste   \n",
       "3              NaN       20    20_3dprinting.stackexchange                NaN   \n",
       "4              9.0       16    16_3dprinting.stackexchange         Adam Davis   \n",
       "...            ...      ...                            ...                ...   \n",
       "8856492    65687.0    44281    44281_writers.stackexchange               Mary   \n",
       "8856493    65697.0    55858    55858_writers.stackexchange          user52445   \n",
       "8856494    65708.0    28298    28298_writers.stackexchange         nick012000   \n",
       "8856495    65696.0    55858    55858_writers.stackexchange          user52445   \n",
       "8856496    65700.0    55584    55584_writers.stackexchange  Krišjānis Liepiņš   \n",
       "\n",
       "                                                  fullText  \\\n",
       "0        When I've printed an object I've had to choose...   \n",
       "1        You could experiment with slicing. For example...   \n",
       "2        For FDM technologies in general with a single ...   \n",
       "3        I would like to buy a 3D printer, but I'm conc...   \n",
       "4        Almost all 3D printers have issues that could ...   \n",
       "...                                                    ...   \n",
       "8856492  Revision.Go back and decide which red herrings...   \n",
       "8856493  The common advice is to write detective puzzle...   \n",
       "8856494  Have a fight scene.I remember some writing adv...   \n",
       "8856495  I feel your question is missing one of the mos...   \n",
       "8856496  From my experience, formal settings are varied...   \n",
       "\n",
       "                                   answer_author_profile  answer_pm_score  \\\n",
       "0                                                    NaN              NaN   \n",
       "1          https://3dprinting.stackexchange.com/users/53              4.0   \n",
       "2        https://3dprinting.stackexchange.com/users/4639              2.0   \n",
       "3                                                    NaN              NaN   \n",
       "4          https://3dprinting.stackexchange.com/users/16              4.0   \n",
       "...                                                  ...              ...   \n",
       "8856492    https://writers.stackexchange.com/users/44281              6.0   \n",
       "8856493    https://writers.stackexchange.com/users/55858              2.0   \n",
       "8856494    https://writers.stackexchange.com/users/28298              1.0   \n",
       "8856495    https://writers.stackexchange.com/users/55858              1.0   \n",
       "8856496    https://writers.stackexchange.com/users/55584              0.0   \n",
       "\n",
       "        answer_selected        question_date  \\\n",
       "0                   NaN  2016-01-12 00:00:00   \n",
       "1                  True  2016-01-12 00:00:00   \n",
       "2                 False  2016-01-12 00:00:00   \n",
       "3                   NaN  2016-01-12 00:00:00   \n",
       "4                 False  2016-01-12 00:00:00   \n",
       "...                 ...                  ...   \n",
       "8856492            True  2023-03-02 00:00:00   \n",
       "8856493           False  2023-03-02 00:00:00   \n",
       "8856494           False  2023-03-02 00:00:00   \n",
       "8856495           False  2023-03-02 00:00:00   \n",
       "8856496           False  2023-03-02 00:00:00   \n",
       "\n",
       "                                                  metadata  \\\n",
       "0        ['https://3dprinting.stackexchange.com/questio...   \n",
       "1        ['https://3dprinting.stackexchange.com/questio...   \n",
       "2        ['https://3dprinting.stackexchange.com/questio...   \n",
       "3        ['https://3dprinting.stackexchange.com/questio...   \n",
       "4        ['https://3dprinting.stackexchange.com/questio...   \n",
       "...                                                    ...   \n",
       "8856492  ['https://writers.stackexchange.com/questions/...   \n",
       "8856493  ['https://writers.stackexchange.com/questions/...   \n",
       "8856494  ['https://writers.stackexchange.com/questions/...   \n",
       "8856495  ['https://writers.stackexchange.com/questions/...   \n",
       "8856496  ['https://writers.stackexchange.com/questions/...   \n",
       "\n",
       "                           domain             answer_domain  \\\n",
       "0        3dprinting.stackexchange                       NaN   \n",
       "1        3dprinting.stackexchange  3dprinting.stackexchange   \n",
       "2        3dprinting.stackexchange  3dprinting.stackexchange   \n",
       "3        3dprinting.stackexchange                       NaN   \n",
       "4        3dprinting.stackexchange  3dprinting.stackexchange   \n",
       "...                           ...                       ...   \n",
       "8856492     writers.stackexchange     writers.stackexchange   \n",
       "8856493     writers.stackexchange     writers.stackexchange   \n",
       "8856494     writers.stackexchange     writers.stackexchange   \n",
       "8856495     writers.stackexchange     writers.stackexchange   \n",
       "8856496     writers.stackexchange     writers.stackexchange   \n",
       "\n",
       "                user_name_                                           authorID  \\\n",
       "0               Adam Davis  stex_id_9896303b2a454d748c0faefeeee2f9fc486d6b...   \n",
       "1                  hroncok  stex_id_a94e685bf4dfe985ff85dc7feb3fc4dc3d7e3b...   \n",
       "2          plaintoothpaste  stex_id_36e0bc2c1812f2e7c651f1d5fd48609bb29e68...   \n",
       "3                   kenorb  stex_id_8e55c4e12c67bcc98e4869c2a5fc40ac01f174...   \n",
       "4               Adam Davis  stex_id_9896303b2a454d748c0faefeeee2f9fc486d6b...   \n",
       "...                    ...                                                ...   \n",
       "8856492               Mary  stex_id_aebac53c46bbeff10fdd26ca0e2196a9bfc1d1...   \n",
       "8856493          user52445  stex_id_7fa0da4d2f9e55f49b3733a0c7aa6a79a9433e...   \n",
       "8856494         nick012000  stex_id_12edbabc1cf50371b0066242cf8ce11d0482fd...   \n",
       "8856495          user52445  stex_id_7fa0da4d2f9e55f49b3733a0c7aa6a79a9433e...   \n",
       "8856496  Krišjānis Liepiņš  stex_id_916c7754bf18705d7b0be06824504a279dd543...   \n",
       "\n",
       "                                                documentID domain_category  \n",
       "0        stex_did_8d23cf6c86e834a7aa6eded54c26ce2bb2e74...      Technology  \n",
       "1        stex_did_f10d91a7596bf5a6773579ff1306afdc363b0...      Technology  \n",
       "2        stex_did_3949ac1596ec77106a709a618bf5adcb19b77...      Technology  \n",
       "3        stex_did_1038e0b72d98745fac0fb015fd9c56704862a...      Technology  \n",
       "4        stex_did_9e11c362bc3d3572970b973d5cd86c073da35...      Technology  \n",
       "...                                                    ...             ...  \n",
       "8856492  stex_did_280de443242adf9e8f773414be2e543a793c5...    Professional  \n",
       "8856493  stex_did_a556648f941b7782bc43ab73a565d534d98f4...    Professional  \n",
       "8856494  stex_did_8e5c0b52e96126e2faa506ff62e04e5814f63...    Professional  \n",
       "8856495  stex_did_52b9f36918d05eb528f94ab95621c19f24967...    Professional  \n",
       "8856496  stex_did_09a5c6e2daf2668f0e10c9af7589f10c0cade...    Professional  \n",
       "\n",
       "[8856497 rows x 20 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfse_categorized = dfse.merge(dfse_category_mapping, how='inner', on=\"domain\")\n",
    "# dfse_categorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Technology           4277439\n",
       "Science              2060337\n",
       "Culturerecreation    1377070\n",
       "Lifearts              946874\n",
       "Professional          194777\n",
       "Name: domain_category, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfse_categorized['domain_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          When I've printed an object I've had to choose...\n",
       "1          You could experiment with slicing. For example...\n",
       "2          For FDM technologies in general with a single ...\n",
       "3          I would like to buy a 3D printer, but I'm conc...\n",
       "4          Almost all 3D printers have issues that could ...\n",
       "                                 ...                        \n",
       "8856492    Revision.Go back and decide which red herrings...\n",
       "8856493    The common advice is to write detective puzzle...\n",
       "8856494    Have a fight scene.I remember some writing adv...\n",
       "8856495    I feel your question is missing one of the mos...\n",
       "8856496    From my experience, formal settings are varied...\n",
       "Name: fullText, Length: 8856497, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfse_categorized['fullText'].str.replace(r'\\s+', ' ', regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>parent_question_id</th>\n",
       "      <th>no_of_answers</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_id_domain</th>\n",
       "      <th>user_name</th>\n",
       "      <th>fullText</th>\n",
       "      <th>answer_author_profile</th>\n",
       "      <th>answer_pm_score</th>\n",
       "      <th>answer_selected</th>\n",
       "      <th>question_date</th>\n",
       "      <th>metadata</th>\n",
       "      <th>domain</th>\n",
       "      <th>answer_domain</th>\n",
       "      <th>user_name_</th>\n",
       "      <th>authorID</th>\n",
       "      <th>documentID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>30_3dprinting.meta.stackexchange</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I have been wanting to learn about 3D printing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-12 00:00:00</td>\n",
       "      <td>['https://3dprinting.meta.stackexchange.com/qu...</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Salmorejo</td>\n",
       "      <td>stex_id_007ec31fa0f17736e394c6de9f8ce48c75aeab...</td>\n",
       "      <td>stex_did_5feceb66ffc86f38d952786c6d696c79c2dbc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>43</td>\n",
       "      <td>43_3dprinting.meta.stackexchange</td>\n",
       "      <td>Eric Johnson</td>\n",
       "      <td>I would suggest doing a bit of basic research ...</td>\n",
       "      <td>https://3dprinting.meta.stackexchange.com/user...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2016-01-12 00:00:00</td>\n",
       "      <td>['https://3dprinting.meta.stackexchange.com/qu...</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>Eric Johnson</td>\n",
       "      <td>stex_id_d159835d21340fcc8d6fae6e87ce700f322f28...</td>\n",
       "      <td>stex_did_6b86b273ff34fce19d6b804eff5a3f5747ada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20</td>\n",
       "      <td>20_3dprinting.meta.stackexchange</td>\n",
       "      <td>kenorb</td>\n",
       "      <td>That's the goal of the site, learn, research a...</td>\n",
       "      <td>https://3dprinting.meta.stackexchange.com/user...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2016-01-12 00:00:00</td>\n",
       "      <td>['https://3dprinting.meta.stackexchange.com/qu...</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>kenorb</td>\n",
       "      <td>stex_id_8e55c4e12c67bcc98e4869c2a5fc40ac01f174...</td>\n",
       "      <td>stex_did_d4735e3a265e16eee03f59718b9b5d03019c0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.0</td>\n",
       "      <td>138</td>\n",
       "      <td>138_3dprinting.meta.stackexchange</td>\n",
       "      <td>Zizouz212</td>\n",
       "      <td>Vote!Private Betas love, love,lovevotes. Witho...</td>\n",
       "      <td>https://3dprinting.meta.stackexchange.com/user...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2016-01-12 00:00:00</td>\n",
       "      <td>['https://3dprinting.meta.stackexchange.com/qu...</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>Zizouz212</td>\n",
       "      <td>stex_id_7ed88b4d5dc4495b290f643ed52bec6818c8cf...</td>\n",
       "      <td>stex_did_4e07408562bedb8b60ce05c1decfe3ad16b72...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>10_3dprinting.meta.stackexchange</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There are many different printing technologies...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-12 00:00:00</td>\n",
       "      <td>['https://3dprinting.meta.stackexchange.com/qu...</td>\n",
       "      <td>3dprinting.meta.stackexchange</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the third dimension</td>\n",
       "      <td>stex_id_72de12635ae5369bbd8c347ab022501f678d00...</td>\n",
       "      <td>stex_did_4b227777d4dd1fc61c6f884f48641d02b4d12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41515399</th>\n",
       "      <td>65687</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64686.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65687.0</td>\n",
       "      <td>44281</td>\n",
       "      <td>44281_writers.stackexchange</td>\n",
       "      <td>Mary</td>\n",
       "      <td>Revision.Go back and decide which red herrings...</td>\n",
       "      <td>https://writers.stackexchange.com/users/44281</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-03-02 00:00:00</td>\n",
       "      <td>['https://writers.stackexchange.com/questions/...</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>Mary</td>\n",
       "      <td>stex_id_aebac53c46bbeff10fdd26ca0e2196a9bfc1d1...</td>\n",
       "      <td>stex_did_280de443242adf9e8f773414be2e543a793c5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41515400</th>\n",
       "      <td>65697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64686.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65697.0</td>\n",
       "      <td>55858</td>\n",
       "      <td>55858_writers.stackexchange</td>\n",
       "      <td>user52445</td>\n",
       "      <td>The common advice is to write detective puzzle...</td>\n",
       "      <td>https://writers.stackexchange.com/users/55858</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-03-02 00:00:00</td>\n",
       "      <td>['https://writers.stackexchange.com/questions/...</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>user52445</td>\n",
       "      <td>stex_id_7fa0da4d2f9e55f49b3733a0c7aa6a79a9433e...</td>\n",
       "      <td>stex_did_a556648f941b7782bc43ab73a565d534d98f4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41515401</th>\n",
       "      <td>65708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64686.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65708.0</td>\n",
       "      <td>28298</td>\n",
       "      <td>28298_writers.stackexchange</td>\n",
       "      <td>nick012000</td>\n",
       "      <td>Have a fight scene.I remember some writing adv...</td>\n",
       "      <td>https://writers.stackexchange.com/users/28298</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-03-02 00:00:00</td>\n",
       "      <td>['https://writers.stackexchange.com/questions/...</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>nick012000</td>\n",
       "      <td>stex_id_12edbabc1cf50371b0066242cf8ce11d0482fd...</td>\n",
       "      <td>stex_did_8e5c0b52e96126e2faa506ff62e04e5814f63...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41515403</th>\n",
       "      <td>65696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65695.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65696.0</td>\n",
       "      <td>55858</td>\n",
       "      <td>55858_writers.stackexchange</td>\n",
       "      <td>user52445</td>\n",
       "      <td>I feel your question is missing one of the mos...</td>\n",
       "      <td>https://writers.stackexchange.com/users/55858</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-03-02 00:00:00</td>\n",
       "      <td>['https://writers.stackexchange.com/questions/...</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>user52445</td>\n",
       "      <td>stex_id_7fa0da4d2f9e55f49b3733a0c7aa6a79a9433e...</td>\n",
       "      <td>stex_did_52b9f36918d05eb528f94ab95621c19f24967...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41515404</th>\n",
       "      <td>65700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65695.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65700.0</td>\n",
       "      <td>55584</td>\n",
       "      <td>55584_writers.stackexchange</td>\n",
       "      <td>Krišjānis Liepiņš</td>\n",
       "      <td>From my experience, formal settings are varied...</td>\n",
       "      <td>https://writers.stackexchange.com/users/55584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-03-02 00:00:00</td>\n",
       "      <td>['https://writers.stackexchange.com/questions/...</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>writers.stackexchange</td>\n",
       "      <td>Krišjānis Liepiņš</td>\n",
       "      <td>stex_id_916c7754bf18705d7b0be06824504a279dd543...</td>\n",
       "      <td>stex_did_09a5c6e2daf2668f0e10c9af7589f10c0cade...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37523135 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          content_id  question_id  parent_question_id  no_of_answers  \\\n",
       "0                  1          1.0                 NaN            3.0   \n",
       "1                 14          NaN                 1.0            NaN   \n",
       "2                 15          NaN                 1.0            NaN   \n",
       "3                 41          NaN                 1.0            NaN   \n",
       "4                  2          2.0                 NaN            3.0   \n",
       "...              ...          ...                 ...            ...   \n",
       "41515399       65687          NaN             64686.0            NaN   \n",
       "41515400       65697          NaN             64686.0            NaN   \n",
       "41515401       65708          NaN             64686.0            NaN   \n",
       "41515403       65696          NaN             65695.0            NaN   \n",
       "41515404       65700          NaN             65695.0            NaN   \n",
       "\n",
       "          answer_id  user_id                     user_id_domain  \\\n",
       "0               NaN       30   30_3dprinting.meta.stackexchange   \n",
       "1              14.0       43   43_3dprinting.meta.stackexchange   \n",
       "2              15.0       20   20_3dprinting.meta.stackexchange   \n",
       "3              41.0      138  138_3dprinting.meta.stackexchange   \n",
       "4               NaN       10   10_3dprinting.meta.stackexchange   \n",
       "...             ...      ...                                ...   \n",
       "41515399    65687.0    44281        44281_writers.stackexchange   \n",
       "41515400    65697.0    55858        55858_writers.stackexchange   \n",
       "41515401    65708.0    28298        28298_writers.stackexchange   \n",
       "41515403    65696.0    55858        55858_writers.stackexchange   \n",
       "41515404    65700.0    55584        55584_writers.stackexchange   \n",
       "\n",
       "                  user_name  \\\n",
       "0                       NaN   \n",
       "1              Eric Johnson   \n",
       "2                    kenorb   \n",
       "3                 Zizouz212   \n",
       "4                       NaN   \n",
       "...                     ...   \n",
       "41515399               Mary   \n",
       "41515400          user52445   \n",
       "41515401         nick012000   \n",
       "41515403          user52445   \n",
       "41515404  Krišjānis Liepiņš   \n",
       "\n",
       "                                                   fullText  \\\n",
       "0         I have been wanting to learn about 3D printing...   \n",
       "1         I would suggest doing a bit of basic research ...   \n",
       "2         That's the goal of the site, learn, research a...   \n",
       "3         Vote!Private Betas love, love,lovevotes. Witho...   \n",
       "4         There are many different printing technologies...   \n",
       "...                                                     ...   \n",
       "41515399  Revision.Go back and decide which red herrings...   \n",
       "41515400  The common advice is to write detective puzzle...   \n",
       "41515401  Have a fight scene.I remember some writing adv...   \n",
       "41515403  I feel your question is missing one of the mos...   \n",
       "41515404  From my experience, formal settings are varied...   \n",
       "\n",
       "                                      answer_author_profile  answer_pm_score  \\\n",
       "0                                                       NaN              NaN   \n",
       "1         https://3dprinting.meta.stackexchange.com/user...              2.0   \n",
       "2         https://3dprinting.meta.stackexchange.com/user...              2.0   \n",
       "3         https://3dprinting.meta.stackexchange.com/user...              4.0   \n",
       "4                                                       NaN              NaN   \n",
       "...                                                     ...              ...   \n",
       "41515399      https://writers.stackexchange.com/users/44281              6.0   \n",
       "41515400      https://writers.stackexchange.com/users/55858              2.0   \n",
       "41515401      https://writers.stackexchange.com/users/28298              1.0   \n",
       "41515403      https://writers.stackexchange.com/users/55858              1.0   \n",
       "41515404      https://writers.stackexchange.com/users/55584              0.0   \n",
       "\n",
       "         answer_selected        question_date  \\\n",
       "0                    NaN  2016-01-12 00:00:00   \n",
       "1                  False  2016-01-12 00:00:00   \n",
       "2                  False  2016-01-12 00:00:00   \n",
       "3                  False  2016-01-12 00:00:00   \n",
       "4                    NaN  2016-01-12 00:00:00   \n",
       "...                  ...                  ...   \n",
       "41515399            True  2023-03-02 00:00:00   \n",
       "41515400           False  2023-03-02 00:00:00   \n",
       "41515401           False  2023-03-02 00:00:00   \n",
       "41515403           False  2023-03-02 00:00:00   \n",
       "41515404           False  2023-03-02 00:00:00   \n",
       "\n",
       "                                                   metadata  \\\n",
       "0         ['https://3dprinting.meta.stackexchange.com/qu...   \n",
       "1         ['https://3dprinting.meta.stackexchange.com/qu...   \n",
       "2         ['https://3dprinting.meta.stackexchange.com/qu...   \n",
       "3         ['https://3dprinting.meta.stackexchange.com/qu...   \n",
       "4         ['https://3dprinting.meta.stackexchange.com/qu...   \n",
       "...                                                     ...   \n",
       "41515399  ['https://writers.stackexchange.com/questions/...   \n",
       "41515400  ['https://writers.stackexchange.com/questions/...   \n",
       "41515401  ['https://writers.stackexchange.com/questions/...   \n",
       "41515403  ['https://writers.stackexchange.com/questions/...   \n",
       "41515404  ['https://writers.stackexchange.com/questions/...   \n",
       "\n",
       "                                 domain                  answer_domain  \\\n",
       "0         3dprinting.meta.stackexchange                            NaN   \n",
       "1         3dprinting.meta.stackexchange  3dprinting.meta.stackexchange   \n",
       "2         3dprinting.meta.stackexchange  3dprinting.meta.stackexchange   \n",
       "3         3dprinting.meta.stackexchange  3dprinting.meta.stackexchange   \n",
       "4         3dprinting.meta.stackexchange                            NaN   \n",
       "...                                 ...                            ...   \n",
       "41515399          writers.stackexchange          writers.stackexchange   \n",
       "41515400          writers.stackexchange          writers.stackexchange   \n",
       "41515401          writers.stackexchange          writers.stackexchange   \n",
       "41515403          writers.stackexchange          writers.stackexchange   \n",
       "41515404          writers.stackexchange          writers.stackexchange   \n",
       "\n",
       "                   user_name_  \\\n",
       "0                   Salmorejo   \n",
       "1                Eric Johnson   \n",
       "2                      kenorb   \n",
       "3                   Zizouz212   \n",
       "4         the third dimension   \n",
       "...                       ...   \n",
       "41515399                 Mary   \n",
       "41515400            user52445   \n",
       "41515401           nick012000   \n",
       "41515403            user52445   \n",
       "41515404    Krišjānis Liepiņš   \n",
       "\n",
       "                                                   authorID  \\\n",
       "0         stex_id_007ec31fa0f17736e394c6de9f8ce48c75aeab...   \n",
       "1         stex_id_d159835d21340fcc8d6fae6e87ce700f322f28...   \n",
       "2         stex_id_8e55c4e12c67bcc98e4869c2a5fc40ac01f174...   \n",
       "3         stex_id_7ed88b4d5dc4495b290f643ed52bec6818c8cf...   \n",
       "4         stex_id_72de12635ae5369bbd8c347ab022501f678d00...   \n",
       "...                                                     ...   \n",
       "41515399  stex_id_aebac53c46bbeff10fdd26ca0e2196a9bfc1d1...   \n",
       "41515400  stex_id_7fa0da4d2f9e55f49b3733a0c7aa6a79a9433e...   \n",
       "41515401  stex_id_12edbabc1cf50371b0066242cf8ce11d0482fd...   \n",
       "41515403  stex_id_7fa0da4d2f9e55f49b3733a0c7aa6a79a9433e...   \n",
       "41515404  stex_id_916c7754bf18705d7b0be06824504a279dd543...   \n",
       "\n",
       "                                                 documentID  \n",
       "0         stex_did_5feceb66ffc86f38d952786c6d696c79c2dbc...  \n",
       "1         stex_did_6b86b273ff34fce19d6b804eff5a3f5747ada...  \n",
       "2         stex_did_d4735e3a265e16eee03f59718b9b5d03019c0...  \n",
       "3         stex_did_4e07408562bedb8b60ce05c1decfe3ad16b72...  \n",
       "4         stex_did_4b227777d4dd1fc61c6f884f48641d02b4d12...  \n",
       "...                                                     ...  \n",
       "41515399  stex_did_280de443242adf9e8f773414be2e543a793c5...  \n",
       "41515400  stex_did_a556648f941b7782bc43ab73a565d534d98f4...  \n",
       "41515401  stex_did_8e5c0b52e96126e2faa506ff62e04e5814f63...  \n",
       "41515403  stex_did_52b9f36918d05eb528f94ab95621c19f24967...  \n",
       "41515404  stex_did_09a5c6e2daf2668f0e10c9af7589f10c0cade...  \n",
       "\n",
       "[37523135 rows x 19 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dfse['fullText_space_lengthWords'] = dfse['fullText'].str.split().str.len()\n",
    "# dfse[dfse['fullText_space_lengthWords'] >= 70].to_json(output_file_json_gte70, orient='records', lines=True, force_ascii=False)\n",
    "# dfse[dfse['fullText_space_lengthWords'] >= 250].to_json(output_file_json_gte250, orient='records', lines=True, force_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del dfse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Shape for gte 70: (18401069, 20)\n",
      "Author Count Shape for gte 70: 1345037\n",
      "Output File for gte 70: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/stackexchange_gte70.jsonl\n"
     ]
    }
   ],
   "source": [
    "# dfse_gte70 = pd.read_json(output_file_json_gte70, lines=True)\n",
    "# print(f\"Combined Shape for gte 70: {dfse_gte70.shape}\")\n",
    "# dfse_gte70['author_docs_count'] = dfse_gte70.groupby('authorID')['documentID'].transform('count')\n",
    "# print(f\"Author Count Shape for gte 70: {dfse_gte70.authorID.nunique()}\")\n",
    "# dfse_gte70.to_json(output_file_json_gte70, orient='records', lines=True, force_ascii=False)\n",
    "# print(f\"Output File for gte 70: {output_file_json_gte70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Shape for gte 70: (2913840, 20)\n",
      "Author Count Shape for gte 70: 468171\n",
      "Output File for gte 70: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/stackexchange_gte250.jsonl\n"
     ]
    }
   ],
   "source": [
    "# dfse_gte250 = pd.read_json(output_file_json_gte250, lines=True)\n",
    "# print(f\"Combined Shape for gte 70: {dfse_gte250.shape}\")\n",
    "# dfse_gte250['author_docs_count'] = dfse_gte250.groupby('authorID')['documentID'].transform('count')\n",
    "# print(f\"Author Count Shape for gte 70: {dfse_gte250.authorID.nunique()}\")\n",
    "# dfse_gte250.to_json(output_file_json_gte250, orient='records', lines=True, force_ascii=False)\n",
    "# print(f\"Output File for gte 70: {output_file_json_gte250}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Detection and Preprocessing\n",
    "### For more details, refer to: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/all_data_prep_metadata.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Content of the metadata file:\n",
    "# V1 - has language to Raw Data\n",
    "# V2 - V1 + removed non english docs, updated author docs count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_raw_folder = \"all_data_raw\"\n",
    "all_data_prep_folder = \"all_data_prep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fasttext\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import hf_hub_download\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "# Download the fastText model (unquantized in this case) from Hugging Face Hub\n",
    "model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n",
    "model = fasttext.load_model(model_path)\n",
    "\n",
    "def v1_add_language_column_to_file(\n",
    "    input_file_path,\n",
    "    output_file_path,\n",
    "    model,\n",
    "    text_column='fullText',\n",
    "    lang_column='language_x_biber',  # 'x' replaced with detection method\n",
    "    method='fasttext',\n",
    "    chunksize=20000\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file in chunks, detects the language of the text column,\n",
    "    and writes a new JSONL file with the detected language column added.\n",
    "\n",
    "    Uses batch predictions per chunk for efficiency.\n",
    "    Keeps the output file open for all chunks to reduce overhead,\n",
    "    and uses tqdm progress bar instead of print statements.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_file_path : str\n",
    "        Path to the input JSONL file (one document per line).\n",
    "    output_file_path : str\n",
    "        Path to the output JSONL file.\n",
    "    model : fasttext.FastText._FastText\n",
    "        A loaded fastText language detection model.\n",
    "    text_column : str\n",
    "        Column name in the dataframe that contains the text to analyze.\n",
    "    lang_column : str\n",
    "        Column name to store the detected language. 'x' in the string\n",
    "        will be replaced by the `method` name. Default is 'language_x_biber'.\n",
    "    method : str\n",
    "        Language detection method name. Defaults to 'fasttext'.\n",
    "    chunksize : int\n",
    "        Number of lines/rows to process at a time.\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace 'x' in lang_column with the detection method name\n",
    "    lang_column = lang_column.replace(\"x\", method)\n",
    "\n",
    "    print(f\"Reading from: {input_file_path}\")\n",
    "    print(f\"Writing to:   {output_file_path}\")\n",
    "    print(f\"Text column:  {text_column}\")\n",
    "    print(f\"New language column: {lang_column}\")\n",
    "    print(f\"Detection method: {method}\")\n",
    "    print(f\"Chunk size: {chunksize}\\n\")\n",
    "\n",
    "    if method.lower() != 'fasttext':\n",
    "        raise ValueError(f\"Only 'fasttext' is supported in this example, got '{method}'.\")\n",
    "\n",
    "    def detect_language_batch(text_list):\n",
    "        # Create a cleaned copy for prediction: remove newline characters,\n",
    "        # but do not modify the original texts.\n",
    "        cleaned_texts = [\n",
    "            t.replace(\"\\n\", \" \") if isinstance(t, str) and t.strip() else \"\"\n",
    "            for t in text_list\n",
    "        ]\n",
    "        # Use the cleaned texts for prediction\n",
    "        labels, _ = model.predict(cleaned_texts)\n",
    "        # Convert labels from \"__label__en\" to \"en\"\n",
    "        return [lbl[0].replace('__label__', '') if lbl else 'unknown' for lbl in labels]\n",
    "\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "        with pd.read_json(input_file_path, lines=True, chunksize=chunksize, convert_dates=False) as reader:\n",
    "            for chunk_df in tqdm(reader, desc=\"Processing chunks\", unit=\"chunk\"):\n",
    "                # Get the original text column (with newlines preserved)\n",
    "                chunk_texts = chunk_df[text_column].tolist()\n",
    "                # Get predictions using the cleaned text copy\n",
    "                predicted_langs = detect_language_batch(chunk_texts)\n",
    "                # Add new language column to the chunk; original text remains unchanged\n",
    "                chunk_df[lang_column] = predicted_langs\n",
    "                # Write the processed chunk to the output file as JSON lines\n",
    "                chunk_df.to_json(outfile, orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"\\nDone. The combined file with language annotations is ready here: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python biber_language_detection.py --input_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/ao3_gte250.jsonl' --output_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/ao3_gte250_v01.jsonl' --chunksize 50000\n",
    "# python biber_language_detection.py --input_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/ao3_gte70.jsonl' --output_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/ao3_gte70_v01.jsonl' --chunksize 50000\n",
    "# python biber_language_detection.py --input_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/back_gte250.jsonl' --output_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/back_gte250_v01.jsonl' --chunksize 50000\n",
    "# python biber_language_detection.py --input_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/back_gte70.jsonl' --output_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/back_gte70_v01.jsonl' --chunksize 50000\n",
    "# python biber_language_detection.py --input_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/bookcorpus_corpus_gte250.jsonl' --output_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/bookcorpus_corpus_gte250_v01.jsonl' --chunksize 50000\n",
    "# python biber_language_detection.py --input_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/bookcorpus_corpus_gte70.jsonl' --output_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/bookcorpus_corpus_gte70_v01.jsonl' --chunksize 50000\n",
    "# python biber_language_detection.py --input_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/gmane_gte250.jsonl' --output_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/gmane_gte250_v01.jsonl' --chunksize 50000\n",
    "# python biber_language_detection.py --input_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/gmane_gte70.jsonl' --output_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/gmane_gte70_v01.jsonl' --chunksize 50000\n",
    "# python biber_language_detection.py --input_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/realnews_gte250.jsonl' --output_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/realnews_gte250_v01.jsonl' --chunksize 50000\n",
    "# python biber_language_detection.py --input_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/realnews_gte70.jsonl' --output_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/realnews_gte70_v01.jsonl' --chunksize 50000\n",
    "# python biber_language_detection.py --input_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/reddit_corpus_gte250.jsonl' --output_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/reddit_corpus_gte250_v01.jsonl' --chunksize 50000\n",
    "# python biber_language_detection.py --input_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/reddit_corpus_gte70.jsonl' --output_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/reddit_corpus_gte70_v01.jsonl' --chunksize 50000\n",
    "\n",
    "\n",
    "# python biber_language_detection.py --input_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/stackexchange_gte70.jsonl' --output_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/stackexchange_gte70_v01.jsonl' --chunksize 50000\n",
    "# python biber_language_detection.py --input_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_raw/stackexchange_gte250.jsonl' --output_file_path '/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/stackexchange_gte250_v01.jsonl' --chunksize 50000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "def v2_filter_out_nonen_upd_auth_docs_count(\n",
    "        input_file_path,\n",
    "        output_file_path,\n",
    "        new_lang_col_name,\n",
    "        author_docs_count_col_name,\n",
    "        en_lang_val_arr,\n",
    "):\n",
    "    if not os.path.isfile(input_file_path):\n",
    "        print(f\"Input File: {input_file_path} not present\")\n",
    "        raise ValueError(f\"Please provide a valid input file path, incorrect: {input_file_path}\")\n",
    "\n",
    "    print(f\"\"\"\n",
    "        input_file_path: {input_file_path}\n",
    "        output_file_path: {output_file_path}\n",
    "        new_lang_col_name: {new_lang_col_name}\n",
    "        author_docs_count_col_name: {author_docs_count_col_name}\n",
    "        en_lang_val_arr: {en_lang_val_arr} \n",
    "     \"\"\")\n",
    "    \n",
    "    df = pd.read_json(input_file_path, lines=True, convert_dates=False)\n",
    "    print(f\"Read input file to dataframe: {input_file_path} with shape: {df.shape}\") \n",
    "    ## Filter out non english language docs\n",
    "    df = df[df[new_lang_col_name].isin(en_lang_val_arr)]\n",
    "    print(f\"Filtered out non english language documents, now with shape: {df.shape}\")\n",
    "    ## Recount author docs count and update it\n",
    "    df[author_docs_count_col_name] = df.groupby('authorID')['documentID'].transform('count')\n",
    "\n",
    "    df.to_json(output_file_path, orient='records', lines=True, force_ascii=False)\n",
    "    print(f\"Output written to file: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def v2_filter_out_nonen_upd_auth_docs_count_chunked(\n",
    "        input_file_path,\n",
    "        output_file_path,\n",
    "        new_lang_col_name,\n",
    "        author_docs_count_col_name,\n",
    "        en_lang_val_arr,\n",
    "        chunksize=100000\n",
    "):\n",
    "    if not os.path.isfile(input_file_path):\n",
    "        print(f\"Input File: {input_file_path} not present\")\n",
    "        raise ValueError(f\"Please provide a valid input file path, incorrect: {input_file_path}\")\n",
    "\n",
    "    print(f\"\"\"\n",
    "        input_file_path: {input_file_path}\n",
    "        output_file_path: {output_file_path}\n",
    "        new_lang_col_name: {new_lang_col_name}\n",
    "        author_docs_count_col_name: {author_docs_count_col_name}\n",
    "        en_lang_val_arr: {en_lang_val_arr}\n",
    "        chunksize: {chunksize}\n",
    "    \"\"\")\n",
    "\n",
    "    # Count total lines to determine number of chunks\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "    total_chunks = (total_lines // chunksize) + (1 if total_lines % chunksize else 0)\n",
    "    print(f\"Total lines in file: {total_lines}, Total chunks (approx.): {total_chunks}\")\n",
    "\n",
    "    # -----------------------\n",
    "    # First pass: Compute global author counts\n",
    "    # -----------------------\n",
    "    global_author_counts = {}\n",
    "    reader = pd.read_json(input_file_path, lines=True, convert_dates=False, chunksize=chunksize)\n",
    "    for chunk in tqdm(reader, total=total_chunks, desc=\"First pass: Counting docs\", unit=\"chunk\"):\n",
    "        # Filter for allowed language docs and create a copy\n",
    "        filtered_chunk = chunk.loc[chunk[new_lang_col_name].isin(en_lang_val_arr)].copy()\n",
    "        chunk_counts = filtered_chunk.groupby('authorID')['documentID'].count()\n",
    "        for author, count in chunk_counts.items():\n",
    "            global_author_counts[author] = global_author_counts.get(author, 0) + count\n",
    "\n",
    "    print(\"Global author counts computed.\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Second pass: Process chunks, update counts, and write output\n",
    "    # -----------------------\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "        reader = pd.read_json(input_file_path, lines=True, convert_dates=False, chunksize=chunksize)\n",
    "        for chunk in tqdm(reader, total=total_chunks, desc=\"Second pass: Processing chunks\", unit=\"chunk\"):\n",
    "            # Filter for allowed language docs and copy to avoid SettingWithCopyWarning\n",
    "            filtered_chunk = chunk.loc[chunk[new_lang_col_name].isin(en_lang_val_arr)].copy()\n",
    "            if not filtered_chunk.empty:\n",
    "                filtered_chunk.loc[:, author_docs_count_col_name] = filtered_chunk['authorID'].map(global_author_counts)\n",
    "                filtered_chunk.to_json(outfile, orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Output written to file: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/ao3_gte250_v01.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/ao3_gte250_v02.jsonl\n",
      "        new_lang_col_name: language_fasttext_biber\n",
      "        author_docs_count_col_name: author_docs_count\n",
      "        en_lang_val_arr: ['en', 'eng_Latn']\n",
      "        chunksize: 100000\n",
      "    \n",
      "Total lines in file: 9324488, Total chunks (approx.): 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First pass: Counting docs: 100%|██████████| 94/94 [10:31<00:00,  6.72s/chunk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global author counts computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Second pass: Processing chunks: 100%|██████████| 94/94 [20:05<00:00, 12.82s/chunk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output written to file: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/ao3_gte250_v02.jsonl\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/ao3_gte70_v01.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/ao3_gte70_v02.jsonl\n",
      "        new_lang_col_name: language_fasttext_biber\n",
      "        author_docs_count_col_name: author_docs_count\n",
      "        en_lang_val_arr: ['en', 'eng_Latn']\n",
      "        chunksize: 100000\n",
      "    \n",
      "Total lines in file: 9370255, Total chunks (approx.): 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First pass: Counting docs: 100%|██████████| 94/94 [10:29<00:00,  6.69s/chunk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global author counts computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Second pass: Processing chunks: 100%|██████████| 94/94 [20:06<00:00, 12.83s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output written to file: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/ao3_gte70_v02.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "v2_filter_out_nonen_upd_auth_docs_count_chunked(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/ao3_gte250_v01.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/ao3_gte250_v02.jsonl\",\n",
    "    new_lang_col_name=\"language_fasttext_biber\",\n",
    "    author_docs_count_col_name=\"author_docs_count\",\n",
    "    en_lang_val_arr=[\"en\", \"eng_Latn\"],\n",
    ")\n",
    "\n",
    "v2_filter_out_nonen_upd_auth_docs_count_chunked(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/ao3_gte70_v01.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/ao3_gte70_v02.jsonl\",\n",
    "    new_lang_col_name=\"language_fasttext_biber\",\n",
    "    author_docs_count_col_name=\"author_docs_count\",\n",
    "    en_lang_val_arr=[\"en\", \"eng_Latn\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/stackexchange_gte250_v01.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/stackexchange_gte250_v02.jsonl\n",
      "        new_lang_col_name: language_fasttext_biber\n",
      "        author_docs_count_col_name: author_docs_count\n",
      "        en_lang_val_arr: ['en', 'eng_Latn']\n",
      "        chunksize: 100000\n",
      "    \n",
      "Total lines in file: 2913840, Total chunks (approx.): 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First pass: Counting docs: 100%|██████████| 30/30 [01:49<00:00,  3.66s/chunk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global author counts computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Second pass: Processing chunks: 100%|██████████| 30/30 [03:16<00:00,  6.54s/chunk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output written to file: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/stackexchange_gte250_v02.jsonl\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/stackexchange_gte70_v01.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/stackexchange_gte70_v02.jsonl\n",
      "        new_lang_col_name: language_fasttext_biber\n",
      "        author_docs_count_col_name: author_docs_count\n",
      "        en_lang_val_arr: ['en', 'eng_Latn']\n",
      "        chunksize: 100000\n",
      "    \n",
      "Total lines in file: 18401069, Total chunks (approx.): 185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First pass: Counting docs: 100%|██████████| 185/185 [08:05<00:00,  2.62s/chunk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global author counts computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Second pass: Processing chunks: 100%|██████████| 185/185 [14:34<00:00,  4.73s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output written to file: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/stackexchange_gte70_v02.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "v2_filter_out_nonen_upd_auth_docs_count_chunked(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/stackexchange_gte250_v01.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/stackexchange_gte250_v02.jsonl\",\n",
    "    new_lang_col_name=\"language_fasttext_biber\",\n",
    "    author_docs_count_col_name=\"author_docs_count\",\n",
    "    en_lang_val_arr=[\"en\", \"eng_Latn\"],\n",
    ")\n",
    "\n",
    "v2_filter_out_nonen_upd_auth_docs_count_chunked(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/stackexchange_gte70_v01.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/stackexchange_gte70_v02.jsonl\",\n",
    "    new_lang_col_name=\"language_fasttext_biber\",\n",
    "    author_docs_count_col_name=\"author_docs_count\",\n",
    "    en_lang_val_arr=[\"en\", \"eng_Latn\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/back_gte250_v01.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/back_gte250_v02.jsonl\n",
      "        new_lang_col_name: language_fasttext_biber\n",
      "        author_docs_count_col_name: author_docs_count\n",
      "        en_lang_val_arr: ['en', 'eng_Latn'] \n",
      "     \n",
      "Read input file to dataframe: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/back_gte250_v01.jsonl with shape: (174977, 11)\n",
      "Filtered out non english language documents, now with shape: (173598, 11)\n",
      "Output written to file: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/back_gte250_v02.jsonl\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/back_gte70_v01.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/back_gte70_v02.jsonl\n",
      "        new_lang_col_name: language_fasttext_biber\n",
      "        author_docs_count_col_name: author_docs_count\n",
      "        en_lang_val_arr: ['en', 'eng_Latn'] \n",
      "     \n",
      "Read input file to dataframe: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/back_gte70_v01.jsonl with shape: (425612, 11)\n",
      "Filtered out non english language documents, now with shape: (421473, 11)\n",
      "Output written to file: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/back_gte70_v02.jsonl\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/bookcorpus_corpus_gte250_v01.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/bookcorpus_corpus_gte250_v02.jsonl\n",
      "        new_lang_col_name: language_fasttext_biber\n",
      "        author_docs_count_col_name: author_docs_count\n",
      "        en_lang_val_arr: ['en', 'eng_Latn'] \n",
      "     \n",
      "Read input file to dataframe: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/bookcorpus_corpus_gte250_v01.jsonl with shape: (1275157, 19)\n",
      "Filtered out non english language documents, now with shape: (1203972, 19)\n",
      "Output written to file: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/bookcorpus_corpus_gte250_v02.jsonl\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/bookcorpus_corpus_gte70_v01.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/bookcorpus_corpus_gte70_v02.jsonl\n",
      "        new_lang_col_name: language_fasttext_biber\n",
      "        author_docs_count_col_name: author_docs_count\n",
      "        en_lang_val_arr: ['en', 'eng_Latn'] \n",
      "     \n",
      "Read input file to dataframe: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/bookcorpus_corpus_gte70_v01.jsonl with shape: (1276444, 19)\n",
      "Filtered out non english language documents, now with shape: (1204999, 19)\n",
      "Output written to file: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/bookcorpus_corpus_gte70_v02.jsonl\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/gmane_gte250_v01.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/gmane_gte250_v02.jsonl\n",
      "        new_lang_col_name: language_fasttext_biber\n",
      "        author_docs_count_col_name: author_docs_count\n",
      "        en_lang_val_arr: ['en', 'eng_Latn'] \n",
      "     \n",
      "Read input file to dataframe: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/gmane_gte250_v01.jsonl with shape: (92482, 18)\n",
      "Filtered out non english language documents, now with shape: (92306, 18)\n",
      "Output written to file: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/gmane_gte250_v02.jsonl\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/gmane_gte70_v01.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/gmane_gte70_v02.jsonl\n",
      "        new_lang_col_name: language_fasttext_biber\n",
      "        author_docs_count_col_name: author_docs_count\n",
      "        en_lang_val_arr: ['en', 'eng_Latn'] \n",
      "     \n",
      "Read input file to dataframe: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/gmane_gte70_v01.jsonl with shape: (1038808, 18)\n",
      "Filtered out non english language documents, now with shape: (1036871, 18)\n",
      "Output written to file: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/gmane_gte70_v02.jsonl\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/realnews_gte250_v01.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/realnews_gte250_v02.jsonl\n",
      "        new_lang_col_name: language_fasttext_biber\n",
      "        author_docs_count_col_name: author_docs_count\n",
      "        en_lang_val_arr: ['en', 'eng_Latn'] \n",
      "     \n",
      "Read input file to dataframe: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/realnews_gte250_v01.jsonl with shape: (385191, 18)\n",
      "Filtered out non english language documents, now with shape: (384239, 18)\n",
      "Output written to file: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/realnews_gte250_v02.jsonl\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/realnews_gte70_v01.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/realnews_gte70_v02.jsonl\n",
      "        new_lang_col_name: language_fasttext_biber\n",
      "        author_docs_count_col_name: author_docs_count\n",
      "        en_lang_val_arr: ['en', 'eng_Latn'] \n",
      "     \n",
      "Read input file to dataframe: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/realnews_gte70_v01.jsonl with shape: (425938, 18)\n",
      "Filtered out non english language documents, now with shape: (424623, 18)\n",
      "Output written to file: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/realnews_gte70_v02.jsonl\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/reddit_corpus_gte250_v01.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/reddit_corpus_gte250_v02.jsonl\n",
      "        new_lang_col_name: language_fasttext_biber\n",
      "        author_docs_count_col_name: author_docs_count\n",
      "        en_lang_val_arr: ['en', 'eng_Latn'] \n",
      "     \n",
      "Read input file to dataframe: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/reddit_corpus_gte250_v01.jsonl with shape: (712381, 19)\n",
      "Filtered out non english language documents, now with shape: (701876, 19)\n",
      "Output written to file: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/reddit_corpus_gte250_v02.jsonl\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/reddit_corpus_gte70_v01.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/reddit_corpus_gte70_v02.jsonl\n",
      "        new_lang_col_name: language_fasttext_biber\n",
      "        author_docs_count_col_name: author_docs_count\n",
      "        en_lang_val_arr: ['en', 'eng_Latn'] \n",
      "     \n",
      "Read input file to dataframe: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/reddit_corpus_gte70_v01.jsonl with shape: (7552986, 19)\n",
      "Filtered out non english language documents, now with shape: (7459979, 19)\n",
      "Output written to file: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/reddit_corpus_gte70_v02.jsonl\n"
     ]
    }
   ],
   "source": [
    "v2_filter_out_nonen_upd_auth_docs_count(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/back_gte250_v01.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/back_gte250_v02.jsonl\",\n",
    "    new_lang_col_name=\"language_fasttext_biber\",\n",
    "    author_docs_count_col_name=\"author_docs_count\",\n",
    "    en_lang_val_arr=[\"en\", \"eng_Latn\"],\n",
    ")\n",
    "\n",
    "v2_filter_out_nonen_upd_auth_docs_count(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/back_gte70_v01.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/back_gte70_v02.jsonl\",\n",
    "    new_lang_col_name=\"language_fasttext_biber\",\n",
    "    author_docs_count_col_name=\"author_docs_count\",\n",
    "    en_lang_val_arr=[\"en\", \"eng_Latn\"],\n",
    ")\n",
    "\n",
    "v2_filter_out_nonen_upd_auth_docs_count(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/bookcorpus_corpus_gte250_v01.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/bookcorpus_corpus_gte250_v02.jsonl\",\n",
    "    new_lang_col_name=\"language_fasttext_biber\",\n",
    "    author_docs_count_col_name=\"author_docs_count\",\n",
    "    en_lang_val_arr=[\"en\", \"eng_Latn\"],\n",
    ")\n",
    "\n",
    "v2_filter_out_nonen_upd_auth_docs_count(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/bookcorpus_corpus_gte70_v01.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/bookcorpus_corpus_gte70_v02.jsonl\",\n",
    "    new_lang_col_name=\"language_fasttext_biber\",\n",
    "    author_docs_count_col_name=\"author_docs_count\",\n",
    "    en_lang_val_arr=[\"en\", \"eng_Latn\"],\n",
    ")\n",
    "\n",
    "v2_filter_out_nonen_upd_auth_docs_count(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/gmane_gte250_v01.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/gmane_gte250_v02.jsonl\",\n",
    "    new_lang_col_name=\"language_fasttext_biber\",\n",
    "    author_docs_count_col_name=\"author_docs_count\",\n",
    "    en_lang_val_arr=[\"en\", \"eng_Latn\"],\n",
    ")\n",
    "\n",
    "v2_filter_out_nonen_upd_auth_docs_count(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/gmane_gte70_v01.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/gmane_gte70_v02.jsonl\",\n",
    "    new_lang_col_name=\"language_fasttext_biber\",\n",
    "    author_docs_count_col_name=\"author_docs_count\",\n",
    "    en_lang_val_arr=[\"en\", \"eng_Latn\"],\n",
    ")\n",
    "\n",
    "v2_filter_out_nonen_upd_auth_docs_count(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/realnews_gte250_v01.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/realnews_gte250_v02.jsonl\",\n",
    "    new_lang_col_name=\"language_fasttext_biber\",\n",
    "    author_docs_count_col_name=\"author_docs_count\",\n",
    "    en_lang_val_arr=[\"en\", \"eng_Latn\"],\n",
    ")\n",
    "\n",
    "v2_filter_out_nonen_upd_auth_docs_count(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/realnews_gte70_v01.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/realnews_gte70_v02.jsonl\",\n",
    "    new_lang_col_name=\"language_fasttext_biber\",\n",
    "    author_docs_count_col_name=\"author_docs_count\",\n",
    "    en_lang_val_arr=[\"en\", \"eng_Latn\"],\n",
    ")\n",
    "\n",
    "v2_filter_out_nonen_upd_auth_docs_count(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/reddit_corpus_gte250_v01.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/reddit_corpus_gte250_v02.jsonl\",\n",
    "    new_lang_col_name=\"language_fasttext_biber\",\n",
    "    author_docs_count_col_name=\"author_docs_count\",\n",
    "    en_lang_val_arr=[\"en\", \"eng_Latn\"],\n",
    ")\n",
    "\n",
    "v2_filter_out_nonen_upd_auth_docs_count(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/reddit_corpus_gte70_v01.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/reddit_corpus_gte70_v02.jsonl\",\n",
    "    new_lang_col_name=\"language_fasttext_biber\",\n",
    "    author_docs_count_col_name=\"author_docs_count\",\n",
    "    en_lang_val_arr=[\"en\", \"eng_Latn\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_raw_folder = \"all_data_raw\"\n",
    "all_data_prep_folder = \"all_data_prep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.utils import shuffle\n",
    "# import os\n",
    "\n",
    "# def batch_create_source_doc(\n",
    "#         batch_from_range,\n",
    "#         batch_to_range_exclusive,\n",
    "#         source,\n",
    "#         doc,\n",
    "#         min_author_docs_count,\n",
    "#         min_batch_authors_sample,\n",
    "#         op_folder_name,\n",
    "#         source_doc_file_path=None,\n",
    "#         input_parent_path=f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{all_data_prep_folder}\"\n",
    "#         ):\n",
    "#     print(f\"Called batch_create_source_doc function with the following parameters:\")\n",
    "#     print(f\"\"\"\n",
    "#         batch_from_range: {batch_from_range}\n",
    "#         batch_to_range_exclusive: {batch_to_range_exclusive} \n",
    "#         source: {source}\n",
    "#         doc: {doc}\n",
    "#         min_author_docs_count: {min_author_docs_count}\n",
    "#         min_batch_authors_sample: {min_batch_authors_sample}\n",
    "#         source_doc_file_path: {source_doc_file_path}\n",
    "#      \"\"\")\n",
    "    \n",
    "#     input_file = os.path.join(input_parent_path, f\"{source}_{doc}.jsonl\")\n",
    "#     if source_doc_file_path is not None:\n",
    "#         input_file = source_doc_file_path\n",
    "\n",
    "#     for batch in range(batch_from_range, batch_to_range_exclusive):\n",
    "#         output_file = f\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/{op_folder_name}/{source}_{doc}_batch_{batch:02d}.jsonl\"\n",
    "        \n",
    "#         df_source_doc = pd.read_json(input_file, lines=True)\n",
    "            \n",
    "#         df_source_doc_author_docs_filtered = df_source_doc[df_source_doc['author_docs_count'] >= min_author_docs_count]\n",
    "#         print(f\"\\tFiltered Shape: {df_source_doc_author_docs_filtered.shape}\")\n",
    "        \n",
    "#         df_source_doc_uniq_authors = df_source_doc_author_docs_filtered.drop_duplicates(subset='authorID', keep='first')[['authorID']]\n",
    "#         print(f\"\\tUnique Authors Shape: {df_source_doc_uniq_authors.shape}\")\n",
    "        \n",
    "#         df_source_doc_uniq_authors_shuffled = shuffle(df_source_doc_uniq_authors, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#         # Compute start and end index for slicing\n",
    "#         start_index = (batch - 1) * min_batch_authors_sample\n",
    "#         end_index = start_index + min_batch_authors_sample  # Fetch exactly `min_batch_authors_sample` rows\n",
    "#         df_source_doc_uniq_authors_sample = df_source_doc_uniq_authors_shuffled.iloc[start_index:end_index]\n",
    "#         print(f\"\\tAuthors Sample Shape: {df_source_doc_uniq_authors_sample.shape}\")\n",
    "\n",
    "#         df_source_doc_author_docs_filtered_batch_sample = df_source_doc_author_docs_filtered.merge(df_source_doc_uniq_authors_sample, on='authorID')\n",
    "#         print(f\"\\tMerged Shape: {df_source_doc_author_docs_filtered_batch_sample.shape}\")\n",
    "\n",
    "#         df_source_doc_author_docs_filtered_batch_sample[\"source_of_biber\"] = source\n",
    "#         df_source_doc_author_docs_filtered_batch_sample[\"min_length_words_biber\"] = doc\n",
    "#         df_source_doc_author_docs_filtered_batch_sample[\"batch_biber\"] = batch\n",
    "#         df_source_doc_author_docs_filtered_batch_sample[\"min_author_docs_count_biber\"] = min_author_docs_count\n",
    "#         df_source_doc_author_docs_filtered_batch_sample[\"min_batch_authors_sample_biber\"] = min_batch_authors_sample\n",
    "\n",
    "#         df_source_doc_author_docs_filtered_batch_sample.to_json(output_file, orient='records', lines=True)\n",
    "#         print(f\"Output File: {output_file}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.utils import shuffle\n",
    "# import os\n",
    "\n",
    "# def batch_create_source_doc(\n",
    "#         input_file_path,\n",
    "#         output_file_path,\n",
    "#         batch_from_range,\n",
    "#         batch_to_range_exclusive,\n",
    "#         min_author_docs_count,\n",
    "#         min_batch_authors_sample,\n",
    "#         clip_authors_docs_count=15,\n",
    "#         ):\n",
    "#     print(f\"Called batch_create_source_doc function with the following parameters:\")\n",
    "#     print(f\"\"\"\n",
    "#         input_file_path: {input_file_path}\n",
    "#         output_file_path: {output_file_path}\n",
    "#         batch_from_range: {batch_from_range}\n",
    "#         batch_to_range_exclusive: {batch_to_range_exclusive} \n",
    "#         min_author_docs_count: {min_author_docs_count}\n",
    "#         min_batch_authors_sample: {min_batch_authors_sample}\n",
    "#         clip_authors_docs_count: {clip_authors_docs_count}\n",
    "#      \"\"\")\n",
    "    \n",
    "#     if not os.path.isfile(input_file_path):\n",
    "#         print(f\"Input File: {input_file_path} not present\")\n",
    "#         raise ValueError(f\"Please provide a valid input file path, incorrect: {input_file_path}\")\n",
    "\n",
    "#     for batch in range(batch_from_range, batch_to_range_exclusive):\n",
    "#         df_source_doc = pd.read_json(input_file_path, lines=True, convert_dates=False)\n",
    "        \n",
    "#         df_source_doc_author_docs_filtered = df_source_doc[df_source_doc['author_docs_count'] >= min_author_docs_count]\n",
    "#         print(f\"\\tFiltered Shape: {df_source_doc_author_docs_filtered.shape}\")\n",
    "        \n",
    "#         df_source_doc_uniq_authors = df_source_doc_author_docs_filtered.drop_duplicates(subset='authorID', keep='first')[['authorID']]\n",
    "#         print(f\"\\tUnique Authors Shape: {df_source_doc_uniq_authors.shape}\")\n",
    "        \n",
    "#         df_source_doc_uniq_authors_shuffled = shuffle(df_source_doc_uniq_authors, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#         # Compute start and end index for slicing\n",
    "#         start_index = (batch - 1) * min_batch_authors_sample\n",
    "#         end_index = start_index + min_batch_authors_sample  # Fetch exactly `min_batch_authors_sample` rows\n",
    "#         df_source_doc_uniq_authors_sample = df_source_doc_uniq_authors_shuffled.iloc[start_index:end_index]\n",
    "#         print(f\"\\tAuthors Sample Shape: {df_source_doc_uniq_authors_sample.shape}\")\n",
    "\n",
    "#         df_source_doc_author_docs_filtered_batch_sample = df_source_doc_author_docs_filtered.merge(df_source_doc_uniq_authors_sample, on='authorID')\n",
    "#         print(f\"\\tMerged Shape: {df_source_doc_author_docs_filtered_batch_sample.shape}\")\n",
    "\n",
    "#         ## Clip authors docs to 15, drop other docs for that author if it exceeds 15\n",
    "        \n",
    "\n",
    "#         df_source_doc_author_docs_filtered_batch_sample[\"source_of_biber\"] = source\n",
    "#         df_source_doc_author_docs_filtered_batch_sample[\"min_length_words_biber\"] = doc\n",
    "#         df_source_doc_author_docs_filtered_batch_sample[\"batch_biber\"] = batch\n",
    "#         df_source_doc_author_docs_filtered_batch_sample[\"min_author_docs_count_biber\"] = min_author_docs_count\n",
    "#         df_source_doc_author_docs_filtered_batch_sample[\"min_batch_authors_sample_biber\"] = min_batch_authors_sample\n",
    "\n",
    "#         df_source_doc_author_docs_filtered_batch_sample.to_json(output_file, orient='records', lines=True)\n",
    "#         print(f\"Output File: {output_file}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_create_source_doc(1,2,\"gmane\",\"gte70\",2,2000,\"batch_01_20250312\")\n",
    "# batch_create_source_doc(1,2,\"gmane\",\"gte250\",2,2000,\"batch_01_20250312\")\n",
    "# batch_create_source_doc(1,2,\"realnews\",\"gte70\",2,2000,\"batch_01_20250312\")\n",
    "# batch_create_source_doc(1,2,\"realnews\",\"gte250\",2,2000,\"batch_01_20250312\")\n",
    "# batch_create_source_doc(1,2,\"bookcorpus_corpus\",\"gte70\",10,2000,\"batch_01_20250312\")\n",
    "# batch_create_source_doc(1,2,\"bookcorpus_corpus\",\"gte250\",10,2000,\"batch_01_20250312\")\n",
    "# batch_create_source_doc(1,2,\"reddit_corpus\",\"gte70\",5,2000,\"batch_01_20250312\")\n",
    "# batch_create_source_doc(1,2,\"reddit_corpus\",\"gte250\",5,2000,\"batch_01_20250312\")\n",
    "# batch_create_source_doc(1,2,\"ao3\",\"gte70\",10,2000,\"batch_01_20250312\")\n",
    "# batch_create_source_doc(1,2,\"ao3\",\"gte250\",10,2000,\"batch_01_20250312\")\n",
    "# batch_create_source_doc(1,2,\"back\",\"gte70\",10,2000,\"batch_01_20250312\")\n",
    "# batch_create_source_doc(1,2,\"back\",\"gte250\",10,2000,\"batch_01_20250312\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "\n",
    "def batch_create_source_doc_clip(\n",
    "        input_file_path,\n",
    "        output_file_path,\n",
    "        source,\n",
    "        doc,\n",
    "        batch_from_range,\n",
    "        batch_to_range_exclusive,\n",
    "        min_author_docs_count,\n",
    "        min_batch_authors_sample,\n",
    "        clip_authors_docs_count=15,\n",
    "        ):\n",
    "    print(f\"Called batch_create_source_doc function with the following parameters:\")\n",
    "    print(f\"\"\"\n",
    "        input_file_path: {input_file_path}\n",
    "        output_file_path: {output_file_path}\n",
    "        source: {source}\n",
    "        doc: {doc} \n",
    "        batch_from_range: {batch_from_range}\n",
    "        batch_to_range_exclusive: {batch_to_range_exclusive} \n",
    "        min_author_docs_count: {min_author_docs_count}\n",
    "        min_batch_authors_sample: {min_batch_authors_sample}\n",
    "        clip_authors_docs_count: {clip_authors_docs_count}\n",
    "     \"\"\")\n",
    "    \n",
    "    if not os.path.isfile(input_file_path):\n",
    "        print(f\"Input File: {input_file_path} not present\")\n",
    "        raise ValueError(f\"Please provide a valid input file path, incorrect: {input_file_path}\")\n",
    "    \n",
    "    ## Create parent directory if not present\n",
    "    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "\n",
    "    for batch in range(batch_from_range, batch_to_range_exclusive):\n",
    "        df_source_doc = pd.read_json(input_file_path, lines=True, convert_dates=False)\n",
    "        \n",
    "        df_source_doc_author_docs_filtered = df_source_doc[df_source_doc['author_docs_count'] >= min_author_docs_count]\n",
    "        print(f\"\\tFiltered Shape: {df_source_doc_author_docs_filtered.shape}\")\n",
    "        \n",
    "        df_source_doc_uniq_authors = df_source_doc_author_docs_filtered.drop_duplicates(subset='authorID', keep='first')[['authorID']]\n",
    "        print(f\"\\tUnique Authors Shape: {df_source_doc_uniq_authors.shape}\")\n",
    "        \n",
    "        df_source_doc_uniq_authors_shuffled = shuffle(df_source_doc_uniq_authors, random_state=42).reset_index(drop=True)\n",
    "\n",
    "        # Compute start and end index for slicing\n",
    "        start_index = (batch - 1) * min_batch_authors_sample\n",
    "        end_index = start_index + min_batch_authors_sample  # Fetch exactly `min_batch_authors_sample` rows\n",
    "        df_source_doc_uniq_authors_sample = df_source_doc_uniq_authors_shuffled.iloc[start_index:end_index]\n",
    "        print(f\"\\tAuthors Sample Shape: {df_source_doc_uniq_authors_sample.shape}\")\n",
    "\n",
    "        df_source_doc_author_docs_filtered_batch_sample = df_source_doc_author_docs_filtered.merge(df_source_doc_uniq_authors_sample, on='authorID')\n",
    "        print(f\"\\tMerged Shape: {df_source_doc_author_docs_filtered_batch_sample.shape}\")\n",
    "\n",
    "        ## Clip authors docs to clip_authors_docs_count (e.g. 15): for each author, if they have more than 15 docs,\n",
    "        ## reproducibly randomly select 15; otherwise, keep them all.\n",
    "        df_source_doc_author_docs_filtered_batch_sample = df_source_doc_author_docs_filtered_batch_sample.groupby('authorID', group_keys=False).apply(\n",
    "            lambda group: group.sample(n=clip_authors_docs_count, random_state=42) if len(group) > clip_authors_docs_count else group\n",
    "        )\n",
    "        print(f\"\\tAfter clipping: {df_source_doc_author_docs_filtered_batch_sample.shape}\")\n",
    "        \n",
    "        # Assuming source, doc, and output_file are defined externally or globally\n",
    "        df_source_doc_author_docs_filtered_batch_sample[\"source_of_biber\"] = source\n",
    "        df_source_doc_author_docs_filtered_batch_sample[\"min_length_words_biber\"] = doc\n",
    "        df_source_doc_author_docs_filtered_batch_sample[\"batch_biber\"] = batch\n",
    "        df_source_doc_author_docs_filtered_batch_sample[\"min_author_docs_count_biber\"] = min_author_docs_count\n",
    "        df_source_doc_author_docs_filtered_batch_sample[\"min_batch_authors_sample_biber\"] = min_batch_authors_sample\n",
    "\n",
    "        df_source_doc_author_docs_filtered_batch_sample.to_json(output_file_path, orient='records', lines=True)\n",
    "        print(f\"Output File: {output_file_path}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called batch_create_source_doc function with the following parameters:\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/back_gte250_v02.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/back_gte250_batch_01.jsonl\n",
      "        source: back_v02_biber\n",
      "        doc: gte250 \n",
      "        batch_from_range: 1\n",
      "        batch_to_range_exclusive: 2 \n",
      "        min_author_docs_count: 10\n",
      "        min_batch_authors_sample: 2000\n",
      "        clip_authors_docs_count: 15\n",
      "     \n",
      "\tFiltered Shape: (134854, 11)\n",
      "\tUnique Authors Shape: (3976, 1)\n",
      "\tAuthors Sample Shape: (2000, 1)\n",
      "\tMerged Shape: (66552, 11)\n",
      "\tAfter clipping: (27877, 11)\n",
      "Output File: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/back_gte250_batch_01.jsonl\n",
      "\n",
      "\n",
      "Called batch_create_source_doc function with the following parameters:\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/back_gte70_v02.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/back_gte70_batch_01.jsonl\n",
      "        source: back_v02_biber\n",
      "        doc: gte70 \n",
      "        batch_from_range: 1\n",
      "        batch_to_range_exclusive: 2 \n",
      "        min_author_docs_count: 10\n",
      "        min_batch_authors_sample: 2000\n",
      "        clip_authors_docs_count: 15\n",
      "     \n",
      "\tFiltered Shape: (373756, 11)\n",
      "\tUnique Authors Shape: (8027, 1)\n",
      "\tAuthors Sample Shape: (2000, 1)\n",
      "\tMerged Shape: (91009, 11)\n",
      "\tAfter clipping: (28039, 11)\n",
      "Output File: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/back_gte70_batch_01.jsonl\n",
      "\n",
      "\n",
      "Called batch_create_source_doc function with the following parameters:\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/realnews_gte250_v02.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/realnews_gte250_batch_01.jsonl\n",
      "        source: realnews_v02_biber\n",
      "        doc: gte250 \n",
      "        batch_from_range: 1\n",
      "        batch_to_range_exclusive: 2 \n",
      "        min_author_docs_count: 2\n",
      "        min_batch_authors_sample: 2000\n",
      "        clip_authors_docs_count: 15\n",
      "     \n",
      "\tFiltered Shape: (357974, 18)\n",
      "\tUnique Authors Shape: (178987, 1)\n",
      "\tAuthors Sample Shape: (2000, 1)\n",
      "\tMerged Shape: (4000, 18)\n",
      "\tAfter clipping: (4000, 18)\n",
      "Output File: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/realnews_gte250_batch_01.jsonl\n",
      "\n",
      "\n",
      "Called batch_create_source_doc function with the following parameters:\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/realnews_gte70_v02.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/realnews_gte70_batch_01.jsonl\n",
      "        source: realnews_v02_biber\n",
      "        doc: gte70 \n",
      "        batch_from_range: 1\n",
      "        batch_to_range_exclusive: 2 \n",
      "        min_author_docs_count: 2\n",
      "        min_batch_authors_sample: 2000\n",
      "        clip_authors_docs_count: 15\n",
      "     \n",
      "\tFiltered Shape: (423936, 18)\n",
      "\tUnique Authors Shape: (211968, 1)\n",
      "\tAuthors Sample Shape: (2000, 1)\n",
      "\tMerged Shape: (4000, 18)\n",
      "\tAfter clipping: (4000, 18)\n",
      "Output File: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/realnews_gte70_batch_01.jsonl\n",
      "\n",
      "\n",
      "Called batch_create_source_doc function with the following parameters:\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/bookcorpus_corpus_gte250_v02.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/bookcorpus_corpus_gte250_batch_01.jsonl\n",
      "        source: bookcorpus_corpus_v02_biber\n",
      "        doc: gte250 \n",
      "        batch_from_range: 1\n",
      "        batch_to_range_exclusive: 2 \n",
      "        min_author_docs_count: 10\n",
      "        min_batch_authors_sample: 2000\n",
      "        clip_authors_docs_count: 15\n",
      "     \n",
      "\tFiltered Shape: (1145337, 19)\n",
      "\tUnique Authors Shape: (56514, 1)\n",
      "\tAuthors Sample Shape: (2000, 1)\n",
      "\tMerged Shape: (42166, 19)\n",
      "\tAfter clipping: (23106, 19)\n",
      "Output File: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/bookcorpus_corpus_gte250_batch_01.jsonl\n",
      "\n",
      "\n",
      "Called batch_create_source_doc function with the following parameters:\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/bookcorpus_corpus_gte70_v02.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/bookcorpus_corpus_gte70_batch_01.jsonl\n",
      "        source: bookcorpus_corpus_v02_biber\n",
      "        doc: gte70 \n",
      "        batch_from_range: 1\n",
      "        batch_to_range_exclusive: 2 \n",
      "        min_author_docs_count: 10\n",
      "        min_batch_authors_sample: 2000\n",
      "        clip_authors_docs_count: 15\n",
      "     \n",
      "\tFiltered Shape: (1148320, 19)\n",
      "\tUnique Authors Shape: (56780, 1)\n",
      "\tAuthors Sample Shape: (2000, 1)\n",
      "\tMerged Shape: (39709, 19)\n",
      "\tAfter clipping: (22930, 19)\n",
      "Output File: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/bookcorpus_corpus_gte70_batch_01.jsonl\n",
      "\n",
      "\n",
      "Called batch_create_source_doc function with the following parameters:\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/reddit_corpus_gte250_v02.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/reddit_corpus_gte250_batch_01.jsonl\n",
      "        source: reddit_corpus_v02_biber\n",
      "        doc: gte250 \n",
      "        batch_from_range: 1\n",
      "        batch_to_range_exclusive: 2 \n",
      "        min_author_docs_count: 5\n",
      "        min_batch_authors_sample: 2000\n",
      "        clip_authors_docs_count: 15\n",
      "     \n",
      "\tFiltered Shape: (4970, 19)\n",
      "\tUnique Authors Shape: (994, 1)\n",
      "\tAuthors Sample Shape: (994, 1)\n",
      "\tMerged Shape: (4970, 19)\n",
      "\tAfter clipping: (4970, 19)\n",
      "Output File: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/reddit_corpus_gte250_batch_01.jsonl\n",
      "\n",
      "\n",
      "Called batch_create_source_doc function with the following parameters:\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/reddit_corpus_gte70_v02.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/reddit_corpus_gte70_batch_01.jsonl\n",
      "        source: reddit_corpus_v02_biber\n",
      "        doc: gte70 \n",
      "        batch_from_range: 1\n",
      "        batch_to_range_exclusive: 2 \n",
      "        min_author_docs_count: 5\n",
      "        min_batch_authors_sample: 2000\n",
      "        clip_authors_docs_count: 15\n",
      "     \n",
      "\tFiltered Shape: (1383657, 19)\n",
      "\tUnique Authors Shape: (276731, 1)\n",
      "\tAuthors Sample Shape: (2000, 1)\n",
      "\tMerged Shape: (10000, 19)\n",
      "\tAfter clipping: (10000, 19)\n",
      "Output File: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/reddit_corpus_gte70_batch_01.jsonl\n",
      "\n",
      "\n",
      "Called batch_create_source_doc function with the following parameters:\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/gmane_gte250_v02.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/gmane_gte250_batch_01.jsonl\n",
      "        source: gmane_v02_biber\n",
      "        doc: gte250 \n",
      "        batch_from_range: 1\n",
      "        batch_to_range_exclusive: 2 \n",
      "        min_author_docs_count: 2\n",
      "        min_batch_authors_sample: 2000\n",
      "        clip_authors_docs_count: 15\n",
      "     \n",
      "\tFiltered Shape: (20202, 18)\n",
      "\tUnique Authors Shape: (10101, 1)\n",
      "\tAuthors Sample Shape: (2000, 1)\n",
      "\tMerged Shape: (4000, 18)\n",
      "\tAfter clipping: (4000, 18)\n",
      "Output File: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/gmane_gte250_batch_01.jsonl\n",
      "\n",
      "\n",
      "Called batch_create_source_doc function with the following parameters:\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/gmane_gte70_v02.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/gmane_gte70_batch_01.jsonl\n",
      "        source: gmane_v02_biber\n",
      "        doc: gte70 \n",
      "        batch_from_range: 1\n",
      "        batch_to_range_exclusive: 2 \n",
      "        min_author_docs_count: 2\n",
      "        min_batch_authors_sample: 2000\n",
      "        clip_authors_docs_count: 15\n",
      "     \n",
      "\tFiltered Shape: (840876, 18)\n",
      "\tUnique Authors Shape: (420438, 1)\n",
      "\tAuthors Sample Shape: (2000, 1)\n",
      "\tMerged Shape: (4000, 18)\n",
      "\tAfter clipping: (4000, 18)\n",
      "Output File: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/gmane_gte70_batch_01.jsonl\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For back_gte250_v02.jsonl\n",
    "batch_create_source_doc_clip(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/back_gte250_v02.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/back_gte250_batch_01.jsonl\",\n",
    "    source=\"back_v02_biber\",\n",
    "    doc=\"gte250\",\n",
    "    batch_from_range=1,\n",
    "    batch_to_range_exclusive=2,\n",
    "    min_author_docs_count=10,\n",
    "    min_batch_authors_sample=2000,\n",
    "    clip_authors_docs_count=15,\n",
    ")\n",
    "\n",
    "# For back_gte70_v02.jsonl\n",
    "batch_create_source_doc_clip(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/back_gte70_v02.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/back_gte70_batch_01.jsonl\",\n",
    "    source=\"back_v02_biber\",\n",
    "    doc=\"gte70\",\n",
    "    batch_from_range=1,\n",
    "    batch_to_range_exclusive=2,\n",
    "    min_author_docs_count=10,\n",
    "    min_batch_authors_sample=2000,\n",
    "    clip_authors_docs_count=15,\n",
    ")\n",
    "\n",
    "# For realnews_gte250_v02.jsonl\n",
    "batch_create_source_doc_clip(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/realnews_gte250_v02.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/realnews_gte250_batch_01.jsonl\",\n",
    "    source=\"realnews_v02_biber\",\n",
    "    doc=\"gte250\",\n",
    "    batch_from_range=1,\n",
    "    batch_to_range_exclusive=2,\n",
    "    min_author_docs_count=2,\n",
    "    min_batch_authors_sample=2000,\n",
    "    clip_authors_docs_count=15,\n",
    ")\n",
    "\n",
    "# For realnews_gte70_v02.jsonl\n",
    "batch_create_source_doc_clip(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/realnews_gte70_v02.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/realnews_gte70_batch_01.jsonl\",\n",
    "    source=\"realnews_v02_biber\",\n",
    "    doc=\"gte70\",\n",
    "    batch_from_range=1,\n",
    "    batch_to_range_exclusive=2,\n",
    "    min_author_docs_count=2,\n",
    "    min_batch_authors_sample=2000,\n",
    "    clip_authors_docs_count=15,\n",
    ")\n",
    "\n",
    "# For bookcorpus_corpus_gte250_v02.jsonl\n",
    "batch_create_source_doc_clip(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/bookcorpus_corpus_gte250_v02.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/bookcorpus_corpus_gte250_batch_01.jsonl\",\n",
    "    source=\"bookcorpus_corpus_v02_biber\",\n",
    "    doc=\"gte250\",\n",
    "    batch_from_range=1,\n",
    "    batch_to_range_exclusive=2,\n",
    "    min_author_docs_count=10,\n",
    "    min_batch_authors_sample=2000,\n",
    "    clip_authors_docs_count=15,\n",
    ")\n",
    "\n",
    "# For bookcorpus_corpus_gte70_v02.jsonl\n",
    "batch_create_source_doc_clip(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/bookcorpus_corpus_gte70_v02.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/bookcorpus_corpus_gte70_batch_01.jsonl\",\n",
    "    source=\"bookcorpus_corpus_v02_biber\",\n",
    "    doc=\"gte70\",\n",
    "    batch_from_range=1,\n",
    "    batch_to_range_exclusive=2,\n",
    "    min_author_docs_count=10,\n",
    "    min_batch_authors_sample=2000,\n",
    "    clip_authors_docs_count=15,\n",
    ")\n",
    "\n",
    "# For reddit_corpus_gte250_v02.jsonl\n",
    "batch_create_source_doc_clip(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/reddit_corpus_gte250_v02.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/reddit_corpus_gte250_batch_01.jsonl\",\n",
    "    source=\"reddit_corpus_v02_biber\",\n",
    "    doc=\"gte250\",\n",
    "    batch_from_range=1,\n",
    "    batch_to_range_exclusive=2,\n",
    "    min_author_docs_count=5,\n",
    "    min_batch_authors_sample=2000,\n",
    "    clip_authors_docs_count=15,\n",
    ")\n",
    "\n",
    "# For reddit_corpus_gte70_v02.jsonl\n",
    "batch_create_source_doc_clip(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/reddit_corpus_gte70_v02.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/reddit_corpus_gte70_batch_01.jsonl\",\n",
    "    source=\"reddit_corpus_v02_biber\",\n",
    "    doc=\"gte70\",\n",
    "    batch_from_range=1,\n",
    "    batch_to_range_exclusive=2,\n",
    "    min_author_docs_count=5,\n",
    "    min_batch_authors_sample=2000,\n",
    "    clip_authors_docs_count=15,\n",
    ")\n",
    "\n",
    "# For gmane_gte250_v02.jsonl\n",
    "batch_create_source_doc_clip(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/gmane_gte250_v02.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/gmane_gte250_batch_01.jsonl\",\n",
    "    source=\"gmane_v02_biber\",\n",
    "    doc=\"gte250\",\n",
    "    batch_from_range=1,\n",
    "    batch_to_range_exclusive=2,\n",
    "    min_author_docs_count=2,\n",
    "    min_batch_authors_sample=2000,\n",
    "    clip_authors_docs_count=15,\n",
    ")\n",
    "\n",
    "# For gmane_gte70_v02.jsonl\n",
    "batch_create_source_doc_clip(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/gmane_gte70_v02.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/gmane_gte70_batch_01.jsonl\",\n",
    "    source=\"gmane_v02_biber\",\n",
    "    doc=\"gte70\",\n",
    "    batch_from_range=1,\n",
    "    batch_to_range_exclusive=2,\n",
    "    min_author_docs_count=2,\n",
    "    min_batch_authors_sample=2000,\n",
    "    clip_authors_docs_count=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called batch_create_source_doc function with the following parameters:\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/ao3_gte250_v02.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/ao3_gte250_batch_01.jsonl\n",
      "        source: ao3_v02_biber\n",
      "        doc: gte250 \n",
      "        batch_from_range: 1\n",
      "        batch_to_range_exclusive: 2 \n",
      "        min_author_docs_count: 10\n",
      "        min_batch_authors_sample: 2000\n",
      "        clip_authors_docs_count: 15\n",
      "     \n",
      "\tFiltered Shape: (9177894, 18)\n",
      "\tUnique Authors Shape: (74753, 1)\n",
      "\tAuthors Sample Shape: (2000, 1)\n",
      "\tMerged Shape: (244242, 18)\n",
      "\tAfter clipping: (29092, 18)\n",
      "Output File: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/ao3_gte250_batch_01.jsonl\n",
      "\n",
      "\n",
      "Called batch_create_source_doc function with the following parameters:\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/ao3_gte70_v02.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/ao3_gte70_batch_01.jsonl\n",
      "        source: ao3_v02_biber\n",
      "        doc: gte70 \n",
      "        batch_from_range: 1\n",
      "        batch_to_range_exclusive: 2 \n",
      "        min_author_docs_count: 10\n",
      "        min_batch_authors_sample: 2000\n",
      "        clip_authors_docs_count: 15\n",
      "     \n",
      "\tFiltered Shape: (9224110, 18)\n",
      "\tUnique Authors Shape: (75082, 1)\n",
      "\tAuthors Sample Shape: (2000, 1)\n",
      "\tMerged Shape: (241318, 18)\n",
      "\tAfter clipping: (29155, 18)\n",
      "Output File: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/ao3_gte70_batch_01.jsonl\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For ao3_gte250_v02.jsonl\n",
    "batch_create_source_doc_clip(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/ao3_gte250_v02.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/ao3_gte250_batch_01.jsonl\",\n",
    "    source=\"ao3_v02_biber\",\n",
    "    doc=\"gte250\",\n",
    "    batch_from_range=1,\n",
    "    batch_to_range_exclusive=2,\n",
    "    min_author_docs_count=10,\n",
    "    min_batch_authors_sample=2000,\n",
    "    clip_authors_docs_count=15,\n",
    ")\n",
    "\n",
    "# For ao3_gte70_v02.jsonl\n",
    "batch_create_source_doc_clip(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/ao3_gte70_v02.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/ao3_gte70_batch_01.jsonl\",\n",
    "    source=\"ao3_v02_biber\",\n",
    "    doc=\"gte70\",\n",
    "    batch_from_range=1,\n",
    "    batch_to_range_exclusive=2,\n",
    "    min_author_docs_count=10,\n",
    "    min_batch_authors_sample=2000,\n",
    "    clip_authors_docs_count=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called batch_create_source_doc function with the following parameters:\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/stackexchange_gte250_v02.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/stackexchange_gte250_batch_01.jsonl\n",
      "        source: stackexchange_v02_biber\n",
      "        doc: gte250 \n",
      "        batch_from_range: 1\n",
      "        batch_to_range_exclusive: 2 \n",
      "        min_author_docs_count: 10\n",
      "        min_batch_authors_sample: 2000\n",
      "        clip_authors_docs_count: 15\n",
      "     \n",
      "\tFiltered Shape: (1846535, 22)\n",
      "\tUnique Authors Shape: (38112, 1)\n",
      "\tAuthors Sample Shape: (2000, 1)\n",
      "\tMerged Shape: (86816, 22)\n",
      "\tAfter clipping: (27635, 22)\n",
      "Output File: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/stackexchange_gte250_batch_01.jsonl\n",
      "\n",
      "\n",
      "Called batch_create_source_doc function with the following parameters:\n",
      "\n",
      "        input_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/stackexchange_gte70_v02.jsonl\n",
      "        output_file_path: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/stackexchange_gte70_batch_01.jsonl\n",
      "        source: stackexchange_v02_biber\n",
      "        doc: gte70 \n",
      "        batch_from_range: 1\n",
      "        batch_to_range_exclusive: 2 \n",
      "        min_author_docs_count: 10\n",
      "        min_batch_authors_sample: 2000\n",
      "        clip_authors_docs_count: 15\n",
      "     \n",
      "\tFiltered Shape: (14548511, 22)\n",
      "\tUnique Authors Shape: (247197, 1)\n",
      "\tAuthors Sample Shape: (2000, 1)\n",
      "\tMerged Shape: (112080, 22)\n",
      "\tAfter clipping: (28170, 22)\n",
      "Output File: /data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/stackexchange_gte70_batch_01.jsonl\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For stackexchange_gte250_v02.jsonl\n",
    "batch_create_source_doc_clip(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/stackexchange_gte250_v02.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/stackexchange_gte250_batch_01.jsonl\",\n",
    "    source=\"stackexchange_v02_biber\",\n",
    "    doc=\"gte250\",\n",
    "    batch_from_range=1,\n",
    "    batch_to_range_exclusive=2,\n",
    "    min_author_docs_count=10,\n",
    "    min_batch_authors_sample=2000,\n",
    "    clip_authors_docs_count=15,\n",
    ")\n",
    "\n",
    "# For stackexchange_gte70_v02.jsonl\n",
    "batch_create_source_doc_clip(\n",
    "    input_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/all_data_prep/stackexchange_gte70_v02.jsonl\",\n",
    "    output_file_path=\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/batch_01_20250314/stackexchange_gte70_batch_01.jsonl\",\n",
    "    source=\"stackexchange_v02_biber\",\n",
    "    doc=\"gte70\",\n",
    "    batch_from_range=1,\n",
    "    batch_to_range_exclusive=2,\n",
    "    min_author_docs_count=10,\n",
    "    min_batch_authors_sample=2000,\n",
    "    clip_authors_docs_count=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.quantization import semantic_search_faiss\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'auth':['a1','a2','a2','a1','a1','a5'], 'docID':[f\"d{x:02d}\" for x in range(6)], 'docText':['haha', 'hehe', 'a a a', 'brr', 'ssss', 'kimono aa lola']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_name: str = \"gabrielloiseau/LUAR-MUD-sentence-transformers\"\n",
    "genre_model_name: str = \"classla/xlm-roberta-base-multilingual-text-genre-classifier\"\n",
    "embed_model = SentenceTransformer(embed_model_name, device=\"cuda\")\n",
    "genre_classifier = pipeline(\"text-classification\",model=genre_model_name,device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_author = df.groupby('auth').agg({'docText':list})['docText'].to_dict()\n",
    "data_by_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute Embeddings of every doc in df\n",
    "## Classify Genre for every doc in df\n",
    "## outer join based on authorID\n",
    "## Discard those rows where the two documents are same\n",
    "## Calculate document similarity of left and right\n",
    "## Discard those that have similarity > threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_pairs = []\n",
    "# Progress bar for mining hard positives over authors.\n",
    "for author, docs in data_by_author.items():\n",
    "    if len(docs) <= 1:\n",
    "        continue\n",
    "    print(f\"docs: {docs}\")\n",
    "    embeddings = embed_model.encode(docs, convert_to_tensor=False)\n",
    "    print(len(embeddings), len(embeddings[0]))\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    print(f\"sim_matrix: {sim_matrix}\")\n",
    "    n = len(docs)\n",
    "            \n",
    "    # For each anchor, gather candidate indices with similarity below threshold.\n",
    "    hard_candidates = {}\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if sim_matrix[i, j] < 0.9:\n",
    "                hard_candidates.setdefault(i, []).append(j)\n",
    "    \n",
    "    print(f\"hard_candidates: {hard_candidates}\")\n",
    "\n",
    "    # Compute the genre for each document.\n",
    "    doc_genres = []\n",
    "    for doc in docs:\n",
    "        classification = genre_classifier(doc, truncation=True)\n",
    "        genre = classification[0][\"label\"]\n",
    "        doc_genres.append(genre)\n",
    "\n",
    "    print(f\"doc_genres: {doc_genres}\")\n",
    "\n",
    "    # For each anchor with candidate positives, choose one candidate with a different genre.\n",
    "    for anchor_idx, candidates in hard_candidates.items():\n",
    "        print(f\"anchor_idx: {anchor_idx}, candidates: {candidates}\")\n",
    "        anchor_genre = doc_genres[anchor_idx]\n",
    "        valid_candidates = [cand for cand in candidates if doc_genres[cand] != anchor_genre]\n",
    "        if valid_candidates:\n",
    "            chosen_positive_idx = random.choice(valid_candidates)\n",
    "            hard_pairs.append({\n",
    "                \"author\": author,\n",
    "                \"anchor\": docs[anchor_idx],\n",
    "                \"positive\": docs[chosen_positive_idx]\n",
    "            })\n",
    "    print(f\"hard_pairs: {hard_pairs}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = df.copy()\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['embeddings'] = dfc['docText'].apply(lambda x: embed_model.encode(x))\n",
    "dfc['genre'] = dfc['docText'].apply(lambda x: genre_classifier(x, truncation=True)[0][\"label\"])\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcm = dfc.merge(dfc, how='outer', on='auth', suffixes=[\"_anchor\", \"_positive\"])\n",
    "dfcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcm = dfcm[(dfcm['docID_anchor'] != dfcm['docID_positive']) & (dfcm['genre_anchor'] != dfcm['genre_positive'])]\n",
    "dfcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexicographically sort each docID pair (unordered) using NumPy\n",
    "doc_min = np.minimum(dfcm['docID_anchor'], dfcm['docID_positive'])\n",
    "doc_max = np.maximum(dfcm['docID_anchor'], dfcm['docID_positive'])\n",
    "\n",
    "print(doc_min)\n",
    "print(doc_max)\n",
    "# Create a unique key per (auth, unordered docID pair)\n",
    "dfcm['pair_key'] = dfcm['auth'].astype(str) + '__' + doc_min + '__' + doc_max\n",
    "dfcm = dfcm.drop_duplicates(subset='pair_key').drop(columns='pair_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcm['similarity_score'] = dfcm.apply(lambda x: cosine_similarity(x['embeddings_anchor'].reshape(1, -1), x['embeddings_positive'].reshape(1, -1))[0][0], axis=1)\n",
    "dfcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcm = dfcm[dfcm['similarity_score'] < 0.9]\n",
    "dfcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcm.groupby(['auth', 'docID_anchor']).sample(n=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combined Code\n",
    "embed_model_name: str = \"gabrielloiseau/LUAR-MUD-sentence-transformers\"\n",
    "genre_model_name: str = \"classla/xlm-roberta-base-multilingual-text-genre-classifier\"\n",
    "embed_model = SentenceTransformer(embed_model_name, device=\"cuda\")\n",
    "genre_classifier = pipeline(\"text-classification\",model=genre_model_name,device=\"cuda\")\n",
    "\n",
    "df = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/ta2_jan_2025_trian_data/train_sadiri_processed_with_embeddings_wo_ao3_filtered.jsonl\", lines=True)\n",
    "# df = pd.DataFrame({'auth':['a1','a2','a2','a1','a1','a5'], 'docID':[f\"d{x:02d}\" for x in range(6)], 'docText':['haha', 'hehe', 'a a a', 'brr', 'ssss', 'kimono aa lola']})\n",
    "dfc = df.copy()\n",
    "\n",
    "dfc['embeddings'] = dfc['docText'].apply(lambda x: embed_model.encode(x))\n",
    "dfc['genre'] = dfc['docText'].apply(lambda x: genre_classifier(x, truncation=True)[0][\"label\"])\n",
    "\n",
    "dfcm = dfc.merge(dfc, how='outer', on='auth', suffixes=[\"_anchor\", \"_positive\"])\n",
    "\n",
    "dfcm = dfcm[(dfcm['docID_anchor'] != dfcm['docID_positive']) & (dfcm['genre_anchor'] != dfcm['genre_positive'])]\n",
    "\n",
    "# Lexicographically sort each docID pair (unordered) using NumPy\n",
    "doc_min = np.minimum(dfcm['docID_anchor'], dfcm['docID_positive'])\n",
    "doc_max = np.maximum(dfcm['docID_anchor'], dfcm['docID_positive'])\n",
    "\n",
    "# Create a unique key per (auth, unordered docID pair)\n",
    "dfcm['pair_key'] = dfcm['auth'].astype(str) + '__' + doc_min + '__' + doc_max\n",
    "dfcm = dfcm.drop_duplicates(subset='pair_key').drop(columns='pair_key')\n",
    "\n",
    "dfcm['similarity_score'] = dfcm.apply(lambda x: cosine_similarity(x['embeddings_anchor'].reshape(1, -1), x['embeddings_positive'].reshape(1, -1))[0][0], axis=1)\n",
    "dfcm = dfcm[dfcm['similarity_score'] < 0.9]\n",
    "fin_ans = dfcm.groupby(['auth', 'docID_anchor']).sample(n=1, random_state=42)\n",
    "fin_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "# -----------------------------\n",
    "# Configs\n",
    "# -----------------------------\n",
    "embed_model_name = \"gabrielloiseau/LUAR-MUD-sentence-transformers\"\n",
    "doc_genre_model_name = \"classla/xlm-roberta-base-multilingual-text-genre-classifier\"\n",
    "data_path = \"/data/araghavan/HIATUS/datadreamer-ta2/data/ta2_jan_2025_trian_data/train_sadiri_processed_with_embeddings_wo_ao3_filtered.jsonl\"\n",
    "batch_size = 2048\n",
    "ceiling_threshold = 0.4\n",
    "print(os.path.isfile(data_path))\n",
    "# -----------------------------\n",
    "# Load Models\n",
    "# -----------------------------\n",
    "embed_model = SentenceTransformer(embed_model_name, device=\"cuda\")\n",
    "doc_genre_classifier = pipeline(\"text-classification\", model=doc_genre_model_name, device=\"cuda\")\n",
    "\n",
    "# -----------------------------\n",
    "# Batched Processing Function\n",
    "# -----------------------------\n",
    "def batched_process(texts, embed_model, doc_genre_classifier, batch_size=64):\n",
    "    embeddings = []\n",
    "    genres = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "\n",
    "        # Embedding generation\n",
    "        batch_embeds = embed_model.encode(batch, convert_to_numpy=True, batch_size=batch_size)\n",
    "        embeddings.extend(batch_embeds)\n",
    "\n",
    "        # Genre classification\n",
    "        batch_genres = doc_genre_classifier(batch, truncation=True, batch_size=batch_size)\n",
    "        genres.extend([g[\"label\"] for g in batch_genres])\n",
    "\n",
    "    return np.array(embeddings), genres\n",
    "\n",
    "# -----------------------------\n",
    "# Load Data\n",
    "# -----------------------------\n",
    "df = pd.read_json(data_path, lines=True)\n",
    "print(f\"Read JSON: {data_path} with shape: {df.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Apply Batched Embedding + Genre Inference\n",
    "# -----------------------------\n",
    "texts = df[\"fullText\"].tolist()\n",
    "embeddings, genres = batched_process(texts, embed_model, doc_genre_classifier, batch_size=batch_size)\n",
    "print(f\"Generated embeddings and genre for all docs with shape: {embeddings.shape}\")\n",
    "\n",
    "df[\"doc_embedding\"] = list(embeddings)  # Keep embeddings as list of numpy arrays\n",
    "df[\"doc_genre\"] = genres\n",
    "\n",
    "# -----------------------------\n",
    "# Create Anchor-Positive Pairs (Within Author)\n",
    "# -----------------------------\n",
    "dfcm = df.merge(df, how=\"outer\", on=\"authorID\", suffixes=[\"_anchor\", \"_positive\"])\n",
    "print(f\"Merged Dataframe to generate author anchor-positive pairs\")\n",
    "print(f\"Now with shape: {dfcm.shape}\")\n",
    "\n",
    "# Filter out self-pairs and same-genre pairs\n",
    "dfcm = dfcm.loc[\n",
    "    (dfcm[\"documentID_anchor\"] != dfcm[\"documentID_positive\"]) &\n",
    "    (dfcm[\"doc_genre_anchor\"] != dfcm[\"doc_genre_positive\"])\n",
    "]\n",
    "print(f\"Filtered same anchor-positive pair documents, same genre anchor-positive pair documents\")\n",
    "print(f\"Now with shape: {dfcm.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Deduplicate Unordered Pairs per Author\n",
    "# -----------------------------\n",
    "doc_min = np.minimum(dfcm[\"documentID_anchor\"], dfcm[\"documentID_positive\"])\n",
    "doc_max = np.maximum(dfcm[\"documentID_anchor\"], dfcm[\"documentID_positive\"])\n",
    "# dfcm[\"pair_key\"] = dfcm[\"authorID\"] + \"__\" + doc_min + \"__\" + doc_max\n",
    "dfcm.loc[:, \"pair_key\"] = (\n",
    "    dfcm[\"authorID\"].astype(str) + \"__\" +\n",
    "    doc_min.astype(str) + \"__\" +\n",
    "    doc_max.astype(str)\n",
    ")\n",
    "dfcm = dfcm.drop_duplicates(subset=\"pair_key\").drop(columns=\"pair_key\")\n",
    "print(f\"Dedup anchor-positive pairs, now with shape: {dfcm.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Compute Cosine Similarity (Vectorized)\n",
    "# -----------------------------\n",
    "anchor_embeddings = np.stack(dfcm[\"doc_embedding_anchor\"].values)\n",
    "\n",
    "positive_embeddings = np.stack(dfcm[\"doc_embedding_positive\"].values)\n",
    "print(f\"Generated pairwise similarity of anchor-positive pairs from their respective embeddings\")\n",
    "\n",
    "dfcm[\"similarity_score\"] = np.diag(cosine_similarity(anchor_embeddings, positive_embeddings))\n",
    "print(f\"Now with shape: {dfcm.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Filter Low-Similarity Pairs\n",
    "# -----------------------------\n",
    "initial_count = dfcm.shape[0]\n",
    "dfcm = dfcm.loc[dfcm[\"similarity_score\"] < ceiling_threshold]\n",
    "dfcm = dfcm.reset_index(drop=True)\n",
    "print(f\"Hard positive filtering of considering anchor-positive pairs below threshold of: {ceiling_threshold}\")\n",
    "print(f\"Dropped {initial_count - dfcm.shape[0]} pairs above similarity threshold\")\n",
    "print(f\"Now with shape: {dfcm.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Sample One Positive per Anchor per Author\n",
    "# -----------------------------\n",
    "fin_ans = dfcm.groupby([\"authorID\", \"documentID_anchor\"], group_keys=False).sample(n=1, random_state=42)\n",
    "print(f\"Grouped author, anchors and sampled candidates to generate auth-anchor-positive pairs\")\n",
    "\n",
    "# -----------------------------\n",
    "# Final Output\n",
    "# -----------------------------\n",
    "print(\"Final sampled result shape:\", fin_ans.shape)\n",
    "# Save or return as needed\n",
    "# fin_ans.to_json(\"final_pairs.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # For each anchor, gather candidate indices with similarity below threshold.\n",
    "    hard_candidates = {}\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if sim_matrix[i, j] < self.ceiling_threshold:\n",
    "                hard_candidates.setdefault(i, []).append(j)\n",
    "\n",
    "    # Compute the genre for each document.\n",
    "    doc_genres = []\n",
    "    for doc in docs:\n",
    "        classification = self.genre_classifier(doc, truncation=True)\n",
    "        genre = classification[0][\"label\"]\n",
    "        doc_genres.append(genre)\n",
    "\n",
    "    # For each anchor with candidate positives, choose one candidate with a different genre.\n",
    "    for anchor_idx, candidates in hard_candidates.items():\n",
    "        anchor_genre = doc_genres[anchor_idx]\n",
    "        valid_candidates = [cand for cand in candidates if doc_genres[cand] != anchor_genre]\n",
    "        if valid_candidates:\n",
    "            chosen_positive_idx = random.choice(valid_candidates)\n",
    "            hard_pairs.append({\n",
    "                \"author\": author,\n",
    "                \"anchor\": docs[anchor_idx],\n",
    "                \"positive\": docs[chosen_positive_idx]\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

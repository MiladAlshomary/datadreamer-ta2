{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every visited = False hard positive pair: X(author Xa, anchor Xda, positive Xdp)\n",
    "#     query -> Xda [anchor of pair X]\n",
    "#     mark -> pair X as visited = True\n",
    "#     candidates -> pairs author != Xa & visited = False\n",
    "#     faiss search (query, candidates) -> search_results\n",
    "#     drop duplicates by authors in search_results -> unique_filtered_search_results\n",
    "#     negatives -> random select batch_size - 1 from unique_filtered_search_results\n",
    "#     mark -> negatives pairs as visited = True\n",
    "#     mark -> batch for X & negatives as batch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_jsonl_into_dataframe(jsonl_path):\n",
    "    \"\"\"\n",
    "    Reads a single .jsonl file (or you could adapt to read multiple).\n",
    "    Returns a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_json(jsonl_path, orient=\"records\", lines=True)\n",
    "    return df\n",
    "\n",
    "def prepare_dataframe(\n",
    "    df, \n",
    "    embedding_col=\"doc_luarmud_embedding_anchor\", \n",
    "    new_embedding_col=\"doc_luarmud_embedding_anchor_nparr\"\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Checks that 'documentID_anchor' exists (unique ID).\n",
    "    2) Extracts 'authorID' from 'authorIDs_anchor' (first element).\n",
    "    3) Converts anchor embeddings to float32 arrays (new_embedding_col).\n",
    "    4) Drops rows missing embeddings.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure 'documentID_anchor' is present\n",
    "    if \"documentID_anchor\" not in df.columns:\n",
    "        raise ValueError(\"Data must contain a 'documentID_anchor' column for unique identification.\")\n",
    "\n",
    "    # If you always have 'authorIDs_anchor' as a list with at least 1 ID,\n",
    "    # create a single 'authorID' column:\n",
    "    if \"authorID\" not in df.columns:\n",
    "        raise ValueError(\"Data must contain 'authorID' for extracting the single authorID.\")\n",
    "\n",
    "    # Convert anchor embeddings to np.float32\n",
    "    if embedding_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{embedding_col}' not found. Check your data.\")\n",
    "\n",
    "    def to_array(x):\n",
    "        return np.array(x, dtype=np.float32) if isinstance(x, list) else None\n",
    "\n",
    "    df[new_embedding_col] = df[embedding_col].apply(to_array)\n",
    "    \n",
    "    # Drop rows missing anchor embeddings\n",
    "    df = df.dropna(subset=[new_embedding_col]).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_faiss_index(df, embedding_col=\"doc_luarmud_embedding_anchor_nparr\", metric=\"l2\"):\n",
    "    \"\"\"\n",
    "    Builds a FAISS index over df[embedding_col].\n",
    "    Returns (index_id_map, id_map).\n",
    "    \"\"\"\n",
    "    embeddings = np.vstack(df[embedding_col].values)  # shape: (N, dim)\n",
    "    dim = embeddings.shape[1]\n",
    "\n",
    "    if metric == \"l2\":\n",
    "        index = faiss.IndexFlatL2(dim)\n",
    "    elif metric == \"ip\":\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid metric. Use 'l2' or 'ip'.\")\n",
    "\n",
    "    # ID map: 0..N-1 -> row index\n",
    "    id_map = np.arange(len(df), dtype=np.int64)\n",
    "\n",
    "    index_id_map = faiss.IndexIDMap2(index)\n",
    "    index_id_map.add_with_ids(embeddings, id_map)\n",
    "    return index_id_map, id_map\n",
    "\n",
    "def create_batches_iterative(\n",
    "    df, \n",
    "    faiss_index, \n",
    "    batch_size=32, \n",
    "    embedding_col=\"doc_luarmud_embedding_anchor_nparr\",\n",
    "    author_col=\"authorID\",  \n",
    "    random_seed=42,\n",
    "    out_jsonl_path=\"iterative_batches_output.jsonl\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Iteratively forms batches using anchor embeddings:\n",
    "      1) Pick an unvisited seed\n",
    "      2) FAISS search for neighbors among unvisited\n",
    "      3) Exclude same-author\n",
    "      4) Randomly pick up to batch_size-1\n",
    "      5) Mark used as visited\n",
    "      6) Write each row (seed + negatives) to out_jsonl_path with \"batch_id\"\n",
    "    \"\"\"\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # row index -> docID_anchor\n",
    "    row_to_documentID = df[\"documentID_anchor\"].to_dict()  # {row_idx: docID}\n",
    "\n",
    "    # Ensure the author_col is present\n",
    "    if author_col not in df.columns:\n",
    "        raise ValueError(f\"Data must contain a column '{author_col}' for negative filtering.\")\n",
    "    \n",
    "    # Get the array of authors\n",
    "    author_ids = df[author_col].values  # shape: (N,)\n",
    "\n",
    "    # Set of row indices that are unvisited\n",
    "    unvisited_rows = set(range(len(df)))\n",
    "\n",
    "    with open(out_jsonl_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        batches = []\n",
    "        batch_num = 0\n",
    "\n",
    "        pbar = tqdm(total=len(df), desc=\"Forming batches\")\n",
    "        while unvisited_rows:\n",
    "            # pick a seed\n",
    "            seed_row = next(iter(unvisited_rows))\n",
    "            seed_author = author_ids[seed_row]\n",
    "\n",
    "            # search in FAISS\n",
    "            top_k = len(unvisited_rows)  # you can reduce this if it's too large\n",
    "            seed_emb = df.at[seed_row, embedding_col].reshape(1, -1)\n",
    "            distances, neighbors = faiss_index.search(seed_emb, top_k)\n",
    "            neighbors = neighbors[0]\n",
    "\n",
    "            # Filter out seed, visited, same-author\n",
    "            valid_negatives = []\n",
    "            for nbr_row in neighbors:\n",
    "                if nbr_row == seed_row:\n",
    "                    continue\n",
    "                if nbr_row not in unvisited_rows:\n",
    "                    continue\n",
    "                if author_ids[nbr_row] == seed_author:\n",
    "                    continue\n",
    "                valid_negatives.append(nbr_row)\n",
    "\n",
    "            # One negative per unique author (if desired)\n",
    "            unique_neg_by_author = {}\n",
    "            for nr in valid_negatives:\n",
    "                a = author_ids[nr]\n",
    "                if a not in unique_neg_by_author:\n",
    "                    unique_neg_by_author[a] = nr\n",
    "            final_candidates = list(unique_neg_by_author.values())\n",
    "\n",
    "            # random pick up to (batch_size - 1)\n",
    "            needed = batch_size - 1\n",
    "            if len(final_candidates) > needed:\n",
    "                final_candidates = random.sample(final_candidates, needed)\n",
    "\n",
    "            # form the batch\n",
    "            batch_rows = [seed_row] + final_candidates\n",
    "\n",
    "            # mark them visited\n",
    "            for r in batch_rows:\n",
    "                unvisited_rows.remove(r)\n",
    "\n",
    "            # write each row to JSONL\n",
    "            doc_ids_in_batch = []\n",
    "            for r in batch_rows:\n",
    "                row_data = df.iloc[r].to_dict()\n",
    "                row_data[\"batch_id\"] = batch_num\n",
    "                # remove the numpy array so JSON dumping won't fail\n",
    "                row_data.pop(embedding_col, None)\n",
    "                row_data.pop(\"doc_luarmud_embedding_anchor\", None)\n",
    "                row_data.pop(\"doc_luarmud_embedding_positive\", None)\n",
    "\n",
    "                out_file.write(json.dumps(row_data, ensure_ascii=False) + \"\\n\")\n",
    "                doc_ids_in_batch.append(row_to_documentID[r])\n",
    "\n",
    "            # keep track in memory (optional)\n",
    "            batches.append({\n",
    "                \"batch_id\": batch_num,\n",
    "                \"doc_ids\": doc_ids_in_batch\n",
    "            })\n",
    "            batch_num += 1\n",
    "            pbar.update(len(batch_rows))\n",
    "\n",
    "        pbar.close()\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 401587 total rows from JSONL.\n"
     ]
    }
   ],
   "source": [
    "input_hard_pos_pairs_path = \"/data/araghavan/HIATUS/datadreamer-ta2/data/ta2_jan_2025_trian_data/trainsadiri_luarmud_chunks/train_sadiri_luarmud_embeddings_hard_pos_pairs.jsonl\"\n",
    "df_raw = load_jsonl_into_dataframe(input_hard_pos_pairs_path)\n",
    "print(f\"Loaded {len(df_raw)} total rows from JSONL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['documentID_anchor', 'authorIDs_anchor', 'fullText_anchor',\n",
       "       'spanAttribution_anchor', 'collectionNum_anchor', 'source_anchor',\n",
       "       'dateCollected_anchor', 'publiclyAvailable_anchor',\n",
       "       'deidentified_anchor', 'languages_anchor', 'lengthWords_anchor',\n",
       "       'dateCreated_anchor', 'timeCreated_anchor', 'isForeground_anchor',\n",
       "       'authorID', 'genre_anchor', 'doc_luarmud_embedding_anchor',\n",
       "       'doc_xrbmtgc_genre_anchor', 'documentID_positive', 'authorIDs_positive',\n",
       "       'fullText_positive', 'spanAttribution_positive',\n",
       "       'collectionNum_positive', 'source_positive', 'dateCollected_positive',\n",
       "       'publiclyAvailable_positive', 'deidentified_positive',\n",
       "       'languages_positive', 'lengthWords_positive', 'dateCreated_positive',\n",
       "       'timeCreated_positive', 'isForeground_positive', 'genre_positive',\n",
       "       'doc_luarmud_embedding_positive', 'doc_xrbmtgc_genre_positive',\n",
       "       'similarity_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after dropping missing embeddings: 401587 rows.\n"
     ]
    }
   ],
   "source": [
    "# 1) Prepare DataFrame\n",
    "df_prepped = prepare_dataframe(df_raw, embedding_col=\"doc_luarmud_embedding_anchor\", new_embedding_col=\"doc_luarmud_embedding_anchor_nparr\")\n",
    "print(f\"Data after dropping missing embeddings: {len(df_prepped)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built FAISS index with size = 401587\n"
     ]
    }
   ],
   "source": [
    "# 2) Build a FAISS index on anchor embeddings\n",
    "faiss_index, id_map = build_faiss_index(\n",
    "    df_prepped, \n",
    "    embedding_col=\"doc_luarmud_embedding_anchor_nparr\", \n",
    "    metric=\"l2\"\n",
    ")\n",
    "print(\"Built FAISS index with size =\", faiss_index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batches of size 32 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forming batches: 100%|██████████| 401587/401587 [45:05<00:00, 148.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formed 13574 batches.\n",
      "Saved each batch record to '/data/araghavan/HIATUS/datadreamer-ta2/data/ta2_jan_2025_trian_data/trainsadiri_luarmud_chunks/train_sadiri_hard_batches_v001.jsonl' in JSON lines format.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) Iterative batching\n",
    "# output_jsonl = \"/data/araghavan/HIATUS/datadreamer-ta2/data/ta2_jan_2025_trian_data/trainsadiri_luarmud_chunks/train_sadiri_hard_batches_v001.jsonl\"\n",
    "output_jsonl = \"/data/araghavan/HIATUS/datadreamer-ta2/data/ta2_jan_2025_trian_data/hard_batching_dataset_luaremb_wo_ao3_filtered/train_sadiri_hard_batches_v001.jsonl\"\n",
    "batch_size = 32\n",
    "print(f\"Creating batches of size {batch_size} ...\")\n",
    "\n",
    "all_batches = create_batches_iterative(\n",
    "    df=df_prepped,\n",
    "    faiss_index=faiss_index,\n",
    "    batch_size=batch_size,\n",
    "    embedding_col=\"doc_luarmud_embedding_anchor_nparr\",\n",
    "    author_col=\"authorID\",   # the single ID we extracted\n",
    "    random_seed=42,\n",
    "    out_jsonl_path=output_jsonl\n",
    ")\n",
    "print(f\"Formed {len(all_batches)} batches.\")\n",
    "print(f\"Saved each batch record to '{output_jsonl}' in JSON lines format.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sadiri Hard Mix Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import Random\n",
    "from transformers import TrainerCallback\n",
    "epoch_tracker = {}\n",
    "epoch_tracker['epoch'] = 0\n",
    "class EpochTrackerCallback(TrainerCallback):\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        epoch_tracker['epoch'] = int(state.epoch)\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        epoch_tracker['epoch'] = int(state.epoch)\n",
    "\n",
    "path = \"/data/araghavan/HIATUS/datadreamer-ta2/data/ta2_jan_2025_trian_data/anchor_pos_train_sadiri_luarmudsbertamllv2.jsonl\"\n",
    "file_df = pd.read_json(path, lines=True)\n",
    "file_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_anchorpos_hardmix_batches_optimized(file_df, split, args):\n",
    "    \"\"\"\n",
    "    Optimized version that:\n",
    "     1) Builds and keeps a single FAISS index\n",
    "     2) Uses a mask to \"remove\" docs from availability\n",
    "     3) Writes each batch to JSONL with a 'batch_id' instead of storing all in memory\n",
    "    \"\"\"\n",
    "    print(f\"Called get_anchorpos_hardmix_batches_optimized with split: {split}\")\n",
    "    print(f\"Reading file: {path}\")\n",
    "    # ---------------------\n",
    "    # 1. Load & sample data\n",
    "    # ---------------------\n",
    "\n",
    "    print(f\"Finished loading file: {path} with shape: {file_df.shape}\")\n",
    "\n",
    "    # ---------------------\n",
    "    # 2. Normalize embeddings once\n",
    "    # ---------------------\n",
    "    def normalize(vecs):\n",
    "        vecs_ = np.vstack(vecs).astype(np.float32)\n",
    "        norms = np.linalg.norm(vecs_, axis=1, keepdims=True)\n",
    "        norms = np.clip(norms, 1e-8, np.inf)\n",
    "        return vecs_ / norms\n",
    "\n",
    "    all_embeddings = normalize(file_df[\"doc_luarmud_embedding_anchor\"].values)\n",
    "    ids_array = file_df.index.to_numpy()\n",
    "\n",
    "    # ---------------------\n",
    "    # 3. Build a single FAISS index\n",
    "    # ---------------------\n",
    "    dim = all_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)       # inner-product index\n",
    "    index = faiss.IndexIDMap(index)\n",
    "    index.add_with_ids(all_embeddings, ids_array.astype(np.int64))\n",
    "\n",
    "    # Boolean mask for availability\n",
    "    available_mask = np.ones(len(file_df), dtype=bool)\n",
    "\n",
    "    # For convenience\n",
    "    sbert_sim = file_df[\"sbertamllv2_embedding_similarity_score\"].values\n",
    "    author_ids = file_df[\"authorID\"].values\n",
    "    doc_ids = file_df[\"documentID_anchor\"].values  # or unique ID\n",
    "\n",
    "    # ---------------------\n",
    "    # Batch parameters\n",
    "    # ---------------------\n",
    "    batch_query_needed_size = int(0.3 * args.batch_size)\n",
    "    batch_candidates_needed_size = int(0.3 * args.batch_size)\n",
    "    print(\n",
    "        f\"Generating batches with batch_query_needed_size={batch_query_needed_size} \"\n",
    "        f\"and batch_candidates_needed_size={batch_candidates_needed_size}\"\n",
    "    )\n",
    "\n",
    "    # We'll count how many batches we produce & how many rows total\n",
    "    batch_id = 0\n",
    "    total_rows = 0\n",
    "\n",
    "    # File path to write out batches\n",
    "    timestamp = int(time.time())\n",
    "    output_path_mixedbatches = os.path.join(args.model_op_dir, f\"train_sadiri_hardmixbatches_{timestamp}.jsonl\")\n",
    "\n",
    "    # We'll do a random object with a seed based on epoch\n",
    "    epoch_seed = getattr(epoch_tracker, \"epoch\", 0)\n",
    "    rng = Random(epoch_seed)\n",
    "\n",
    "    while True:\n",
    "        # ----------------------------------------------\n",
    "        # 4.1. Pick queries from docs with sbert_sim < threshold & still available\n",
    "        # ----------------------------------------------\n",
    "        valid_query_mask = (sbert_sim < args.sem_sim_threshold) & (available_mask)\n",
    "        valid_queries = np.nonzero(valid_query_mask)[0]  # array of doc indices\n",
    "\n",
    "        if len(valid_queries) < batch_query_needed_size:\n",
    "            # Not enough queries to form a batch => break\n",
    "            break\n",
    "\n",
    "        # Sample the queries\n",
    "        chosen_query_indices = rng.sample(list(valid_queries), batch_query_needed_size)\n",
    "        # Mark them unavailable\n",
    "        for idx in chosen_query_indices:\n",
    "            available_mask[idx] = False\n",
    "\n",
    "        # Collect their authors\n",
    "        chosen_query_authors = set(author_ids[idx] for idx in chosen_query_indices)\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # 4.2. For each query, search top-K in FAISS\n",
    "        # ----------------------------------------------\n",
    "        chosen_query_embeddings = all_embeddings[chosen_query_indices, :]\n",
    "        # Instead of searching entire corpus_size, limit to a top-K\n",
    "        top_k = 500\n",
    "        distances, neighbors = index.search(chosen_query_embeddings, top_k)\n",
    "\n",
    "        # We'll gather candidate IDs from these searches\n",
    "        candidate_indices_collector = []\n",
    "        max_cands_per_query = max(1, batch_candidates_needed_size // batch_query_needed_size)\n",
    "\n",
    "        used_authors_in_batch = set(chosen_query_authors)\n",
    "\n",
    "        for i, q_idx in enumerate(chosen_query_indices):\n",
    "            q_author = author_ids[q_idx]\n",
    "            picks_for_this_query = 0\n",
    "\n",
    "            for rank, cand_id in enumerate(neighbors[i]):\n",
    "                if cand_id == -1:\n",
    "                    # FAISS sometimes returns -1 if no neighbor\n",
    "                    continue\n",
    "\n",
    "                # Already used in a prior batch?\n",
    "                if not available_mask[cand_id]:\n",
    "                    continue\n",
    "\n",
    "                # Check similarity threshold\n",
    "                sim_score = distances[i][rank]\n",
    "                if sim_score < args.sty_sim_threshold:\n",
    "                    continue\n",
    "\n",
    "                cand_author = author_ids[cand_id]\n",
    "                # Must differ from query author & not used in this batch\n",
    "                if cand_author == q_author or cand_author in used_authors_in_batch:\n",
    "                    continue\n",
    "\n",
    "                # If it passes all checks, we pick it\n",
    "                candidate_indices_collector.append(cand_id)\n",
    "                used_authors_in_batch.add(cand_author)\n",
    "                picks_for_this_query += 1\n",
    "\n",
    "                if picks_for_this_query >= max_cands_per_query:\n",
    "                    break  # done for this query\n",
    "\n",
    "        # We might have more than needed => sample down\n",
    "        if len(candidate_indices_collector) > batch_candidates_needed_size:\n",
    "            candidate_indices_collector = rng.sample(candidate_indices_collector, batch_candidates_needed_size)\n",
    "\n",
    "        # Mark them unavailable\n",
    "        for idx in candidate_indices_collector:\n",
    "            available_mask[idx] = False\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # 4.3. Random docs to fill remainder of the batch\n",
    "        # ----------------------------------------------\n",
    "        batch_query_size = len(chosen_query_indices)\n",
    "        batch_cand_size = len(candidate_indices_collector)\n",
    "        random_needed = args.batch_size - batch_query_size - batch_cand_size\n",
    "\n",
    "        random_indices_chosen = []\n",
    "        if random_needed > 0:\n",
    "            # Sample from available_mask\n",
    "            avails = np.nonzero(available_mask)[0]\n",
    "            if len(avails) < random_needed:\n",
    "                break  # Not enough docs left\n",
    "\n",
    "            # Get authors to exclude (from query + candidate docs)\n",
    "            used_authors_in_batch = set(author_ids[np.concatenate([chosen_query_indices, candidate_indices_collector])])\n",
    "\n",
    "            # Shuffle & pick random docs whose authors haven't been used yet\n",
    "            for idx in rng.sample(list(avails), len(avails)):\n",
    "                if author_ids[idx] in used_authors_in_batch:\n",
    "                    continue\n",
    "                random_indices_chosen.append(idx)\n",
    "                used_authors_in_batch.add(author_ids[idx])  # keep author uniqueness\n",
    "                if len(random_indices_chosen) == random_needed:\n",
    "                    break\n",
    "\n",
    "            if len(random_indices_chosen) < random_needed:\n",
    "                print(f\"Insufficient random docs with unique authors for batch {batch_id}\")\n",
    "                break\n",
    "\n",
    "            for idx in random_indices_chosen:\n",
    "                available_mask[idx] = False\n",
    "\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # 4.4. Construct the DataFrame for the batch & write it\n",
    "        # ----------------------------------------------\n",
    "        all_batch_indices = chosen_query_indices + candidate_indices_collector + random_indices_chosen\n",
    "\n",
    "        # If we can't form a full batch, break\n",
    "        if len(all_batch_indices) < args.batch_size:\n",
    "            print(f\"Breaking at batch_id={batch_id} due to insufficient rows.\")\n",
    "            break\n",
    "\n",
    "        # Build the batch DF\n",
    "        batch_df = file_df.iloc[all_batch_indices].copy()\n",
    "        batch_df[\"batch_id\"] = batch_id\n",
    "\n",
    "        # Write this batch to the JSONL file in append mode\n",
    "        # Each row has a field \"batch_id\"\n",
    "        # batch_df.to_json(output_path_mixedbatches, orient=\"records\", lines=True, mode=\"a\")\n",
    "        with open(output_path_mixedbatches, \"a\") as f:\n",
    "            batch_df.to_json(f, orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "        batch_id += 1\n",
    "        total_rows += len(batch_df)\n",
    "\n",
    "        print(f\"\\r[Progress] Generated batch_id={batch_id}, total_rows={total_rows}, left={available_mask.sum()} \", end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\nDone generating batches.\")\n",
    "    print(f\"Total batches: {batch_id}  | Total rows: {total_rows}\")\n",
    "\n",
    "## Create args namespace with necessary attributes\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 32\n",
    "        self.sem_sim_threshold = 0.1\n",
    "        self.sty_sim_threshold = 0.3\n",
    "        self.model_op_dir = \"/data/araghavan/HIATUS/datadreamer-ta2/data/ta2_jan_2025_trian_data\"\n",
    "\n",
    "args = Args()\n",
    "get_anchorpos_hardmix_batches_optimized(file_df, \"train\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biber Anchor - Pos Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To generate luar and sbert embeddings for biber data, refer to ~/datadreamer-ta2/src/generate_luar_sbert_genre_emb_biber.py\n",
    "## Once done, read the embeddings file to create hard batches\n",
    "import pandas as pd\n",
    "biber_data_genre_emb_df = pd.read_json(\"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/genre_data/biber_train_luar_sbert_genre_embeddings_v001.jsonl\", lines=True)\n",
    "biber_data_genre_emb_df.rename(columns={\"predicted_genre\": \"biber_pred_genre\"}, inplace=True)\n",
    "biber_data_genre_emb_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documentID</th>\n",
       "      <th>authorIDs</th>\n",
       "      <th>fullText</th>\n",
       "      <th>spanAttribution</th>\n",
       "      <th>collectionNum</th>\n",
       "      <th>source</th>\n",
       "      <th>dateCollected</th>\n",
       "      <th>publiclyAvailable</th>\n",
       "      <th>deidentified</th>\n",
       "      <th>languages</th>\n",
       "      <th>...</th>\n",
       "      <th>sourceSpecific</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>biber_pred_genre</th>\n",
       "      <th>doc_luarmud_embedding</th>\n",
       "      <th>doc_sbertamllv2_embedding</th>\n",
       "      <th>doc_xrbmtgc_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bdfad251aa714c409edcc84579a217e0</td>\n",
       "      <td>[002b041f3b0848edb40005d51f2826c8]</td>\n",
       "      <td>“You have done some amazing work, Kurt.” Was a...</td>\n",
       "      <td>[{'authorID': '002b041f3b0848edb40005d51f2826c...</td>\n",
       "      <td>HRS 1</td>\n",
       "      <td>https://archiveofourown.org/</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[en, un, un]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.050881873800000005, 0.3831697106, -0.137104...</td>\n",
       "      <td>[-0.035311203400000005, 0.058806069200000005, ...</td>\n",
       "      <td>Prose/Lyrical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f408b9535f7d46e597c8d19b356b5141</td>\n",
       "      <td>[002b041f3b0848edb40005d51f2826c8]</td>\n",
       "      <td>Will gave Blaine a key to the teachers bathroo...</td>\n",
       "      <td>[{'authorID': '002b041f3b0848edb40005d51f2826c...</td>\n",
       "      <td>HRS 1</td>\n",
       "      <td>https://archiveofourown.org/</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[en, un, un]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.0454084352, 0.31967493890000004, 0.15773031...</td>\n",
       "      <td>[-0.0980639383, 0.0466547422, 0.04821296040000...</td>\n",
       "      <td>Prose/Lyrical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48f3efd4362c42b883d45a8671b75744</td>\n",
       "      <td>[002b041f3b0848edb40005d51f2826c8]</td>\n",
       "      <td>\\n1. Finding Out\\nMercedes and Sam had gone to...</td>\n",
       "      <td>[{'authorID': '002b041f3b0848edb40005d51f2826c...</td>\n",
       "      <td>HRS 1</td>\n",
       "      <td>https://archiveofourown.org/</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[en, un, un]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.157700792, 0.2678265274, -0.0553099811, 0.3...</td>\n",
       "      <td>[-0.0577936843, 0.030992426000000003, 0.061523...</td>\n",
       "      <td>Prose/Lyrical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65a57bca1a1444cc87a7e7c368cf6606</td>\n",
       "      <td>[002b041f3b0848edb40005d51f2826c8]</td>\n",
       "      <td>“He is,” Blaine said grabbing the boy’s hand. ...</td>\n",
       "      <td>[{'authorID': '002b041f3b0848edb40005d51f2826c...</td>\n",
       "      <td>HRS 1</td>\n",
       "      <td>https://archiveofourown.org/</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[en, un, un]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.3562221229, 0.0158033203, 0.1227550954, 0.4...</td>\n",
       "      <td>[-0.1175161228, 0.0711242929, 0.0032115169, 0....</td>\n",
       "      <td>Prose/Lyrical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f54c730ce1fa49f0a44998d1e6376ca8</td>\n",
       "      <td>[002b041f3b0848edb40005d51f2826c8]</td>\n",
       "      <td>They talked for the next half an hour, and dis...</td>\n",
       "      <td>[{'authorID': '002b041f3b0848edb40005d51f2826c...</td>\n",
       "      <td>HRS 1</td>\n",
       "      <td>https://archiveofourown.org/</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[en, un, un]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.5024769306, 0.1137882546, -0.0644055903, 0....</td>\n",
       "      <td>[-0.1070570871, 0.037984032200000004, 0.024115...</td>\n",
       "      <td>Prose/Lyrical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         documentID                           authorIDs  \\\n",
       "0  bdfad251aa714c409edcc84579a217e0  [002b041f3b0848edb40005d51f2826c8]   \n",
       "1  f408b9535f7d46e597c8d19b356b5141  [002b041f3b0848edb40005d51f2826c8]   \n",
       "2  48f3efd4362c42b883d45a8671b75744  [002b041f3b0848edb40005d51f2826c8]   \n",
       "3  65a57bca1a1444cc87a7e7c368cf6606  [002b041f3b0848edb40005d51f2826c8]   \n",
       "4  f54c730ce1fa49f0a44998d1e6376ca8  [002b041f3b0848edb40005d51f2826c8]   \n",
       "\n",
       "                                            fullText  \\\n",
       "0  “You have done some amazing work, Kurt.” Was a...   \n",
       "1  Will gave Blaine a key to the teachers bathroo...   \n",
       "2  \\n1. Finding Out\\nMercedes and Sam had gone to...   \n",
       "3  “He is,” Blaine said grabbing the boy’s hand. ...   \n",
       "4  They talked for the next half an hour, and dis...   \n",
       "\n",
       "                                     spanAttribution collectionNum  \\\n",
       "0  [{'authorID': '002b041f3b0848edb40005d51f2826c...         HRS 1   \n",
       "1  [{'authorID': '002b041f3b0848edb40005d51f2826c...         HRS 1   \n",
       "2  [{'authorID': '002b041f3b0848edb40005d51f2826c...         HRS 1   \n",
       "3  [{'authorID': '002b041f3b0848edb40005d51f2826c...         HRS 1   \n",
       "4  [{'authorID': '002b041f3b0848edb40005d51f2826c...         HRS 1   \n",
       "\n",
       "                         source dateCollected  publiclyAvailable  \\\n",
       "0  https://archiveofourown.org/    2023-09-20                1.0   \n",
       "1  https://archiveofourown.org/    2023-09-20                1.0   \n",
       "2  https://archiveofourown.org/    2023-09-20                1.0   \n",
       "3  https://archiveofourown.org/    2023-09-20                1.0   \n",
       "4  https://archiveofourown.org/    2023-09-20                1.0   \n",
       "\n",
       "   deidentified     languages  ...  sourceSpecific gender age  topic  sign  \\\n",
       "0           1.0  [en, un, un]  ...            None   None NaN   None  None   \n",
       "1           1.0  [en, un, un]  ...            None   None NaN   None  None   \n",
       "2           1.0  [en, un, un]  ...            None   None NaN   None  None   \n",
       "3           1.0  [en, un, un]  ...            None   None NaN   None  None   \n",
       "4           1.0  [en, un, un]  ...            None   None NaN   None  None   \n",
       "\n",
       "   date  biber_pred_genre                              doc_luarmud_embedding  \\\n",
       "0  None                 5  [0.050881873800000005, 0.3831697106, -0.137104...   \n",
       "1  None                 5  [0.0454084352, 0.31967493890000004, 0.15773031...   \n",
       "2  None                 5  [0.157700792, 0.2678265274, -0.0553099811, 0.3...   \n",
       "3  None                 5  [0.3562221229, 0.0158033203, 0.1227550954, 0.4...   \n",
       "4  None                 5  [0.5024769306, 0.1137882546, -0.0644055903, 0....   \n",
       "\n",
       "                           doc_sbertamllv2_embedding doc_xrbmtgc_genre  \n",
       "0  [-0.035311203400000005, 0.058806069200000005, ...     Prose/Lyrical  \n",
       "1  [-0.0980639383, 0.0466547422, 0.04821296040000...     Prose/Lyrical  \n",
       "2  [-0.0577936843, 0.030992426000000003, 0.061523...     Prose/Lyrical  \n",
       "3  [-0.1175161228, 0.0711242929, 0.0032115169, 0....     Prose/Lyrical  \n",
       "4  [-0.1070570871, 0.037984032200000004, 0.024115...     Prose/Lyrical  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biber_data_genre_emb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do outer join of biber_data_genre_emb_df with itself on authorID column, have suffixes as _anchor, _positive\n",
    "## Filter out docs where the biber_pred_genre_anchor and biber_pred_genre_positive are same\n",
    "## generate batchwise cosine similarity scores between doc_sbertamllv2_embedding_anchor and doc_sbertamllv2_embedding_positive and store in a new column sbertamllv2_embedding_similarity_score\n",
    "## create random batches of size 32 with below conditions\n",
    "## 1. 30% of the batch: query docs: docs from authors with same biber_pred_genre_anchor and sbertamllv2_embedding_similarity_score < sem_sim_threshold\n",
    "## 2. 30% of the batch: candidate docs: docs from different authors with same biber_pred_genre_anchor and FAISS vectorized search on doc_luarmud_embedding score > sem_sim_threshold\n",
    "## 3. 40% of the batch: random docs: docs from different authors with same biber_pred_genre_anchor\n",
    "\n",
    "## Repeat the above process for 3 epochs, with corpus same as the anchor-pos pairs every epoch while having different random seed and different random batches but continued batch_id, starting from the last batch_id of the previous epoch and incrementing it by 1 and storing the batch_id in the jsonl file but with corpus dataframe as the whole dataframe for every epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144628, 51)\n",
      "(807442, 101)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documentID_anchor</th>\n",
       "      <th>authorIDs_anchor</th>\n",
       "      <th>fullText_anchor</th>\n",
       "      <th>spanAttribution_anchor</th>\n",
       "      <th>collectionNum_anchor</th>\n",
       "      <th>source_anchor</th>\n",
       "      <th>dateCollected_anchor</th>\n",
       "      <th>publiclyAvailable_anchor</th>\n",
       "      <th>deidentified_anchor</th>\n",
       "      <th>languages_anchor</th>\n",
       "      <th>...</th>\n",
       "      <th>sourceSpecific_positive</th>\n",
       "      <th>gender_positive</th>\n",
       "      <th>age_positive</th>\n",
       "      <th>topic_positive</th>\n",
       "      <th>sign_positive</th>\n",
       "      <th>date_positive</th>\n",
       "      <th>biber_pred_genre_positive</th>\n",
       "      <th>doc_luarmud_embedding_positive</th>\n",
       "      <th>doc_sbertamllv2_embedding_positive</th>\n",
       "      <th>doc_xrbmtgc_genre_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>e73a55df06db47f28dfcd15626e345ee</td>\n",
       "      <td>[017457df53294886a3593e5a2977eb6d]</td>\n",
       "      <td>Mirchandani had made Ronon Head of Offworld Li...</td>\n",
       "      <td>[{'authorID': '017457df53294886a3593e5a2977eb6...</td>\n",
       "      <td>HRS 1</td>\n",
       "      <td>https://archiveofourown.org/</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[en, un, un]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.3409982622, 0.0741266087, 0.2813294232, 0.2...</td>\n",
       "      <td>[-0.0947817564, 0.0683237612, 0.0600385629, -0...</td>\n",
       "      <td>Prose/Lyrical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>e73a55df06db47f28dfcd15626e345ee</td>\n",
       "      <td>[017457df53294886a3593e5a2977eb6d]</td>\n",
       "      <td>Mirchandani had made Ronon Head of Offworld Li...</td>\n",
       "      <td>[{'authorID': '017457df53294886a3593e5a2977eb6...</td>\n",
       "      <td>HRS 1</td>\n",
       "      <td>https://archiveofourown.org/</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[en, un, un]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>14</td>\n",
       "      <td>[-0.19908979540000002, 0.26586478950000003, 0....</td>\n",
       "      <td>[-0.0569949374, 0.0539704487, 0.0345009565, -0...</td>\n",
       "      <td>Prose/Lyrical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>591fa218125e4f5bad8cd3fe26d7d7c0</td>\n",
       "      <td>[017457df53294886a3593e5a2977eb6d]</td>\n",
       "      <td>‘Right, I’m going out to get you some clothes ...</td>\n",
       "      <td>[{'authorID': '017457df53294886a3593e5a2977eb6...</td>\n",
       "      <td>HRS 1</td>\n",
       "      <td>https://archiveofourown.org/</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[en, un, un]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.3409982622, 0.0741266087, 0.2813294232, 0.2...</td>\n",
       "      <td>[-0.0947817564, 0.0683237612, 0.0600385629, -0...</td>\n",
       "      <td>Prose/Lyrical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>591fa218125e4f5bad8cd3fe26d7d7c0</td>\n",
       "      <td>[017457df53294886a3593e5a2977eb6d]</td>\n",
       "      <td>‘Right, I’m going out to get you some clothes ...</td>\n",
       "      <td>[{'authorID': '017457df53294886a3593e5a2977eb6...</td>\n",
       "      <td>HRS 1</td>\n",
       "      <td>https://archiveofourown.org/</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[en, un, un]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>14</td>\n",
       "      <td>[-0.19908979540000002, 0.26586478950000003, 0....</td>\n",
       "      <td>[-0.0569949374, 0.0539704487, 0.0345009565, -0...</td>\n",
       "      <td>Prose/Lyrical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>141e32047f914ac3b17b3d45bf6d8929</td>\n",
       "      <td>[017457df53294886a3593e5a2977eb6d]</td>\n",
       "      <td>Rodney had read the report and been impressed ...</td>\n",
       "      <td>[{'authorID': '017457df53294886a3593e5a2977eb6...</td>\n",
       "      <td>HRS 1</td>\n",
       "      <td>https://archiveofourown.org/</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[en, un, un]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.3409982622, 0.0741266087, 0.2813294232, 0.2...</td>\n",
       "      <td>[-0.0947817564, 0.0683237612, 0.0600385629, -0...</td>\n",
       "      <td>Prose/Lyrical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    documentID_anchor                    authorIDs_anchor  \\\n",
       "799  e73a55df06db47f28dfcd15626e345ee  [017457df53294886a3593e5a2977eb6d]   \n",
       "802  e73a55df06db47f28dfcd15626e345ee  [017457df53294886a3593e5a2977eb6d]   \n",
       "812  591fa218125e4f5bad8cd3fe26d7d7c0  [017457df53294886a3593e5a2977eb6d]   \n",
       "815  591fa218125e4f5bad8cd3fe26d7d7c0  [017457df53294886a3593e5a2977eb6d]   \n",
       "825  141e32047f914ac3b17b3d45bf6d8929  [017457df53294886a3593e5a2977eb6d]   \n",
       "\n",
       "                                       fullText_anchor  \\\n",
       "799  Mirchandani had made Ronon Head of Offworld Li...   \n",
       "802  Mirchandani had made Ronon Head of Offworld Li...   \n",
       "812  ‘Right, I’m going out to get you some clothes ...   \n",
       "815  ‘Right, I’m going out to get you some clothes ...   \n",
       "825  Rodney had read the report and been impressed ...   \n",
       "\n",
       "                                spanAttribution_anchor collectionNum_anchor  \\\n",
       "799  [{'authorID': '017457df53294886a3593e5a2977eb6...                HRS 1   \n",
       "802  [{'authorID': '017457df53294886a3593e5a2977eb6...                HRS 1   \n",
       "812  [{'authorID': '017457df53294886a3593e5a2977eb6...                HRS 1   \n",
       "815  [{'authorID': '017457df53294886a3593e5a2977eb6...                HRS 1   \n",
       "825  [{'authorID': '017457df53294886a3593e5a2977eb6...                HRS 1   \n",
       "\n",
       "                    source_anchor dateCollected_anchor  \\\n",
       "799  https://archiveofourown.org/           2023-09-20   \n",
       "802  https://archiveofourown.org/           2023-09-20   \n",
       "812  https://archiveofourown.org/           2023-09-20   \n",
       "815  https://archiveofourown.org/           2023-09-20   \n",
       "825  https://archiveofourown.org/           2023-09-20   \n",
       "\n",
       "     publiclyAvailable_anchor  deidentified_anchor languages_anchor  ...  \\\n",
       "799                       1.0                  1.0     [en, un, un]  ...   \n",
       "802                       1.0                  1.0     [en, un, un]  ...   \n",
       "812                       1.0                  1.0     [en, un, un]  ...   \n",
       "815                       1.0                  1.0     [en, un, un]  ...   \n",
       "825                       1.0                  1.0     [en, un, un]  ...   \n",
       "\n",
       "     sourceSpecific_positive gender_positive age_positive  topic_positive  \\\n",
       "799                     None            None          NaN            None   \n",
       "802                     None            None          NaN            None   \n",
       "812                     None            None          NaN            None   \n",
       "815                     None            None          NaN            None   \n",
       "825                     None            None          NaN            None   \n",
       "\n",
       "    sign_positive  date_positive  biber_pred_genre_positive  \\\n",
       "799          None           None                         14   \n",
       "802          None           None                         14   \n",
       "812          None           None                         14   \n",
       "815          None           None                         14   \n",
       "825          None           None                         14   \n",
       "\n",
       "                        doc_luarmud_embedding_positive  \\\n",
       "799  [0.3409982622, 0.0741266087, 0.2813294232, 0.2...   \n",
       "802  [-0.19908979540000002, 0.26586478950000003, 0....   \n",
       "812  [0.3409982622, 0.0741266087, 0.2813294232, 0.2...   \n",
       "815  [-0.19908979540000002, 0.26586478950000003, 0....   \n",
       "825  [0.3409982622, 0.0741266087, 0.2813294232, 0.2...   \n",
       "\n",
       "                    doc_sbertamllv2_embedding_positive  \\\n",
       "799  [-0.0947817564, 0.0683237612, 0.0600385629, -0...   \n",
       "802  [-0.0569949374, 0.0539704487, 0.0345009565, -0...   \n",
       "812  [-0.0947817564, 0.0683237612, 0.0600385629, -0...   \n",
       "815  [-0.0569949374, 0.0539704487, 0.0345009565, -0...   \n",
       "825  [-0.0947817564, 0.0683237612, 0.0600385629, -0...   \n",
       "\n",
       "    doc_xrbmtgc_genre_positive  \n",
       "799              Prose/Lyrical  \n",
       "802              Prose/Lyrical  \n",
       "812              Prose/Lyrical  \n",
       "815              Prose/Lyrical  \n",
       "825              Prose/Lyrical  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(biber_data_genre_emb_df.shape)\n",
    "biber_data_genre_emb_anchorpos_df = biber_data_genre_emb_df.merge(biber_data_genre_emb_df, how=\"outer\", left_on=\"authorID\", right_on=\"authorID\", suffixes=('_anchor', '_positive'))\n",
    "biber_data_genre_emb_anchorpos_df = biber_data_genre_emb_anchorpos_df.loc[\n",
    "    (biber_data_genre_emb_anchorpos_df.documentID_anchor != biber_data_genre_emb_anchorpos_df.documentID_positive) \n",
    "    & (biber_data_genre_emb_anchorpos_df.biber_pred_genre_anchor != biber_data_genre_emb_anchorpos_df.biber_pred_genre_positive)\n",
    "]\n",
    "print(biber_data_genre_emb_anchorpos_df.shape)\n",
    "display(biber_data_genre_emb_anchorpos_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 100%|██████████| 81/81 [00:00<00:00, 106.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Compute Cosine Similarity (Vectorized)\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "def batched_cosine_similarity(a_embeddings, b_embeddings, batch_size=10000):\n",
    "    scores = []\n",
    "    for i in tqdm(range(0, len(a_embeddings), batch_size), desc=\"Cosine Similarity\"):\n",
    "        a_batch = a_embeddings[i:i+batch_size]\n",
    "        b_batch = b_embeddings[i:i+batch_size]\n",
    "        sim = np.sum(a_batch * b_batch, axis=1) / (\n",
    "            np.linalg.norm(a_batch, axis=1) * np.linalg.norm(b_batch, axis=1)\n",
    "        )\n",
    "        scores.extend(sim)\n",
    "    return np.array(scores)\n",
    "anchor_embeddings = np.stack(biber_data_genre_emb_anchorpos_df[\"doc_sbertamllv2_embedding_anchor\"].values)\n",
    "positive_embeddings = np.stack(biber_data_genre_emb_anchorpos_df[\"doc_sbertamllv2_embedding_positive\"].values)\n",
    "biber_data_genre_emb_anchorpos_df[\"sbertamllv2_embedding_similarity_score\"] = batched_cosine_similarity(anchor_embeddings, positive_embeddings, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "biber_data_genre_emb_anchorpos_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_file_path = \"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/genre_data/biber_train_anchorpos_raw_v001.jsonl\"\n",
    "biber_data_genre_emb_anchorpos_df.to_json(op_file_path, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biber 3 Epochs Cached Hard Mix Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called get_anchorpos_biber_hardmix_batches_optimized with split: train\n",
      "Passed df with shape: (807442, 102)\n",
      "Generating batches with batch_query_needed_size=9 and batch_candidates_needed_size=9\n",
      "[Progress] Generated batch_id_ctr=22469, batch_id=22469, total_rows=719008, left=88434  Breaking at batch_id_ctr=22469, batch_id=22469 due to no valid genres with enough available docs.\n",
      "\n",
      "Done generating batches.\n",
      "Total batches: 22469, batch_id: 22469, args.batch_id: 22469  | Total rows: 719008\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import Random\n",
    "from transformers import TrainerCallback\n",
    "epoch_tracker = {}\n",
    "epoch_tracker['epoch'] = 0\n",
    "class EpochTrackerCallback(TrainerCallback):\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        epoch_tracker['epoch'] = int(state.epoch)\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        epoch_tracker['epoch'] = int(state.epoch)\n",
    "\n",
    "def get_anchorpos_biber_hardmix_batches_optimized(file_df, split, args, uniq_biber_genres):\n",
    "    \"\"\"\n",
    "    Optimized version that:\n",
    "     1) Builds and keeps a single FAISS index\n",
    "     2) Uses a mask to \"remove\" docs from availability\n",
    "     3) Writes each batch to JSONL with a 'batch_id' instead of storing all in memory\n",
    "    \"\"\"\n",
    "    print(f\"Called get_anchorpos_biber_hardmix_batches_optimized with split: {split}\")\n",
    "    # ---------------------\n",
    "    # 1. Load & sample data\n",
    "    # ---------------------\n",
    "\n",
    "    print(f\"Passed df with shape: {file_df.shape}\")\n",
    "\n",
    "    # ---------------------\n",
    "    # 2. Normalize embeddings once\n",
    "    # ---------------------\n",
    "    def normalize(vecs):\n",
    "        vecs_ = np.vstack(vecs).astype(np.float32)\n",
    "        norms = np.linalg.norm(vecs_, axis=1, keepdims=True)\n",
    "        norms = np.clip(norms, 1e-8, np.inf)\n",
    "        return vecs_ / norms\n",
    "\n",
    "    all_embeddings = normalize(file_df[\"doc_luarmud_embedding_anchor\"].values)\n",
    "    ids_array = file_df.index.to_numpy()\n",
    "\n",
    "    # ---------------------\n",
    "    # 3. Build a single FAISS index\n",
    "    # ---------------------\n",
    "    dim = all_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)       # inner-product index\n",
    "    index = faiss.IndexIDMap(index)\n",
    "    index.add_with_ids(all_embeddings, ids_array.astype(np.int64))\n",
    "\n",
    "    # Boolean mask for availability\n",
    "    available_mask = np.ones(len(file_df), dtype=bool)\n",
    "\n",
    "    # For convenience\n",
    "    sbert_sim = file_df[\"sbertamllv2_embedding_similarity_score\"].values\n",
    "    author_ids = file_df[\"authorID\"].values\n",
    "    biber_genre = file_df[\"biber_pred_genre_anchor\"].values\n",
    "    doc_ids = file_df[\"documentID_anchor\"].values  # or unique ID\n",
    "\n",
    "    # ---------------------\n",
    "    # Batch parameters\n",
    "    # ---------------------\n",
    "    batch_query_needed_size = int(0.3 * args.batch_size)\n",
    "    batch_candidates_needed_size = int(0.3 * args.batch_size)\n",
    "    print(\n",
    "        f\"Generating batches with batch_query_needed_size={batch_query_needed_size} \"\n",
    "        f\"and batch_candidates_needed_size={batch_candidates_needed_size}\"\n",
    "    )\n",
    "\n",
    "    # We'll count how many batches we produce & how many rows total\n",
    "    batch_id = args.batch_id\n",
    "    batch_id_ctr = 0\n",
    "    total_rows = 0\n",
    "\n",
    "    # File path to write out batches\n",
    "    timestamp = int(time.time())\n",
    "    output_path_mixedbatches = os.path.join(args.model_op_dir, args.op_file_name)\n",
    "\n",
    "    # We'll do a random object with a seed based on epoch\n",
    "    epoch_seed = getattr(epoch_tracker, \"epoch\", 0)\n",
    "    rng = Random(epoch_seed)\n",
    "\n",
    "    while True:\n",
    "        # ----------------------------------------------\n",
    "        # 4.1. Pick queries from docs with sbert_sim < threshold & still available\n",
    "        # ----------------------------------------------\n",
    "\n",
    "        # Distribution of biber_pred_genre is not the same, there are some genres with very few samples, while some with more. While selecting queries, for a particular genre, if the number of samples remaining is less than the batch_query_needed_size, then we can skip that genre and move to the next one, until we dont find for multiple batches\n",
    "        # We can also use a random seed to ensure we get different genres each time\n",
    "        local_genres = list(uniq_biber_genres)  # make a copy so global list isn't modified\n",
    "        selected_biber_genre = None\n",
    "        while local_genres:\n",
    "            genre_try = rng.choice(local_genres)\n",
    "            genre_mask = (biber_genre == genre_try)\n",
    "            sim_mask = (sbert_sim < args.sem_sim_threshold)\n",
    "            genre_count = np.sum(genre_mask & available_mask & sim_mask)\n",
    "            if genre_count >= args.batch_size:\n",
    "                selected_biber_genre = genre_try\n",
    "                break\n",
    "            else:\n",
    "                local_genres.remove(genre_try)\n",
    "\n",
    "        if selected_biber_genre is None:\n",
    "            print(f\"Breaking at batch_id_ctr={batch_id_ctr}, batch_id={batch_id} due to no valid genres with enough available docs.\")\n",
    "            break\n",
    "\n",
    "        # Now we have a selected genre with enough samples\n",
    "        # Filter the valid queries based on the selected genre\n",
    "        valid_query_mask = (sbert_sim < args.sem_sim_threshold) & (available_mask) & (biber_genre == selected_biber_genre)\n",
    "        valid_queries = np.nonzero(valid_query_mask)[0]  # array of doc indices\n",
    "\n",
    "        if len(valid_queries) < batch_query_needed_size:\n",
    "            print(f\"Not enough queries for genre %s, available count: %d\", selected_biber_genre, len(valid_queries))\n",
    "            # Not enough queries to form a batch => break\n",
    "            break\n",
    "\n",
    "        # Sample the queries\n",
    "        chosen_query_indices = rng.sample(list(valid_queries), batch_query_needed_size)\n",
    "        # Mark them unavailable\n",
    "        for idx in chosen_query_indices:\n",
    "            available_mask[idx] = False\n",
    "\n",
    "        # Collect their authors\n",
    "        chosen_query_authors = set(author_ids[idx] for idx in chosen_query_indices)\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # 4.2. For each query, search top-K in FAISS\n",
    "        # ----------------------------------------------\n",
    "        chosen_query_embeddings = all_embeddings[chosen_query_indices, :]\n",
    "        # Instead of searching entire corpus_size, limit to a top-K\n",
    "        top_k = 1000\n",
    "        distances, neighbors = index.search(chosen_query_embeddings, top_k)\n",
    "\n",
    "        # We'll gather candidate IDs from these searches\n",
    "        candidate_indices_collector = []\n",
    "        max_cands_per_query = max(1, batch_candidates_needed_size // batch_query_needed_size)\n",
    "\n",
    "        used_authors_in_batch = set(chosen_query_authors)\n",
    "\n",
    "        for i, q_idx in enumerate(chosen_query_indices):\n",
    "            q_author = author_ids[q_idx]\n",
    "            picks_for_this_query = 0\n",
    "\n",
    "            for rank, cand_id in enumerate(neighbors[i]):\n",
    "                if cand_id == -1:\n",
    "                    # FAISS sometimes returns -1 if no neighbor\n",
    "                    continue\n",
    "\n",
    "                # Already used in a prior batch?\n",
    "                if not available_mask[cand_id]:\n",
    "                    continue\n",
    "\n",
    "                # Check similarity threshold\n",
    "                sim_score = distances[i][rank]\n",
    "                if sim_score < args.sty_sim_threshold:\n",
    "                    continue\n",
    "\n",
    "                cand_author = author_ids[cand_id]\n",
    "                # Must differ from query author & not used in this batch\n",
    "                if cand_author == q_author or cand_author in used_authors_in_batch:\n",
    "                    continue\n",
    "\n",
    "                # Check if the candidate belongs to the same biber genre\n",
    "                if biber_genre[cand_id] != selected_biber_genre:\n",
    "                    continue\n",
    "\n",
    "                # If it passes all checks, we pick it\n",
    "                candidate_indices_collector.append(cand_id)\n",
    "                used_authors_in_batch.add(cand_author)\n",
    "                picks_for_this_query += 1\n",
    "\n",
    "                if picks_for_this_query >= max_cands_per_query:\n",
    "                    break  # done for this query\n",
    "\n",
    "        # We might have more than needed => sample down\n",
    "        if len(candidate_indices_collector) > batch_candidates_needed_size:\n",
    "            candidate_indices_collector = rng.sample(candidate_indices_collector, batch_candidates_needed_size)\n",
    "\n",
    "        # Mark them unavailable\n",
    "        for idx in candidate_indices_collector:\n",
    "            available_mask[idx] = False\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # 4.3. Random docs to fill remainder of the batch\n",
    "        # ----------------------------------------------\n",
    "        batch_query_size = len(chosen_query_indices)\n",
    "        batch_cand_size = len(candidate_indices_collector)\n",
    "        random_needed = args.batch_size - batch_query_size - batch_cand_size\n",
    "\n",
    "        random_indices_chosen = []\n",
    "        if random_needed > 0:\n",
    "            genre_avail_mask = available_mask & (biber_genre == selected_biber_genre)\n",
    "            # Restrict available docs to same genre\n",
    "            avails = np.nonzero(genre_avail_mask)[0]\n",
    "\n",
    "            if len(avails) < random_needed:\n",
    "                print(f\"Not enough random docs for genre %s, available count: %d\", selected_biber_genre, len(avails))\n",
    "                # Not enough docs => break\n",
    "                break\n",
    "\n",
    "        # used_authors_in_batch = set(author_ids[np.concatenate([chosen_query_indices, candidate_indices_collector])])\n",
    "        used_authors_in_batch = set(author_ids[np.concatenate(\n",
    "            [\n",
    "                np.array(chosen_query_indices, dtype=int), \n",
    "                np.array(candidate_indices_collector, dtype=int)\n",
    "            ]\n",
    "        )])\n",
    "\n",
    "\n",
    "        for idx in rng.sample(list(avails), len(avails)):\n",
    "            if author_ids[idx] in used_authors_in_batch:\n",
    "                continue\n",
    "            random_indices_chosen.append(idx)\n",
    "            used_authors_in_batch.add(author_ids[idx])\n",
    "            if len(random_indices_chosen) == random_needed:\n",
    "                break\n",
    "\n",
    "\n",
    "        all_batch_indices = chosen_query_indices + candidate_indices_collector + random_indices_chosen\n",
    "        # If we can't form a full batch, break\n",
    "        if len(all_batch_indices) < args.batch_size:\n",
    "            print(f\"Breaking at batch_id_ctr={batch_id_ctr}, batch_id={batch_id} due to insufficient rows.\")\n",
    "            break\n",
    "\n",
    "\n",
    "        for idx in random_indices_chosen:\n",
    "            available_mask[idx] = False\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # 4.4. Construct the DataFrame for the batch & write it\n",
    "        # ----------------------------------------------\n",
    "        # Build the batch DF\n",
    "        batch_df = file_df.iloc[all_batch_indices].copy()\n",
    "        batch_df[\"batch_id\"] = batch_id\n",
    "\n",
    "        # Write this batch to the JSONL file in append mode\n",
    "        # Each row has a field \"batch_id\"\n",
    "        # batch_df.to_json(output_path_mixedbatches, orient=\"records\", lines=True, mode=\"a\")\n",
    "        with open(output_path_mixedbatches, \"a\") as f:\n",
    "            batch_df.to_json(f, orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "        batch_id += 1\n",
    "        batch_id_ctr += 1\n",
    "        total_rows += len(batch_df)\n",
    "\n",
    "        print(f\"\\r[Progress] Generated batch_id_ctr={batch_id_ctr}, batch_id={batch_id}, total_rows={total_rows}, left={available_mask.sum()} \", end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\nDone generating batches.\")\n",
    "    args.batch_id = batch_id\n",
    "    print(f\"Total batches: {batch_id_ctr}, batch_id: {batch_id}, args.batch_id: {args.batch_id}  | Total rows: {total_rows}\")\n",
    "\n",
    "## Create args namespace with necessary attributes\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 32\n",
    "        self.sem_sim_threshold = 0.2\n",
    "        self.sty_sim_threshold = 0.3\n",
    "        self.model_op_dir = \"/data/araghavan/HIATUS/datadreamer-ta2/data/biber/genre_data/\"\n",
    "        self.op_file_name = f\"biber_train_sadiri_anchorpos_cachedhardmixbatches_epochs3_{int(time.time())}.jsonl\"\n",
    "        self.batch_id = 0\n",
    "\n",
    "args = Args()\n",
    "file_df = biber_data_genre_emb_anchorpos_df.copy()\n",
    "uniq_biber_genres = biber_data_genre_emb_anchorpos_df.biber_pred_genre_anchor.unique()\n",
    "get_anchorpos_biber_hardmix_batches_optimized(file_df, \"train\", args, uniq_biber_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36194"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.batch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_tracker['epoch'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called get_anchorpos_biber_hardmix_batches_optimized with split: train\n",
      "Passed df with shape: (807442, 102)\n",
      "Generating batches with batch_query_needed_size=9 and batch_candidates_needed_size=9\n",
      "[Progress] Generated batch_id_ctr=13725, batch_id=36194, total_rows=439200, left=368242 Breaking at batch_id_ctr=13725, batch_id=36194 due to no valid genres with enough available docs.\n",
      "\n",
      "Done generating batches.\n",
      "Total batches: 13725, batch_id: 36194, args.batch_id: 36194  | Total rows: 439200\n"
     ]
    }
   ],
   "source": [
    "args.sem_sim_threshold = 0.05\n",
    "args.sty_sim_threshold = 0.4\n",
    "get_anchorpos_biber_hardmix_batches_optimized(file_df, \"train\", args, uniq_biber_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called get_anchorpos_biber_hardmix_batches_optimized with split: train\n",
      "Passed df with shape: (807442, 102)\n",
      "Generating batches with batch_query_needed_size=9 and batch_candidates_needed_size=9\n",
      "[Progress] Generated batch_id_ctr=8394, batch_id=44588, total_rows=268608, left=538834 Breaking at batch_id_ctr=8394, batch_id=44588 due to no valid genres with enough available docs.\n",
      "\n",
      "Done generating batches.\n",
      "Total batches: 8394, batch_id: 44588, args.batch_id: 44588  | Total rows: 268608\n"
     ]
    }
   ],
   "source": [
    "args.sem_sim_threshold = 0\n",
    "args.sty_sim_threshold = 0.5\n",
    "epoch_tracker['epoch'] = 69\n",
    "get_anchorpos_biber_hardmix_batches_optimized(file_df, \"train\", args, uniq_biber_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

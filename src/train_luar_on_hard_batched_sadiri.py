from sadiri_training import get_data_generator_hard_batches, DataSource, DataDreamer, get_luar_trainer, LoraConfig, EpochTrackerCallback, losses
import os


def train_datadreamer_ta2_on_hard_batched_data(fold, output_folder, used_loss, batches_path,
                                               luar_model_path='./rrivera1849', batch_size=128, epochs=25):
    """
    Train using hard-batched data loaded from batches.json.

    Parameters:
        fold (str): A placeholder pattern for the fold (not used in this generator).
        output_folder (str): Directory to store outputs.
        used_loss (str): Loss type (dummy placeholder here).
        batches_path (str): Path to the JSON file containing batches.
        luar_model_path (str): Path to the LUAR model.
        batch_size (int): Batch size used in training.
        epochs (int): Number of epochs.
    """
    # Get data generators for training and dev (using the same batches for now as placeholder)
    train_num_batches, train_data_generator = get_data_generator_hard_batches(batches_path, "train")
    dev_num_batches, dev_data_generator = get_data_generator_hard_batches(batches_path, "dev")

    with DataDreamer(output_folder):
        dataset = DataSource(
            "Train Data",
            data=train_data_generator(),
            total_num_rows=train_num_batches,
        )
        dev_dataset = DataSource(
            "Dev Data",
            data=dev_data_generator(),
            total_num_rows=dev_num_batches,
        )

        trainer = get_luar_trainer()(
            "LUAR Trainer",
            model_name=luar_model_path,
            peft_config=LoraConfig(),
            trust_remote_code=True,
            device='cuda',
            dtype="bfloat16",
            force=False,  # so we can resume training if things shut down
        )
        trainer.train_with_positive_pairs(
            train_anchors=dataset.output["anchors"],
            train_positives=dataset.output["positives"],
            validation_anchors=dev_dataset.output["anchors"],
            validation_positives=dev_dataset.output["positives"],
            epochs=epochs,
            batch_size=batch_size,
            loss=losses.MultipleNegativesSymmetricRankingLoss,
            learning_rate=0.0005,
            early_stopping_threshold=0.001,
            early_stopping_patience=5,
            eval_strategy='steps',
            logging_strategy='steps',
            save_strategy='steps',
            logging_steps=200,
            save_steps=600,
            eval_steps=600,
            resume_from_checkpoint=False,
            overwrite_output_dir=True,
            accelerator_config={"dispatch_batches": False},
            callbacks=[EpochTrackerCallback()]
        )

        trainer.export_to_disk(output_folder + 'final_model', adapter_only=False)


# Example usage:
# Assume that batches.json has been generated by your hard_negative_batching function.
output_path = '../output'
luar_model_path = '../training_source/rrivera1849/LUAR-MUD'
fold = "../data/{split}_sadiri_processed_with_embeddings_wo_ao3_filtered.jsonl"
model_op_dir = os.path.join(output_path, 'sadiri_hard_batch_model_v1')

train_datadreamer_ta2_on_hard_batched_data(
    fold=fold,
    output_folder=model_op_dir,
    used_loss='MultipleNegativesSymmetricRankingLoss',  # dummy placeholder
    batches_path="../output/batches.json",
    luar_model_path=luar_model_path,
    batch_size=64,
    epochs=3
)
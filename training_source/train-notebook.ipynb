{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc1b260-9fd1-4076-a90b-ba63f8b8ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac0f8f8-c3fb-4016-bfff-211fdf261c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a977db5-ba6a-41ab-986a-368dc42cb39c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from trainer import *\n",
    "from SupContrastLoss import MultiPosConLoss\n",
    "from RankingLoss import MultipleNegativesSymmetricRankingLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d7b12fdc-2526-42ec-a8f2-59d6dd7013a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_datadreamer_ta2(fold, output_folder, used_loss, luar_model_path='./rrivera1849', batch_size=128, epochs=25):\n",
    "    train_num_rows, train_data_generator = get_performs_data_generator(fold, \"train\")\n",
    "    dev_num_rows, dev_data_generator = get_performs_data_generator(fold, \"dev\")\n",
    "    \n",
    "    with DataDreamer(output_folder):\n",
    "        dataset = DataSource(\n",
    "            \"Train Data\",\n",
    "            data=train_data_generator,\n",
    "            total_num_rows=train_num_rows,\n",
    "        )\n",
    "        dev_dataset = DataSource(\n",
    "            \"Dev Data\",\n",
    "            data=dev_data_generator,\n",
    "            total_num_rows=dev_num_rows,\n",
    "        )\n",
    "    \n",
    "        trainer = get_luar_trainer()(\n",
    "            \"LUAR Trainer\",\n",
    "            model_name=luar_model_path,\n",
    "            peft_config=LoraConfig(),\n",
    "            trust_remote_code=True,\n",
    "            device='cuda:0',\n",
    "            dtype=\"bfloat16\",\n",
    "            force=False, #so we can resume training\n",
    "        )\n",
    "\n",
    "        #loss = SupConLoss if used_loss=='SupConLoss' else losses.MultipleNegativesSymmetricRankingLoss\n",
    "        loss = MultiPosConLoss if used_loss=='SupConLoss' else losses.MultipleNegativesSymmetricRankingLoss\n",
    "\n",
    "        if used_loss == 'SupConLoss':\n",
    "            trainer.train_with_labeled_pairs(\n",
    "                train_anchors=dataset.output[\"anchors\"],\n",
    "                train_others= dataset.output[\"others\"],\n",
    "                train_labels=dataset.output[\"labels\"],\n",
    "                validation_anchors=dev_dataset.output[\"anchors\"],\n",
    "                validation_others=dev_dataset.output[\"others\"],\n",
    "                validation_labels=dev_dataset.output[\"labels\"],\n",
    "                epochs=epochs,    \n",
    "                batch_size=batch_size,\n",
    "                logging_steps=0.2,\n",
    "                loss=loss,\n",
    "                learning_rate=0.0005,\n",
    "                early_stopping_threshold=0.001,\n",
    "                early_stopping_patience=5,\n",
    "                accelerator_config={\n",
    "                    \"dispatch_batches\": False,\n",
    "                },\n",
    "                callbacks=[EpochTrackerCallback()]\n",
    "            )\n",
    "        else:\n",
    "            trainer.train_with_positive_pairs(\n",
    "                train_anchors=dataset.output[\"anchors\"],\n",
    "                train_positives=dataset.output[\"positives\"],\n",
    "                validation_anchors=dev_dataset.output[\"anchors\"],\n",
    "                validation_positives=dev_dataset.output[\"positives\"],\n",
    "                epochs=epochs,    \n",
    "                batch_size=batch_size,\n",
    "                loss=loss,\n",
    "                learning_rate=0.0005,\n",
    "                early_stopping_threshold=0.001,\n",
    "                early_stopping_patience=5,\n",
    "                accelerator_config={\n",
    "                    \"dispatch_batches\": False,\n",
    "                },\n",
    "                callbacks=[EpochTrackerCallback()]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a31adc-ec8c-414d-a5e3-638e49ee5981",
   "metadata": {},
   "source": [
    "### Training original TA2 system on performers' data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc4dee6-f8c6-4d4a-82d8-2ae20c90ed4f",
   "metadata": {},
   "source": [
    "- Train 3 epochs on the performers data\n",
    "- Resume training on HRS dataset from phase 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22c5d595-9d25-46c7-8dc9-afe487739232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_datadreamer_ta2__on_performers_data(fold, output_folder, used_loss, luar_model_path='./rrivera1849', batch_size=128, epochs=25):\n",
    "    train_num_rows, train_data_generator = get_performs_data_generator(fold, \"train\")\n",
    "    dev_num_rows, dev_data_generator = get_performs_data_generator(fold, \"dev\")\n",
    "    \n",
    "    with DataDreamer(output_folder):\n",
    "        dataset = DataSource(\n",
    "            \"Train Data\",\n",
    "            data=train_data_generator,\n",
    "            total_num_rows=train_num_rows,\n",
    "        )\n",
    "        dev_dataset = DataSource(\n",
    "            \"Dev Data\",\n",
    "            data=dev_data_generator,\n",
    "            total_num_rows=dev_num_rows,\n",
    "        )\n",
    "    \n",
    "        trainer = get_luar_trainer()(\n",
    "            \"LUAR Trainer\",\n",
    "            model_name=luar_model_path,\n",
    "            peft_config=LoraConfig(),\n",
    "            trust_remote_code=True,\n",
    "            device='cuda:0',\n",
    "            dtype=\"bfloat16\",\n",
    "            force=False, #so we can resume training if things shutsdown\n",
    "        )\n",
    "        \n",
    "        trainer.train_with_positive_pairs(\n",
    "            train_anchors=dataset.output[\"anchors\"],\n",
    "            train_positives=dataset.output[\"positives\"],\n",
    "            validation_anchors=dev_dataset.output[\"anchors\"],\n",
    "            validation_positives=dev_dataset.output[\"positives\"],\n",
    "            epochs=epochs,    \n",
    "            batch_size=batch_size,\n",
    "            loss=losses.MultipleNegativesSymmetricRankingLoss,\n",
    "            learning_rate=0.0005,\n",
    "            early_stopping_threshold=0.001,\n",
    "            early_stopping_patience=5,\n",
    "            eval_strategy='steps',\n",
    "            logging_strategy='steps',\n",
    "            save_strategy='steps',\n",
    "            logging_steps=200,\n",
    "            save_steps=600,\n",
    "            eval_steps=200,\n",
    "            save_total_limit=3,\n",
    "            resume_from_checkpoint=True,\n",
    "            overwrite_output_dir=False,\n",
    "            accelerator_config={\n",
    "                \"dispatch_batches\": False,\n",
    "            },\n",
    "            callbacks=[EpochTrackerCallback()]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2641421a-26f5-497f-89d4-0b52424eab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path = '/mnt/swordfish-pool2/milad/datadreamer-ta2/'\n",
    "# luar_model_path = '/mnt/swordfish-pool2/milad/rrivera1849'\n",
    "# fold = \"/mnt/swordfish-pool2/milad/hiatus-data/performers-data/tmp-data/*/{split}\"\n",
    "\n",
    "output_path = '/burg/old_dsi/users/ma4608/ajay-ta2-system/output'\n",
    "luar_model_path = '/burg/old_dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849'\n",
    "fold = \"/burg/old_dsi/users/ma4608/hiatus_data/sadiri/*/{split}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc5bc279-1875-4c0f-b465-4e3251a3ebf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/burg/old_dsi/users/ma4608/hiatus_performers_data/sadiri/gmane/train_queries_filtered.jsonl\n",
      "/burg/old_dsi/users/ma4608/hiatus_performers_data/sadiri/realnews/train_queries_filtered.jsonl\n",
      "/burg/old_dsi/users/ma4608/hiatus_performers_data/sadiri/bookcorpus/train_queries_filtered.jsonl\n",
      "/burg/old_dsi/users/ma4608/hiatus_performers_data/sadiri/gmane/train_candidates_filtered.jsonl\n",
      "/burg/old_dsi/users/ma4608/hiatus_performers_data/sadiri/realnews/train_candidates_filtered.jsonl\n",
      "/burg/old_dsi/users/ma4608/hiatus_performers_data/sadiri/bookcorpus/train_candidates_filtered.jsonl\n",
      "Dataset Statistics: train 103605\n",
      "/burg/old_dsi/users/ma4608/hiatus_performers_data/sadiri/gmane/dev_queries_filtered.jsonl\n",
      "/burg/old_dsi/users/ma4608/hiatus_performers_data/sadiri/realnews/dev_queries_filtered.jsonl\n",
      "/burg/old_dsi/users/ma4608/hiatus_performers_data/sadiri/bookcorpus/dev_queries_filtered.jsonl\n",
      "/burg/old_dsi/users/ma4608/hiatus_performers_data/sadiri/gmane/dev_candidates_filtered.jsonl\n",
      "/burg/old_dsi/users/ma4608/hiatus_performers_data/sadiri/realnews/dev_candidates_filtered.jsonl\n",
      "/burg/old_dsi/users/ma4608/hiatus_performers_data/sadiri/bookcorpus/dev_candidates_filtered.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Initialized. ğŸš€ Dreaming to folder: /burg/old_dsi/users/ma4608/ajay-ta2-system/output/original_ta2_performers_data_model_50k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: dev 8284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Train Data' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Train Data' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Dev Data' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Dev Data' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Trainer 'LUAR Trainer' is running (resumed). â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Train Anchors' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Train Anchors' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Train Positives' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Train Positives' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Validation Anchors' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Validation Anchors' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Validation Positives' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Validation Positives' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/old_dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Loading model...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for /burg/old_dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849 contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//burg/old_dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/old_dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Finished loading.\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [ ğŸ¤— Accelerate] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/old_dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.507232401157184 -- {'loss': 1.2535, 'grad_norm': 1.5801076889038086, 'learning_rate': 8.212793314046931e-05}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Dev Data' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Validation Anchors' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Validation Positives' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/old_dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 2.507232401157184 -- {'loss': 1.213529109954834, 'runtime': 187.2039, 'samples_per_second': 44.251, 'steps_per_second': 0.443}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/old_dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.700096432015429 -- {'loss': 1.2424, 'grad_norm': 1.3658034801483154, 'learning_rate': 4.998392799742848e-05}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/old_dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 2.700096432015429 -- {'loss': 1.2037039995193481, 'runtime': 187.3098, 'samples_per_second': 44.226, 'steps_per_second': 0.443}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/old_dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.892960462873674 -- {'loss': 1.239, 'grad_norm': 1.7539148330688477, 'learning_rate': 1.7839922854387655e-05}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/old_dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 2.892960462873674 -- {'loss': 1.200058102607727, 'runtime': 194.9743, 'samples_per_second': 42.488, 'steps_per_second': 0.426}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Train Data' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Train Anchors' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Train Positives' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/old_dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.0 -- {'runtime': 3924.7536, 'samples_per_second': 79.194, 'steps_per_second': 0.793, 'loss': 0.283841335746645}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/old_dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Loading trained model from disk...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for /burg/old_dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849 contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//burg/old_dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/old_dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Finished loading.\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Trainer 'LUAR Trainer' finished and is saved to disk. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Done. âœ¨ Results in folder: /burg/old_dsi/users/ma4608/ajay-ta2-system/output/original_ta2_performers_data_model_50k\n"
     ]
    }
   ],
   "source": [
    "train_datadreamer_ta2__on_performers_data(fold, output_path + '/original_ta2_performers_data_model_50k', 'MultipleNegativesSymmetricRankingLoss', luar_model_path=luar_model_path, batch_size=100, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e15d0aa7-450f-40ab-bfc7-a6e73267e38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6a7fa-1ba9-4d1a-90d5-521ed54d3064",
   "metadata": {},
   "source": [
    "Now training on training split of HRS1&2\n",
    " - For this, I copied original_ta2_performers_data_model_50k to original_ta2_performers_data_model_hrs_continued\n",
    " - Then continued training for further epcohs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4d507270-51f5-45d9-8217-08cb77d2a017",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/burg/old_dsi/users/ma4608/ajay-ta2-system/output'\n",
    "luar_model_path = '/burg/old_dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849'\n",
    "fold = \"/burg/old_dsi/users/ma4608/hiatus_data/hrs_data_combined/{split}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d2c8230f-0179-47a7-92ca-4a1ce7cae3b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['documentID', 'fullText', 'languages', 'lengthWords'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'authorIDs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_num_rows, train_data_generator \u001b[38;5;241m=\u001b[39m get_data_generator_for_combined_hrs(fold, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m dev_num_rows, dev_data_generator \u001b[38;5;241m=\u001b[39m get_data_generator_for_combined_hrs(fold,  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/burg/old_dsi/users/ma4608/ajay-ta2-system/training_source/trainer.py:139\u001b[0m, in \u001b[0;36mget_data_generator_for_combined_hrs\u001b[0;34m(path, fold, split, split_percent)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m jsonlines\u001b[38;5;241m.\u001b[39mopen(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(train_folder_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*_input_queries.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mprint\u001b[39m(line\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m--> 139\u001b[0m     author_id \u001b[38;5;241m=\u001b[39m author_global_ids[line[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthorIDs\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    140\u001b[0m     query_author_ids\u001b[38;5;241m.\u001b[39madd(author_id)\n\u001b[1;32m    141\u001b[0m     author_documents[author_id]\u001b[38;5;241m.\u001b[39mappend(line[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfullText\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'authorIDs'"
     ]
    }
   ],
   "source": [
    "train_num_rows, train_data_generator = get_data_generator_for_combined_hrs(fold, \"\", \"train\")\n",
    "dev_num_rows, dev_data_generator = get_data_generator_for_combined_hrs(fold,  \"\", \"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3727fb29-ed90-4c32-b6fe-cf7319230afd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Initialized. ğŸš€ Dreaming to folder: /burg/old_dsi/users/ma4608/ajay-ta2-system/output/original_ta2_performers_data_model_hrs_continued\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Train Data' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Train Data' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Train Data' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Dev Data' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Dev Data' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Dev Data' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Done. âœ¨ Results in folder: /burg/old_dsi/users/ma4608/ajay-ta2-system/output/original_ta2_performers_data_model_hrs_continued\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: train 0\n",
      "Dataset Statistics: dev 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected single column only, got []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_datadreamer_ta2(fold, output_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/original_ta2_performers_data_model_hrs_continued\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultipleNegativesSymmetricRankingLoss\u001b[39m\u001b[38;5;124m'\u001b[39m, luar_model_path\u001b[38;5;241m=\u001b[39mluar_model_path, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 52\u001b[0m, in \u001b[0;36mtrain_datadreamer_ta2\u001b[0;34m(fold, output_folder, used_loss, luar_model_path, batch_size, epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain_with_labeled_pairs(\n\u001b[1;32m     32\u001b[0m         train_anchors\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manchors\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     33\u001b[0m         train_others\u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mothers\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[EpochTrackerCallback()]\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain_with_positive_pairs(\n\u001b[0;32m---> 52\u001b[0m         train_anchors\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manchors\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     53\u001b[0m         train_positives\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositives\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     54\u001b[0m         validation_anchors\u001b[38;5;241m=\u001b[39mdev_dataset\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manchors\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     55\u001b[0m         validation_positives\u001b[38;5;241m=\u001b[39mdev_dataset\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositives\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     56\u001b[0m         epochs\u001b[38;5;241m=\u001b[39mepochs,    \n\u001b[1;32m     57\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     58\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m     59\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m,\n\u001b[1;32m     60\u001b[0m         early_stopping_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m     61\u001b[0m         early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     62\u001b[0m         accelerator_config\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdispatch_batches\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     64\u001b[0m         },\n\u001b[1;32m     65\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[EpochTrackerCallback()]\n\u001b[1;32m     66\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datadreamer/datasets/datasets.py:77\u001b[0m, in \u001b[0;36mOutputDatasetMixin.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m OutputDatasetColumn(\n\u001b[1;32m     71\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step,  \u001b[38;5;66;03m# type:ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mselect_columns([key]),  \u001b[38;5;66;03m# type:ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     73\u001b[0m             pickled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pickled  \u001b[38;5;66;03m# type:ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m feature_is_pickle_type,\n\u001b[1;32m     75\u001b[0m         )\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m OutputIterableDatasetColumn(\n\u001b[1;32m     78\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step,  \u001b[38;5;66;03m# type:ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     79\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mselect_columns([key]),  \u001b[38;5;66;03m# type:ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     80\u001b[0m             pickled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pickled  \u001b[38;5;66;03m# type:ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     81\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m feature_is_pickle_type,\n\u001b[1;32m     82\u001b[0m             total_num_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_num_rows,  \u001b[38;5;66;03m# type:ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     83\u001b[0m         )\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pickled \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pickled_inferred:  \u001b[38;5;66;03m# type:ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datadreamer/datasets/datasets.py:286\u001b[0m, in \u001b[0;36mOutputIterableDatasetColumn.__init__\u001b[0;34m(self, step, dataset, pickled, total_num_rows)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected single column only, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected single column only, got []"
     ]
    }
   ],
   "source": [
    "train_datadreamer_ta2(fold, output_path + '/original_ta2_performers_data_model_hrs_continued', 'MultipleNegativesSymmetricRankingLoss', luar_model_path=luar_model_path, batch_size=100, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d5eef-275d-4458-a919-5830fe79e46f",
   "metadata": {},
   "source": [
    "### Train original TA2 system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efe4397e-c25b-4306-b335-177771705eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path = '/mnt/swordfish-pool2/milad/datadreamer-ta2/'\n",
    "# luar_model_path = '/mnt/swordfish-pool2/milad/rrivera1849'\n",
    "output_path = '/burg/dsi/users/ma4608/ajay-ta2-system/output'\n",
    "luar_model_path = '/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10dd4486-104d-4ecf-852b-5f22e2991c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fold = \"/mnt/swordfish-pool2/milad/hiatus-data/train-test-dev split/{split}/Official Query-candidate format/\"\n",
    "fold = \"/burg/dsi/users/ma4608/ajay-ta2-system/training_source/data/train-test-dev-split/{split}/official-query-candidate-format/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3450f7a9-d3e4-4f0b-8b0f-cf91bb5fe69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: cross_genre_all train None 575 4142\n",
      "Dataset Statistics: cross_genre_all dev None 79 635\n"
     ]
    }
   ],
   "source": [
    "train_num_rows, train_data_generator = get_data_generator(fold, \"cross_genre_all\", \"train\", split_percent=ast.literal_eval(\"None\"))\n",
    "dev_num_rows, dev_data_generator = get_data_generator(fold, \"cross_genre_all\", \"dev\", split_percent=ast.literal_eval(\"None\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc6306a9-e13e-4790-b379-1dc425b34707",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Initialized. ğŸš€ Dreaming to folder: /burg/dsi/users/ma4608/ajay-ta2-system/output/original_ta2_model\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Train Data' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Train Data' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Dev Data' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Dev Data' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Trainer 'LUAR Trainer' was previously run and saved, but was outdated. ğŸ˜\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Trainer 'LUAR Trainer' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Train Anchors' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Train Anchors' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Train Positives' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Train Positives' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Validation Anchors' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Dev Data' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Validation Anchors' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Validation Positives' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Validation Positives' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Loading model...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for /burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849 contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Finished loading.\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [ ğŸ¤— Accelerate] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Validation Anchors' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Validation Positives' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 0.0 -- {'loss': 2.626582384109497, 'model_preparation_time': 0.0923, 'runtime': 9.619, 'samples_per_second': 16.426, 'steps_per_second': 0.312}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 0.05555555555555555 -- {'loss': 2.4062, 'grad_norm': 0.5505410432815552, 'learning_rate': 0.0004988888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 0.1111111111111111 -- {'loss': 2.8125, 'grad_norm': 0.8742350339889526, 'learning_rate': 0.0004977777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 0.16666666666666666 -- {'loss': 2.5938, 'grad_norm': 0.6774972081184387, 'learning_rate': 0.0004966666666666666}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 0.2222222222222222 -- {'loss': 3.0, 'grad_norm': 0.7084795236587524, 'learning_rate': 0.0004955555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 0.2777777777777778 -- {'loss': 2.1562, 'grad_norm': 0.8386009931564331, 'learning_rate': 0.0004944444444444445}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 0.3333333333333333 -- {'loss': 2.5, 'grad_norm': 0.743576169013977, 'learning_rate': 0.0004933333333333334}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 0.3888888888888889 -- {'loss': 2.2656, 'grad_norm': 0.7609158158302307, 'learning_rate': 0.0004922222222222222}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 0.4444444444444444 -- {'loss': 2.25, 'grad_norm': 0.8400083184242249, 'learning_rate': 0.0004911111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 0.5 -- {'loss': 2.2812, 'grad_norm': 0.8323845863342285, 'learning_rate': 0.00049}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 0.5555555555555556 -- {'loss': 2.25, 'grad_norm': 0.9417882561683655, 'learning_rate': 0.0004888888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 0.6111111111111112 -- {'loss': 1.9922, 'grad_norm': 0.8537153005599976, 'learning_rate': 0.0004877777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 0.6666666666666666 -- {'loss': 1.9141, 'grad_norm': 0.8937997221946716, 'learning_rate': 0.0004866666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 0.7222222222222222 -- {'loss': 1.7109, 'grad_norm': 0.9777269959449768, 'learning_rate': 0.0004855555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 0.7777777777777778 -- {'loss': 2.1562, 'grad_norm': 1.04523503780365, 'learning_rate': 0.00048444444444444446}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Train Data' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 0.8333333333333334 -- {'loss': 1.9844, 'grad_norm': 1.262719750404358, 'learning_rate': 0.00048333333333333334}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 0.8888888888888888 -- {'loss': 1.875, 'grad_norm': 1.0282317399978638, 'learning_rate': 0.0004822222222222222}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Train Anchors' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Train Positives' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 0.9444444444444444 -- {'loss': 2.2188, 'grad_norm': 1.1788305044174194, 'learning_rate': 0.0004811111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 1.0 -- {'loss': 2.125, 'grad_norm': 1.3439730405807495, 'learning_rate': 0.00048}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 1.0 -- {'loss': 2.109473943710327, 'model_preparation_time': 0.0923, 'runtime': 4.1576, 'samples_per_second': 38.003, 'steps_per_second': 0.722}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 1.0555555555555556 -- {'loss': 1.75, 'grad_norm': 1.1149348020553589, 'learning_rate': 0.0004788888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 1.1111111111111112 -- {'loss': 1.9375, 'grad_norm': 1.2737936973571777, 'learning_rate': 0.0004777777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 1.1666666666666667 -- {'loss': 1.8984, 'grad_norm': 1.421396017074585, 'learning_rate': 0.0004766666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 1.2222222222222223 -- {'loss': 1.9531, 'grad_norm': 1.261631965637207, 'learning_rate': 0.00047555555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 1.2777777777777777 -- {'loss': 1.4531, 'grad_norm': 1.0623539686203003, 'learning_rate': 0.00047444444444444444}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 1.3333333333333333 -- {'loss': 2.2188, 'grad_norm': 1.2418233156204224, 'learning_rate': 0.00047333333333333336}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 1.3888888888888888 -- {'loss': 1.6875, 'grad_norm': 1.0069092512130737, 'learning_rate': 0.00047222222222222224}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 1.4444444444444444 -- {'loss': 1.7812, 'grad_norm': 1.0805723667144775, 'learning_rate': 0.0004711111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 1.5 -- {'loss': 1.8594, 'grad_norm': 1.3406187295913696, 'learning_rate': 0.00047}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 1.5555555555555556 -- {'loss': 1.3438, 'grad_norm': 0.9812130928039551, 'learning_rate': 0.0004688888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 1.6111111111111112 -- {'loss': 2.4062, 'grad_norm': 1.3821672201156616, 'learning_rate': 0.0004677777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 1.6666666666666665 -- {'loss': 1.8594, 'grad_norm': 1.254576325416565, 'learning_rate': 0.00046666666666666666}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 1.7222222222222223 -- {'loss': 2.0781, 'grad_norm': 1.3436551094055176, 'learning_rate': 0.0004655555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 1.7777777777777777 -- {'loss': 2.0625, 'grad_norm': 1.119897484779358, 'learning_rate': 0.00046444444444444446}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 1.8333333333333335 -- {'loss': 1.4375, 'grad_norm': 1.219364881515503, 'learning_rate': 0.00046333333333333334}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 1.8888888888888888 -- {'loss': 1.6719, 'grad_norm': 1.0600916147232056, 'learning_rate': 0.0004622222222222222}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 1.9444444444444444 -- {'loss': 1.6172, 'grad_norm': 1.1998803615570068, 'learning_rate': 0.00046111111111111114}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.0 -- {'loss': 1.6719, 'grad_norm': 1.2716180086135864, 'learning_rate': 0.00046}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 2.0 -- {'loss': 1.9490704536437988, 'model_preparation_time': 0.0923, 'runtime': 3.8476, 'samples_per_second': 41.064, 'steps_per_second': 0.78}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.0555555555555554 -- {'loss': 1.2344, 'grad_norm': 1.0112214088439941, 'learning_rate': 0.0004588888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.111111111111111 -- {'loss': 1.7969, 'grad_norm': 1.0918045043945312, 'learning_rate': 0.0004577777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.1666666666666665 -- {'loss': 1.6406, 'grad_norm': 1.1268965005874634, 'learning_rate': 0.0004566666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.2222222222222223 -- {'loss': 1.75, 'grad_norm': 1.2502412796020508, 'learning_rate': 0.00045555555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.2777777777777777 -- {'loss': 1.5391, 'grad_norm': 1.1639174222946167, 'learning_rate': 0.00045444444444444444}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.3333333333333335 -- {'loss': 1.6094, 'grad_norm': 1.1905136108398438, 'learning_rate': 0.0004533333333333333}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.388888888888889 -- {'loss': 2.0938, 'grad_norm': 1.2428617477416992, 'learning_rate': 0.00045222222222222224}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.4444444444444446 -- {'loss': 1.6875, 'grad_norm': 1.2004574537277222, 'learning_rate': 0.0004511111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.5 -- {'loss': 1.8438, 'grad_norm': 1.135056972503662, 'learning_rate': 0.00045000000000000004}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.5555555555555554 -- {'loss': 1.7109, 'grad_norm': 1.2300692796707153, 'learning_rate': 0.0004488888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.611111111111111 -- {'loss': 2.0, 'grad_norm': 1.2313834428787231, 'learning_rate': 0.0004477777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.6666666666666665 -- {'loss': 2.0938, 'grad_norm': 1.2711703777313232, 'learning_rate': 0.00044666666666666666}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.7222222222222223 -- {'loss': 1.7734, 'grad_norm': 1.3098137378692627, 'learning_rate': 0.00044555555555555554}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.7777777777777777 -- {'loss': 1.9062, 'grad_norm': 1.4070568084716797, 'learning_rate': 0.0004444444444444444}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.8333333333333335 -- {'loss': 1.6406, 'grad_norm': 1.2299257516860962, 'learning_rate': 0.00044333333333333334}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.888888888888889 -- {'loss': 1.5391, 'grad_norm': 1.1655941009521484, 'learning_rate': 0.00044222222222222227}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 2.9444444444444446 -- {'loss': 2.2188, 'grad_norm': 1.406800389289856, 'learning_rate': 0.00044111111111111114}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.0 -- {'loss': 1.5703, 'grad_norm': 1.208540678024292, 'learning_rate': 0.00044}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 3.0 -- {'loss': 1.880439043045044, 'model_preparation_time': 0.0923, 'runtime': 3.8895, 'samples_per_second': 40.622, 'steps_per_second': 0.771}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.0555555555555554 -- {'loss': 1.75, 'grad_norm': 1.3428285121917725, 'learning_rate': 0.0004388888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.111111111111111 -- {'loss': 1.3906, 'grad_norm': 1.1437816619873047, 'learning_rate': 0.00043777777777777776}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.1666666666666665 -- {'loss': 1.6484, 'grad_norm': 1.3278428316116333, 'learning_rate': 0.00043666666666666664}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.2222222222222223 -- {'loss': 1.5781, 'grad_norm': 1.2311460971832275, 'learning_rate': 0.0004355555555555555}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.2777777777777777 -- {'loss': 1.4844, 'grad_norm': 1.3066294193267822, 'learning_rate': 0.0004344444444444445}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.3333333333333335 -- {'loss': 1.8438, 'grad_norm': 1.4604954719543457, 'learning_rate': 0.00043333333333333337}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.388888888888889 -- {'loss': 1.9375, 'grad_norm': 1.4149836301803589, 'learning_rate': 0.00043222222222222224}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.4444444444444446 -- {'loss': 1.3828, 'grad_norm': 1.3294142484664917, 'learning_rate': 0.0004311111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.5 -- {'loss': 1.4688, 'grad_norm': 1.2779110670089722, 'learning_rate': 0.00043}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.5555555555555554 -- {'loss': 1.5859, 'grad_norm': 1.4545228481292725, 'learning_rate': 0.00042888888888888886}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.611111111111111 -- {'loss': 1.5312, 'grad_norm': 1.3617914915084839, 'learning_rate': 0.0004277777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.6666666666666665 -- {'loss': 1.5156, 'grad_norm': 1.328688383102417, 'learning_rate': 0.0004266666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.7222222222222223 -- {'loss': 1.6406, 'grad_norm': 1.402251958847046, 'learning_rate': 0.0004255555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.7777777777777777 -- {'loss': 1.6094, 'grad_norm': 1.44136643409729, 'learning_rate': 0.00042444444444444447}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.8333333333333335 -- {'loss': 1.8203, 'grad_norm': 1.5480705499649048, 'learning_rate': 0.00042333333333333334}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.888888888888889 -- {'loss': 1.5156, 'grad_norm': 1.4320796728134155, 'learning_rate': 0.0004222222222222222}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 3.9444444444444446 -- {'loss': 1.7812, 'grad_norm': 1.6007887125015259, 'learning_rate': 0.0004211111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 4.0 -- {'loss': 1.3672, 'grad_norm': 1.373823642730713, 'learning_rate': 0.00042}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 4.0 -- {'loss': 1.8251582384109497, 'model_preparation_time': 0.0923, 'runtime': 3.6368, 'samples_per_second': 43.444, 'steps_per_second': 0.825}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 4.055555555555555 -- {'loss': 1.3281, 'grad_norm': 1.4710466861724854, 'learning_rate': 0.0004188888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 4.111111111111111 -- {'loss': 1.125, 'grad_norm': 1.3696095943450928, 'learning_rate': 0.0004177777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 4.166666666666667 -- {'loss': 1.4531, 'grad_norm': 1.4624943733215332, 'learning_rate': 0.0004166666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 4.222222222222222 -- {'loss': 1.3125, 'grad_norm': 1.527000904083252, 'learning_rate': 0.00041555555555555557}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 4.277777777777778 -- {'loss': 1.5547, 'grad_norm': 1.6164062023162842, 'learning_rate': 0.00041444444444444444}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 4.333333333333333 -- {'loss': 1.8438, 'grad_norm': 1.7571704387664795, 'learning_rate': 0.0004133333333333333}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 4.388888888888889 -- {'loss': 1.4844, 'grad_norm': 1.4790754318237305, 'learning_rate': 0.00041222222222222224}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 4.444444444444445 -- {'loss': 1.9688, 'grad_norm': 1.9066694974899292, 'learning_rate': 0.0004111111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 4.5 -- {'loss': 1.5156, 'grad_norm': 1.63901686668396, 'learning_rate': 0.00041}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 4.555555555555555 -- {'loss': 1.375, 'grad_norm': 1.6325663328170776, 'learning_rate': 0.0004088888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 4.611111111111111 -- {'loss': 1.2891, 'grad_norm': 1.4380944967269897, 'learning_rate': 0.0004077777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 4.666666666666667 -- {'loss': 2.0312, 'grad_norm': 1.7383368015289307, 'learning_rate': 0.00040666666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 4.722222222222222 -- {'loss': 1.0625, 'grad_norm': 1.3185914754867554, 'learning_rate': 0.00040555555555555554}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 4.777777777777778 -- {'loss': 1.6875, 'grad_norm': 1.6615781784057617, 'learning_rate': 0.00040444444444444447}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 4.833333333333333 -- {'loss': 1.2891, 'grad_norm': 1.4781476259231567, 'learning_rate': 0.00040333333333333334}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 4.888888888888889 -- {'loss': 1.6094, 'grad_norm': 1.6242398023605347, 'learning_rate': 0.0004022222222222222}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 4.944444444444445 -- {'loss': 1.4688, 'grad_norm': 1.5943913459777832, 'learning_rate': 0.0004011111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 5.0 -- {'loss': 1.7969, 'grad_norm': 1.7101104259490967, 'learning_rate': 0.0004}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 5.0 -- {'loss': 1.79835844039917, 'model_preparation_time': 0.0923, 'runtime': 3.6949, 'samples_per_second': 42.761, 'steps_per_second': 0.812}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 5.055555555555555 -- {'loss': 1.3672, 'grad_norm': 1.5356661081314087, 'learning_rate': 0.0003988888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 5.111111111111111 -- {'loss': 1.6094, 'grad_norm': 1.6750518083572388, 'learning_rate': 0.00039777777777777777}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 5.166666666666667 -- {'loss': 1.4844, 'grad_norm': 1.7386611700057983, 'learning_rate': 0.0003966666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 5.222222222222222 -- {'loss': 1.4766, 'grad_norm': 1.643216609954834, 'learning_rate': 0.00039555555555555557}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 5.277777777777778 -- {'loss': 1.7109, 'grad_norm': 1.5276873111724854, 'learning_rate': 0.00039444444444444444}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 5.333333333333333 -- {'loss': 1.5469, 'grad_norm': 1.5298207998275757, 'learning_rate': 0.0003933333333333333}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 5.388888888888889 -- {'loss': 1.2969, 'grad_norm': 1.6019349098205566, 'learning_rate': 0.00039222222222222225}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 5.444444444444445 -- {'loss': 1.1094, 'grad_norm': 1.4504384994506836, 'learning_rate': 0.0003911111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 5.5 -- {'loss': 1.0938, 'grad_norm': 1.5301119089126587, 'learning_rate': 0.00039000000000000005}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 5.555555555555555 -- {'loss': 1.7656, 'grad_norm': 1.7772431373596191, 'learning_rate': 0.0003888888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 5.611111111111111 -- {'loss': 1.2812, 'grad_norm': 1.5840942859649658, 'learning_rate': 0.0003877777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 5.666666666666667 -- {'loss': 1.9375, 'grad_norm': 1.8244584798812866, 'learning_rate': 0.00038666666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 5.722222222222222 -- {'loss': 1.2578, 'grad_norm': 1.6754778623580933, 'learning_rate': 0.00038555555555555554}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 5.777777777777778 -- {'loss': 1.2031, 'grad_norm': 1.4829845428466797, 'learning_rate': 0.0003844444444444444}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 5.833333333333333 -- {'loss': 1.5391, 'grad_norm': 1.7218081951141357, 'learning_rate': 0.00038333333333333334}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 5.888888888888889 -- {'loss': 1.2969, 'grad_norm': 1.7751493453979492, 'learning_rate': 0.0003822222222222223}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 5.944444444444445 -- {'loss': 1.875, 'grad_norm': 2.010504722595215, 'learning_rate': 0.00038111111111111115}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 6.0 -- {'loss': 1.3516, 'grad_norm': 1.6400490999221802, 'learning_rate': 0.00038}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 6.0 -- {'loss': 1.7918314933776855, 'model_preparation_time': 0.0923, 'runtime': 3.3731, 'samples_per_second': 46.841, 'steps_per_second': 0.889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 6.055555555555555 -- {'loss': 1.4922, 'grad_norm': 1.7688343524932861, 'learning_rate': 0.0003788888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 6.111111111111111 -- {'loss': 1.3906, 'grad_norm': 1.7755942344665527, 'learning_rate': 0.00037777777777777777}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 6.166666666666667 -- {'loss': 1.5625, 'grad_norm': 1.8837120532989502, 'learning_rate': 0.00037666666666666664}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 6.222222222222222 -- {'loss': 1.4688, 'grad_norm': 1.9291132688522339, 'learning_rate': 0.0003755555555555555}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 6.277777777777778 -- {'loss': 1.4453, 'grad_norm': 1.9813145399093628, 'learning_rate': 0.0003744444444444445}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 6.333333333333333 -- {'loss': 1.1719, 'grad_norm': 1.8832225799560547, 'learning_rate': 0.0003733333333333334}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 6.388888888888889 -- {'loss': 1.1406, 'grad_norm': 1.8247289657592773, 'learning_rate': 0.00037222222222222225}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 6.444444444444445 -- {'loss': 1.5781, 'grad_norm': 2.175546646118164, 'learning_rate': 0.0003711111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 6.5 -- {'loss': 1.3984, 'grad_norm': 1.9350755214691162, 'learning_rate': 0.00037}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 6.555555555555555 -- {'loss': 1.2344, 'grad_norm': 1.81903874874115, 'learning_rate': 0.00036888888888888887}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 6.611111111111111 -- {'loss': 1.6641, 'grad_norm': 2.188872814178467, 'learning_rate': 0.00036777777777777774}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 6.666666666666667 -- {'loss': 1.4766, 'grad_norm': 2.06166410446167, 'learning_rate': 0.00036666666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 6.722222222222222 -- {'loss': 1.4375, 'grad_norm': 1.9179127216339111, 'learning_rate': 0.0003655555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 6.777777777777778 -- {'loss': 1.8047, 'grad_norm': 2.207838773727417, 'learning_rate': 0.00036444444444444447}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 6.833333333333333 -- {'loss': 1.7344, 'grad_norm': 2.288649559020996, 'learning_rate': 0.00036333333333333335}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 6.888888888888889 -- {'loss': 1.1328, 'grad_norm': 1.757293462753296, 'learning_rate': 0.0003622222222222222}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 6.944444444444445 -- {'loss': 1.5, 'grad_norm': 2.0253238677978516, 'learning_rate': 0.0003611111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 7.0 -- {'loss': 1.2188, 'grad_norm': 1.826310634613037, 'learning_rate': 0.00035999999999999997}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 7.0 -- {'loss': 1.7492088079452515, 'model_preparation_time': 0.0923, 'runtime': 3.0765, 'samples_per_second': 51.357, 'steps_per_second': 0.975}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 7.055555555555555 -- {'loss': 1.4766, 'grad_norm': 1.7213801145553589, 'learning_rate': 0.0003588888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 7.111111111111111 -- {'loss': 1.5234, 'grad_norm': 1.9910426139831543, 'learning_rate': 0.00035777777777777777}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 7.166666666666667 -- {'loss': 1.4844, 'grad_norm': 1.8513801097869873, 'learning_rate': 0.0003566666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 7.222222222222222 -- {'loss': 1.4219, 'grad_norm': 1.971427321434021, 'learning_rate': 0.00035555555555555557}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 7.277777777777778 -- {'loss': 1.6875, 'grad_norm': 2.0541510581970215, 'learning_rate': 0.00035444444444444445}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 7.333333333333333 -- {'loss': 1.5078, 'grad_norm': 1.958308219909668, 'learning_rate': 0.0003533333333333333}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 7.388888888888889 -- {'loss': 1.3047, 'grad_norm': 1.857793927192688, 'learning_rate': 0.00035222222222222225}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 7.444444444444445 -- {'loss': 1.4688, 'grad_norm': 2.0101656913757324, 'learning_rate': 0.0003511111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 7.5 -- {'loss': 1.2969, 'grad_norm': 1.8949532508850098, 'learning_rate': 0.00035}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 7.555555555555555 -- {'loss': 1.5, 'grad_norm': 1.9533064365386963, 'learning_rate': 0.0003488888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 7.611111111111111 -- {'loss': 1.375, 'grad_norm': 1.7401264905929565, 'learning_rate': 0.0003477777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 7.666666666666667 -- {'loss': 1.3594, 'grad_norm': 2.0193514823913574, 'learning_rate': 0.00034666666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 7.722222222222222 -- {'loss': 1.2109, 'grad_norm': 1.861523985862732, 'learning_rate': 0.00034555555555555555}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 7.777777777777778 -- {'loss': 1.3125, 'grad_norm': 1.8046517372131348, 'learning_rate': 0.0003444444444444445}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 7.833333333333333 -- {'loss': 1.6875, 'grad_norm': 2.2061607837677, 'learning_rate': 0.00034333333333333335}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 7.888888888888889 -- {'loss': 1.2188, 'grad_norm': 1.8669885396957397, 'learning_rate': 0.0003422222222222222}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 7.944444444444445 -- {'loss': 1.5469, 'grad_norm': 2.0189969539642334, 'learning_rate': 0.0003411111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 8.0 -- {'loss': 1.25, 'grad_norm': 2.0335474014282227, 'learning_rate': 0.00034}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 8.0 -- {'loss': 1.7496044635772705, 'model_preparation_time': 0.0923, 'runtime': 4.205, 'samples_per_second': 37.575, 'steps_per_second': 0.713}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 8.055555555555555 -- {'loss': 1.625, 'grad_norm': 2.11570143699646, 'learning_rate': 0.0003388888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 8.11111111111111 -- {'loss': 1.3203, 'grad_norm': 1.7994868755340576, 'learning_rate': 0.00033777777777777777}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 8.166666666666666 -- {'loss': 1.2969, 'grad_norm': 1.8194137811660767, 'learning_rate': 0.0003366666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 8.222222222222221 -- {'loss': 1.5469, 'grad_norm': 2.0895776748657227, 'learning_rate': 0.0003355555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 8.277777777777779 -- {'loss': 1.375, 'grad_norm': 2.098896026611328, 'learning_rate': 0.00033444444444444445}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 8.333333333333334 -- {'loss': 0.9453, 'grad_norm': 1.615260124206543, 'learning_rate': 0.0003333333333333333}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 8.38888888888889 -- {'loss': 1.375, 'grad_norm': 1.9464579820632935, 'learning_rate': 0.0003322222222222222}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 8.444444444444445 -- {'loss': 1.5781, 'grad_norm': 2.3075149059295654, 'learning_rate': 0.0003311111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 8.5 -- {'loss': 1.1719, 'grad_norm': 1.7749344110488892, 'learning_rate': 0.00033}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 8.555555555555555 -- {'loss': 1.4062, 'grad_norm': 2.0970044136047363, 'learning_rate': 0.0003288888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 8.61111111111111 -- {'loss': 1.0625, 'grad_norm': 1.753335952758789, 'learning_rate': 0.0003277777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 8.666666666666666 -- {'loss': 1.3828, 'grad_norm': 2.107781410217285, 'learning_rate': 0.0003266666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 8.722222222222221 -- {'loss': 1.625, 'grad_norm': 2.193129777908325, 'learning_rate': 0.00032555555555555555}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 8.777777777777779 -- {'loss': 1.1875, 'grad_norm': 1.956990361213684, 'learning_rate': 0.0003244444444444444}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 8.833333333333334 -- {'loss': 1.125, 'grad_norm': 2.0898306369781494, 'learning_rate': 0.0003233333333333333}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 8.88888888888889 -- {'loss': 1.5781, 'grad_norm': 2.3328499794006348, 'learning_rate': 0.0003222222222222222}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 8.944444444444445 -- {'loss': 1.2344, 'grad_norm': 1.863913655281067, 'learning_rate': 0.00032111111111111115}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 9.0 -- {'loss': 1.4688, 'grad_norm': 2.0395641326904297, 'learning_rate': 0.00032}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 9.0 -- {'loss': 1.7384295463562012, 'model_preparation_time': 0.0923, 'runtime': 4.0368, 'samples_per_second': 39.14, 'steps_per_second': 0.743}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 9.055555555555555 -- {'loss': 1.1016, 'grad_norm': 2.0191047191619873, 'learning_rate': 0.0003188888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 9.11111111111111 -- {'loss': 1.4219, 'grad_norm': 2.081234931945801, 'learning_rate': 0.0003177777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 9.166666666666666 -- {'loss': 1.2031, 'grad_norm': 1.9072462320327759, 'learning_rate': 0.00031666666666666665}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 9.222222222222221 -- {'loss': 1.1094, 'grad_norm': 1.996065616607666, 'learning_rate': 0.0003155555555555555}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 9.277777777777779 -- {'loss': 1.3281, 'grad_norm': 1.9413288831710815, 'learning_rate': 0.0003144444444444445}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 9.333333333333334 -- {'loss': 1.1094, 'grad_norm': 1.9703205823898315, 'learning_rate': 0.0003133333333333334}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 9.38888888888889 -- {'loss': 1.4375, 'grad_norm': 2.227562189102173, 'learning_rate': 0.00031222222222222225}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 9.444444444444445 -- {'loss': 1.5625, 'grad_norm': 1.9725251197814941, 'learning_rate': 0.0003111111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 9.5 -- {'loss': 1.4609, 'grad_norm': 2.210465908050537, 'learning_rate': 0.00031}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 9.555555555555555 -- {'loss': 1.5703, 'grad_norm': 2.257678747177124, 'learning_rate': 0.0003088888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 9.61111111111111 -- {'loss': 1.1406, 'grad_norm': 1.8871771097183228, 'learning_rate': 0.00030777777777777775}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 9.666666666666666 -- {'loss': 1.125, 'grad_norm': 1.9107744693756104, 'learning_rate': 0.0003066666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 9.722222222222221 -- {'loss': 1.2578, 'grad_norm': 1.920648217201233, 'learning_rate': 0.0003055555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 9.777777777777779 -- {'loss': 1.0469, 'grad_norm': 1.81595778465271, 'learning_rate': 0.0003044444444444445}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 9.833333333333334 -- {'loss': 0.7188, 'grad_norm': 1.6511809825897217, 'learning_rate': 0.00030333333333333335}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 9.88888888888889 -- {'loss': 1.4453, 'grad_norm': 2.2821574211120605, 'learning_rate': 0.0003022222222222222}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 9.944444444444445 -- {'loss': 1.2344, 'grad_norm': 2.0750420093536377, 'learning_rate': 0.0003011111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 10.0 -- {'loss': 1.3281, 'grad_norm': 2.0930633544921875, 'learning_rate': 0.0003}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 10.0 -- {'loss': 1.7432752847671509, 'model_preparation_time': 0.0923, 'runtime': 5.1482, 'samples_per_second': 30.691, 'steps_per_second': 0.583}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 10.055555555555555 -- {'loss': 1.2031, 'grad_norm': 2.1255271434783936, 'learning_rate': 0.0002988888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 10.11111111111111 -- {'loss': 1.4844, 'grad_norm': 2.3687803745269775, 'learning_rate': 0.0002977777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 10.166666666666666 -- {'loss': 1.1484, 'grad_norm': 2.092487096786499, 'learning_rate': 0.0002966666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 10.222222222222221 -- {'loss': 1.5391, 'grad_norm': 2.444385051727295, 'learning_rate': 0.0002955555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 10.277777777777779 -- {'loss': 1.3438, 'grad_norm': 2.1845271587371826, 'learning_rate': 0.00029444444444444445}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 10.333333333333334 -- {'loss': 1.1875, 'grad_norm': 2.071824073791504, 'learning_rate': 0.0002933333333333333}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 10.38888888888889 -- {'loss': 1.3281, 'grad_norm': 2.2509524822235107, 'learning_rate': 0.0002922222222222222}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 10.444444444444445 -- {'loss': 1.3359, 'grad_norm': 2.3109776973724365, 'learning_rate': 0.00029111111111111113}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 10.5 -- {'loss': 1.1797, 'grad_norm': 2.4250328540802, 'learning_rate': 0.00029}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 10.555555555555555 -- {'loss': 1.1406, 'grad_norm': 2.2501888275146484, 'learning_rate': 0.0002888888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 10.61111111111111 -- {'loss': 1.4297, 'grad_norm': 2.410146951675415, 'learning_rate': 0.0002877777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 10.666666666666666 -- {'loss': 1.2344, 'grad_norm': 2.2761147022247314, 'learning_rate': 0.0002866666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 10.722222222222221 -- {'loss': 1.0156, 'grad_norm': 1.9537813663482666, 'learning_rate': 0.00028555555555555555}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 10.777777777777779 -- {'loss': 1.0469, 'grad_norm': 2.0175395011901855, 'learning_rate': 0.0002844444444444444}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 10.833333333333334 -- {'loss': 1.2109, 'grad_norm': 2.140869617462158, 'learning_rate': 0.00028333333333333335}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 10.88888888888889 -- {'loss': 1.1719, 'grad_norm': 2.208239793777466, 'learning_rate': 0.00028222222222222223}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 10.944444444444445 -- {'loss': 1.0078, 'grad_norm': 1.9681413173675537, 'learning_rate': 0.0002811111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 11.0 -- {'loss': 1.4688, 'grad_norm': 2.65921688079834, 'learning_rate': 0.00028000000000000003}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 11.0 -- {'loss': 1.7529667615890503, 'model_preparation_time': 0.0923, 'runtime': 3.9873, 'samples_per_second': 39.626, 'steps_per_second': 0.752}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 11.055555555555555 -- {'loss': 1.4688, 'grad_norm': 2.676218271255493, 'learning_rate': 0.0002788888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 11.11111111111111 -- {'loss': 1.125, 'grad_norm': 2.280761957168579, 'learning_rate': 0.0002777777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 11.166666666666666 -- {'loss': 1.0547, 'grad_norm': 1.9910764694213867, 'learning_rate': 0.00027666666666666665}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 11.222222222222221 -- {'loss': 0.9219, 'grad_norm': 2.226811170578003, 'learning_rate': 0.0002755555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 11.277777777777779 -- {'loss': 1.375, 'grad_norm': 2.2508504390716553, 'learning_rate': 0.00027444444444444445}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 11.333333333333334 -- {'loss': 1.3125, 'grad_norm': 2.2862768173217773, 'learning_rate': 0.00027333333333333333}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 11.38888888888889 -- {'loss': 1.5938, 'grad_norm': 2.4970977306365967, 'learning_rate': 0.0002722222222222222}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 11.444444444444445 -- {'loss': 1.1562, 'grad_norm': 2.4228973388671875, 'learning_rate': 0.00027111111111111113}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 11.5 -- {'loss': 1.3281, 'grad_norm': 2.304417371749878, 'learning_rate': 0.00027}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 11.555555555555555 -- {'loss': 1.0391, 'grad_norm': 2.014716863632202, 'learning_rate': 0.00026888888888888893}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 11.61111111111111 -- {'loss': 1.4219, 'grad_norm': 2.21867299079895, 'learning_rate': 0.0002677777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 11.666666666666666 -- {'loss': 0.9805, 'grad_norm': 2.2266623973846436, 'learning_rate': 0.0002666666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 11.722222222222221 -- {'loss': 1.0859, 'grad_norm': 2.089416742324829, 'learning_rate': 0.00026555555555555555}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 11.777777777777779 -- {'loss': 1.1484, 'grad_norm': 2.105853796005249, 'learning_rate': 0.00026444444444444443}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 11.833333333333334 -- {'loss': 1.0469, 'grad_norm': 1.956687092781067, 'learning_rate': 0.0002633333333333333}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 11.88888888888889 -- {'loss': 1.2266, 'grad_norm': 2.077002763748169, 'learning_rate': 0.00026222222222222223}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 11.944444444444445 -- {'loss': 1.1172, 'grad_norm': 2.091644048690796, 'learning_rate': 0.00026111111111111116}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 12.0 -- {'loss': 1.1719, 'grad_norm': 2.4006776809692383, 'learning_rate': 0.00026000000000000003}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 12.0 -- {'loss': 1.75, 'model_preparation_time': 0.0923, 'runtime': 3.6148, 'samples_per_second': 43.709, 'steps_per_second': 0.83}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 12.055555555555555 -- {'loss': 1.4453, 'grad_norm': 2.5575239658355713, 'learning_rate': 0.0002588888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 12.11111111111111 -- {'loss': 1.0781, 'grad_norm': 2.2194087505340576, 'learning_rate': 0.0002577777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 12.166666666666666 -- {'loss': 1.3984, 'grad_norm': 2.4309704303741455, 'learning_rate': 0.00025666666666666665}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 12.222222222222221 -- {'loss': 1.3594, 'grad_norm': 2.3507747650146484, 'learning_rate': 0.00025555555555555553}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 12.277777777777779 -- {'loss': 1.1406, 'grad_norm': 2.3567376136779785, 'learning_rate': 0.0002544444444444444}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 12.333333333333334 -- {'loss': 1.0625, 'grad_norm': 2.209348678588867, 'learning_rate': 0.0002533333333333334}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 12.38888888888889 -- {'loss': 1.1406, 'grad_norm': 2.2903757095336914, 'learning_rate': 0.00025222222222222226}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 12.444444444444445 -- {'loss': 1.1016, 'grad_norm': 2.2327160835266113, 'learning_rate': 0.00025111111111111113}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 12.5 -- {'loss': 1.2031, 'grad_norm': 2.2971930503845215, 'learning_rate': 0.00025}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 12.555555555555555 -- {'loss': 1.2656, 'grad_norm': 2.372053861618042, 'learning_rate': 0.0002488888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 12.61111111111111 -- {'loss': 1.4688, 'grad_norm': 2.4805195331573486, 'learning_rate': 0.0002477777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 12.666666666666666 -- {'loss': 1.25, 'grad_norm': 2.5182456970214844, 'learning_rate': 0.0002466666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 12.722222222222221 -- {'loss': 1.8594, 'grad_norm': 2.6599197387695312, 'learning_rate': 0.00024555555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 12.777777777777779 -- {'loss': 0.9375, 'grad_norm': 2.150871753692627, 'learning_rate': 0.00024444444444444443}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 12.833333333333334 -- {'loss': 1.0, 'grad_norm': 2.1219968795776367, 'learning_rate': 0.00024333333333333336}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 12.88888888888889 -- {'loss': 1.2344, 'grad_norm': 2.6069958209991455, 'learning_rate': 0.00024222222222222223}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 12.944444444444445 -- {'loss': 1.4766, 'grad_norm': 2.7585601806640625, 'learning_rate': 0.0002411111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 13.0 -- {'loss': 0.9531, 'grad_norm': 2.0403640270233154, 'learning_rate': 0.00024}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 13.0 -- {'loss': 1.7462420463562012, 'model_preparation_time': 0.0923, 'runtime': 3.9163, 'samples_per_second': 40.344, 'steps_per_second': 0.766}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 13.055555555555555 -- {'loss': 0.7188, 'grad_norm': 2.0253326892852783, 'learning_rate': 0.0002388888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 13.11111111111111 -- {'loss': 1.1875, 'grad_norm': 2.2222540378570557, 'learning_rate': 0.00023777777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 13.166666666666666 -- {'loss': 1.1406, 'grad_norm': 2.3122034072875977, 'learning_rate': 0.00023666666666666668}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 13.222222222222221 -- {'loss': 1.4141, 'grad_norm': 2.4363837242126465, 'learning_rate': 0.00023555555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 13.277777777777779 -- {'loss': 1.3359, 'grad_norm': 2.250913143157959, 'learning_rate': 0.00023444444444444446}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 13.333333333333334 -- {'loss': 0.9453, 'grad_norm': 2.1400234699249268, 'learning_rate': 0.00023333333333333333}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 13.38888888888889 -- {'loss': 1.3906, 'grad_norm': 2.9473907947540283, 'learning_rate': 0.00023222222222222223}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 13.444444444444445 -- {'loss': 0.9141, 'grad_norm': 2.0844764709472656, 'learning_rate': 0.0002311111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 13.5 -- {'loss': 1.4766, 'grad_norm': 2.664769172668457, 'learning_rate': 0.00023}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 13.555555555555555 -- {'loss': 1.2422, 'grad_norm': 2.305434465408325, 'learning_rate': 0.0002288888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 13.61111111111111 -- {'loss': 1.0312, 'grad_norm': 2.24920916557312, 'learning_rate': 0.00022777777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 13.666666666666666 -- {'loss': 1.1953, 'grad_norm': 2.5248918533325195, 'learning_rate': 0.00022666666666666666}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 13.722222222222221 -- {'loss': 1.1406, 'grad_norm': 2.1088368892669678, 'learning_rate': 0.00022555555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 13.777777777777779 -- {'loss': 1.5312, 'grad_norm': 2.8998563289642334, 'learning_rate': 0.00022444444444444446}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 13.833333333333334 -- {'loss': 1.2188, 'grad_norm': 2.3629801273345947, 'learning_rate': 0.00022333333333333333}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 13.88888888888889 -- {'loss': 1.375, 'grad_norm': 2.3492558002471924, 'learning_rate': 0.0002222222222222222}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 13.944444444444445 -- {'loss': 1.2734, 'grad_norm': 2.6362249851226807, 'learning_rate': 0.00022111111111111113}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 14.0 -- {'loss': 0.7188, 'grad_norm': 1.9213261604309082, 'learning_rate': 0.00022}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 14.0 -- {'loss': 1.728935956954956, 'model_preparation_time': 0.0923, 'runtime': 4.8396, 'samples_per_second': 32.648, 'steps_per_second': 0.62}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 14.055555555555555 -- {'loss': 1.5156, 'grad_norm': 2.72796893119812, 'learning_rate': 0.00021888888888888888}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 14.11111111111111 -- {'loss': 1.2812, 'grad_norm': 2.393645763397217, 'learning_rate': 0.00021777777777777776}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 14.166666666666666 -- {'loss': 1.0625, 'grad_norm': 2.314666986465454, 'learning_rate': 0.00021666666666666668}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 14.222222222222221 -- {'loss': 1.3828, 'grad_norm': 2.498471736907959, 'learning_rate': 0.00021555555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 14.277777777777779 -- {'loss': 0.9922, 'grad_norm': 2.3077807426452637, 'learning_rate': 0.00021444444444444443}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 14.333333333333334 -- {'loss': 1.0156, 'grad_norm': 2.218552827835083, 'learning_rate': 0.00021333333333333336}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 14.38888888888889 -- {'loss': 0.9219, 'grad_norm': 2.086569309234619, 'learning_rate': 0.00021222222222222223}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 14.444444444444445 -- {'loss': 0.8984, 'grad_norm': 2.124617338180542, 'learning_rate': 0.0002111111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 14.5 -- {'loss': 1.1875, 'grad_norm': 2.6828861236572266, 'learning_rate': 0.00021}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 14.555555555555555 -- {'loss': 1.2344, 'grad_norm': 2.458070755004883, 'learning_rate': 0.0002088888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 14.61111111111111 -- {'loss': 1.2812, 'grad_norm': 2.406280994415283, 'learning_rate': 0.00020777777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 14.666666666666666 -- {'loss': 1.2031, 'grad_norm': 2.3996410369873047, 'learning_rate': 0.00020666666666666666}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 14.722222222222221 -- {'loss': 1.2891, 'grad_norm': 2.5936312675476074, 'learning_rate': 0.00020555555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 14.777777777777779 -- {'loss': 1.4297, 'grad_norm': 2.8305165767669678, 'learning_rate': 0.00020444444444444446}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 14.833333333333334 -- {'loss': 0.7578, 'grad_norm': 2.0507705211639404, 'learning_rate': 0.00020333333333333333}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 14.88888888888889 -- {'loss': 1.0156, 'grad_norm': 2.5337114334106445, 'learning_rate': 0.00020222222222222223}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 14.944444444444445 -- {'loss': 1.0156, 'grad_norm': 2.294502019882202, 'learning_rate': 0.0002011111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 15.0 -- {'loss': 1.0234, 'grad_norm': 2.337120771408081, 'learning_rate': 0.0002}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 15.0 -- {'loss': 1.7715585231781006, 'model_preparation_time': 0.0923, 'runtime': 4.4939, 'samples_per_second': 35.158, 'steps_per_second': 0.668}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 15.055555555555555 -- {'loss': 1.0938, 'grad_norm': 2.4453861713409424, 'learning_rate': 0.00019888888888888888}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 15.11111111111111 -- {'loss': 1.2344, 'grad_norm': 2.675736427307129, 'learning_rate': 0.00019777777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 15.166666666666666 -- {'loss': 1.1172, 'grad_norm': 2.485410690307617, 'learning_rate': 0.00019666666666666666}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 15.222222222222221 -- {'loss': 0.9062, 'grad_norm': 2.3797924518585205, 'learning_rate': 0.00019555555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 15.277777777777779 -- {'loss': 1.2969, 'grad_norm': 2.613401412963867, 'learning_rate': 0.00019444444444444446}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 15.333333333333334 -- {'loss': 1.2812, 'grad_norm': 2.7391321659088135, 'learning_rate': 0.00019333333333333333}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 15.38888888888889 -- {'loss': 1.0938, 'grad_norm': 2.4892783164978027, 'learning_rate': 0.0001922222222222222}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 15.444444444444445 -- {'loss': 0.7891, 'grad_norm': 2.242030382156372, 'learning_rate': 0.00019111111111111114}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 15.5 -- {'loss': 1.5781, 'grad_norm': 3.0850918292999268, 'learning_rate': 0.00019}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 15.555555555555555 -- {'loss': 1.1797, 'grad_norm': 2.6686065196990967, 'learning_rate': 0.00018888888888888888}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 15.61111111111111 -- {'loss': 1.1719, 'grad_norm': 2.6948482990264893, 'learning_rate': 0.00018777777777777776}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 15.666666666666666 -- {'loss': 1.0156, 'grad_norm': 2.222282886505127, 'learning_rate': 0.0001866666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 15.722222222222221 -- {'loss': 1.4062, 'grad_norm': 2.9084606170654297, 'learning_rate': 0.00018555555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 15.777777777777779 -- {'loss': 1.1562, 'grad_norm': 2.8110601902008057, 'learning_rate': 0.00018444444444444443}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 15.833333333333334 -- {'loss': 0.9805, 'grad_norm': 2.241408109664917, 'learning_rate': 0.00018333333333333334}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 15.88888888888889 -- {'loss': 1.0938, 'grad_norm': 2.5189883708953857, 'learning_rate': 0.00018222222222222224}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 15.944444444444445 -- {'loss': 1.4688, 'grad_norm': 2.948096752166748, 'learning_rate': 0.0001811111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 16.0 -- {'loss': 0.9609, 'grad_norm': 2.5262250900268555, 'learning_rate': 0.00017999999999999998}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 16.0 -- {'loss': 1.76710844039917, 'model_preparation_time': 0.0923, 'runtime': 4.3634, 'samples_per_second': 36.211, 'steps_per_second': 0.688}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 16.055555555555557 -- {'loss': 0.6523, 'grad_norm': 2.0170865058898926, 'learning_rate': 0.00017888888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 16.11111111111111 -- {'loss': 1.2969, 'grad_norm': 2.6030819416046143, 'learning_rate': 0.00017777777777777779}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 16.166666666666668 -- {'loss': 1.1406, 'grad_norm': 2.6226024627685547, 'learning_rate': 0.00017666666666666666}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 16.22222222222222 -- {'loss': 1.3203, 'grad_norm': 2.969121217727661, 'learning_rate': 0.00017555555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 16.27777777777778 -- {'loss': 1.1094, 'grad_norm': 2.4541409015655518, 'learning_rate': 0.00017444444444444446}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 16.333333333333332 -- {'loss': 1.0938, 'grad_norm': 2.6003220081329346, 'learning_rate': 0.00017333333333333334}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 16.38888888888889 -- {'loss': 1.0859, 'grad_norm': 2.469172954559326, 'learning_rate': 0.00017222222222222224}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 16.444444444444443 -- {'loss': 1.2422, 'grad_norm': 2.7386696338653564, 'learning_rate': 0.0001711111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 16.5 -- {'loss': 0.9062, 'grad_norm': 2.2680118083953857, 'learning_rate': 0.00017}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 16.555555555555557 -- {'loss': 1.0625, 'grad_norm': 2.256208896636963, 'learning_rate': 0.00016888888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 16.61111111111111 -- {'loss': 1.2109, 'grad_norm': 2.6142852306365967, 'learning_rate': 0.0001677777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 16.666666666666668 -- {'loss': 0.875, 'grad_norm': 2.2405447959899902, 'learning_rate': 0.00016666666666666666}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 16.72222222222222 -- {'loss': 1.1094, 'grad_norm': 2.4659454822540283, 'learning_rate': 0.00016555555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 16.77777777777778 -- {'loss': 1.1094, 'grad_norm': 2.6074562072753906, 'learning_rate': 0.00016444444444444446}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 16.833333333333332 -- {'loss': 1.3281, 'grad_norm': 2.6168251037597656, 'learning_rate': 0.00016333333333333334}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 16.88888888888889 -- {'loss': 0.9492, 'grad_norm': 2.2410075664520264, 'learning_rate': 0.0001622222222222222}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 16.944444444444443 -- {'loss': 0.8281, 'grad_norm': 2.5591087341308594, 'learning_rate': 0.0001611111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 17.0 -- {'loss': 1.3594, 'grad_norm': 2.660891056060791, 'learning_rate': 0.00016}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 17.0 -- {'loss': 1.746835470199585, 'model_preparation_time': 0.0923, 'runtime': 3.8178, 'samples_per_second': 41.385, 'steps_per_second': 0.786}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 17.055555555555557 -- {'loss': 1.3125, 'grad_norm': 2.570718288421631, 'learning_rate': 0.0001588888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 17.11111111111111 -- {'loss': 1.2344, 'grad_norm': 2.632138252258301, 'learning_rate': 0.00015777777777777776}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 17.166666666666668 -- {'loss': 1.4219, 'grad_norm': 2.989495277404785, 'learning_rate': 0.0001566666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 17.22222222222222 -- {'loss': 1.3906, 'grad_norm': 2.7120718955993652, 'learning_rate': 0.00015555555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 17.27777777777778 -- {'loss': 0.9805, 'grad_norm': 2.417234420776367, 'learning_rate': 0.00015444444444444444}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 17.333333333333332 -- {'loss': 1.0156, 'grad_norm': 2.3051657676696777, 'learning_rate': 0.00015333333333333334}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 17.38888888888889 -- {'loss': 1.1719, 'grad_norm': 2.474764347076416, 'learning_rate': 0.00015222222222222224}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 17.444444444444443 -- {'loss': 0.7734, 'grad_norm': 2.3268425464630127, 'learning_rate': 0.0001511111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 17.5 -- {'loss': 1.0312, 'grad_norm': 2.224616050720215, 'learning_rate': 0.00015}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 17.555555555555557 -- {'loss': 1.0312, 'grad_norm': 2.6530840396881104, 'learning_rate': 0.0001488888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 17.61111111111111 -- {'loss': 1.0078, 'grad_norm': 2.3547263145446777, 'learning_rate': 0.0001477777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 17.666666666666668 -- {'loss': 1.0156, 'grad_norm': 2.504263401031494, 'learning_rate': 0.00014666666666666666}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 17.72222222222222 -- {'loss': 1.0156, 'grad_norm': 2.6002817153930664, 'learning_rate': 0.00014555555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 17.77777777777778 -- {'loss': 1.0156, 'grad_norm': 2.417064905166626, 'learning_rate': 0.00014444444444444444}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 17.833333333333332 -- {'loss': 0.9922, 'grad_norm': 2.741145372390747, 'learning_rate': 0.00014333333333333334}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 17.88888888888889 -- {'loss': 1.1094, 'grad_norm': 2.7242512702941895, 'learning_rate': 0.0001422222222222222}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 17.944444444444443 -- {'loss': 0.9297, 'grad_norm': 2.6396608352661133, 'learning_rate': 0.00014111111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 18.0 -- {'loss': 0.8828, 'grad_norm': 2.1426968574523926, 'learning_rate': 0.00014000000000000001}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 18.0 -- {'loss': 1.7496044635772705, 'model_preparation_time': 0.0923, 'runtime': 7.8351, 'samples_per_second': 20.166, 'steps_per_second': 0.383}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 18.055555555555557 -- {'loss': 1.125, 'grad_norm': 2.8145735263824463, 'learning_rate': 0.0001388888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 18.11111111111111 -- {'loss': 1.0312, 'grad_norm': 2.7159759998321533, 'learning_rate': 0.0001377777777777778}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 18.166666666666668 -- {'loss': 1.0547, 'grad_norm': 2.426114559173584, 'learning_rate': 0.00013666666666666666}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 18.22222222222222 -- {'loss': 1.0625, 'grad_norm': 2.588935613632202, 'learning_rate': 0.00013555555555555556}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 18.27777777777778 -- {'loss': 1.0, 'grad_norm': 2.4954710006713867, 'learning_rate': 0.00013444444444444447}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 18.333333333333332 -- {'loss': 0.8906, 'grad_norm': 2.4262142181396484, 'learning_rate': 0.00013333333333333334}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 18.38888888888889 -- {'loss': 1.0234, 'grad_norm': 2.503889799118042, 'learning_rate': 0.00013222222222222221}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 18.444444444444443 -- {'loss': 1.0078, 'grad_norm': 2.53574800491333, 'learning_rate': 0.00013111111111111111}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 18.5 -- {'loss': 1.0156, 'grad_norm': 2.664597272872925, 'learning_rate': 0.00013000000000000002}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 18.555555555555557 -- {'loss': 0.8594, 'grad_norm': 2.522784948348999, 'learning_rate': 0.0001288888888888889}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 18.61111111111111 -- {'loss': 1.0078, 'grad_norm': 2.4738850593566895, 'learning_rate': 0.00012777777777777776}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 18.666666666666668 -- {'loss': 0.8594, 'grad_norm': 2.543616533279419, 'learning_rate': 0.0001266666666666667}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 18.72222222222222 -- {'loss': 1.2109, 'grad_norm': 2.875166177749634, 'learning_rate': 0.00012555555555555557}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 18.77777777777778 -- {'loss': 0.9453, 'grad_norm': 2.6173627376556396, 'learning_rate': 0.00012444444444444444}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 18.833333333333332 -- {'loss': 0.8203, 'grad_norm': 2.5071840286254883, 'learning_rate': 0.00012333333333333334}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 18.88888888888889 -- {'loss': 0.9258, 'grad_norm': 2.2866153717041016, 'learning_rate': 0.00012222222222222221}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 18.944444444444443 -- {'loss': 1.3281, 'grad_norm': 2.843083381652832, 'learning_rate': 0.00012111111111111112}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 19.0 -- {'loss': 1.1875, 'grad_norm': 2.8902108669281006, 'learning_rate': 0.00012}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 19.0 -- {'loss': 1.7745252847671509, 'model_preparation_time': 0.0923, 'runtime': 4.4643, 'samples_per_second': 35.392, 'steps_per_second': 0.672}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Train Epoch: 19.0 -- {'runtime': 739.7339, 'samples_per_second': 38.865, 'steps_per_second': 0.608, 'loss': 1.3796600877192982}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Loading trained model from disk...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for /burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849 contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Finished loading.\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Trainer 'LUAR Trainer' finished and is saved to disk. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Done. âœ¨ Results in folder: /burg/dsi/users/ma4608/ajay-ta2-system/output/original_ta2_model\n"
     ]
    }
   ],
   "source": [
    "train_datadreamer_ta2(fold, output_path + '/original_ta2_model', 'MultipleNegativesSymmetricRankingLoss', luar_model_path=luar_model_path, batch_size=64, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8914eba7-5840-472c-995c-0f9053075098",
   "metadata": {},
   "source": [
    "### Train original TA2 system with Supervised Contrastive Loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7481718-de21-4f6b-90a1-faadcf1f7f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path = '/mnt/swordfish-pool2/milad/datadreamer-ta2/'\n",
    "# luar_model_path = '/mnt/swordfish-pool2/milad/rrivera1849'\n",
    "output_path = '/burg/dsi/users/ma4608/ajay-ta2-system/output'\n",
    "luar_model_path = '/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f50be29d-286f-4b60-b987-20fec50b0c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = \"/burg/dsi/users/ma4608/ajay-ta2-system/training_source/data/train-test-dev-split/{split}/official-query-candidate-format/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fece15d-8086-46f6-aace-5dab211db206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of authors 8896\n",
      "number of authors after filtering  1651\n",
      "Dataset Statistics: 1651 9761\n",
      "number of authors 1351\n",
      "number of authors after filtering  270\n",
      "Dataset Statistics: 270 1570\n"
     ]
    }
   ],
   "source": [
    "train_num_rows, train_data_generator = get_data_generator_for_supervised_contrastive_learning(fold, \"cross_genre_all\", \"train\", split_percent=ast.literal_eval(\"None\"))\n",
    "dev_num_rows, dev_data_generator = get_data_generator_for_supervised_contrastive_learning(fold, \"cross_genre_all\", \"dev\", split_percent=ast.literal_eval(\"None\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d20f11b3-3b2f-4fda-8f6f-d513cb7db3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Initialized. ğŸš€ Dreaming to folder: /burg/dsi/users/ma4608/ajay-ta2-system/output/supcon_ta2_model\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Train Data' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Train Data' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Dev Data' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Dev Data' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Trainer 'LUAR Trainer' was previously run and saved, but was outdated. ğŸ˜\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Trainer 'LUAR Trainer' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Train Anchors' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Train Anchors' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Train Others' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Train Others' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Validation Anchors' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Validation Anchors' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Validation Others' is running. â³\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Validation Others' will run lazily. ğŸ¥±\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Loading model...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for /burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849 contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Finished loading.\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [ ğŸ¤— Accelerate] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Dev Data' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Validation Anchors' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Validation Others' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 0.0 -- {'loss': 4.055605411529541, 'model_preparation_time': 0.0581, 'runtime': 20.6433, 'samples_per_second': 76.054, 'steps_per_second': 0.63}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'Train Data' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Train Anchors' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Step 'LUAR Trainer / Tokenize Train Others' finished running lazily. ğŸ‰\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 1.0 -- {'loss': 3.916020631790161, 'model_preparation_time': 0.0581, 'runtime': 14.1073, 'samples_per_second': 111.29, 'steps_per_second': 0.922}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] [LUAR Trainer (/burg/dsi/users/ma4608/ajay-ta2-system/training_source/rrivera1849)] Eval Epoch: 2.0 -- {'loss': 3.9026615619659424, 'model_preparation_time': 0.0581, 'runtime': 13.7729, 'samples_per_second': 113.992, 'steps_per_second': 0.944}\n",
      "[ \u001b[35mğŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ğŸ’¤ ] Done. âœ¨ Results in folder: /burg/dsi/users/ma4608/ajay-ta2-system/output/supcon_ta2_model\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 530.69 MiB is free. Including non-PyTorch memory, this process has 39.04 GiB memory in use. Of the allocated memory 37.21 GiB is allocated by PyTorch, and 1.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_datadreamer_ta2(fold, output_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/supcon_ta2_model\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSupConLoss\u001b[39m\u001b[38;5;124m'\u001b[39m, luar_model_path\u001b[38;5;241m=\u001b[39mluar_model_path, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m, in \u001b[0;36mtrain_datadreamer_ta2\u001b[0;34m(fold, output_folder, used_loss, luar_model_path, batch_size, epochs)\u001b[0m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m MultiPosConLoss \u001b[38;5;28;01mif\u001b[39;00m used_loss\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSupConLoss\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m losses\u001b[38;5;241m.\u001b[39mMultipleNegativesSymmetricRankingLoss\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m used_loss \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSupConLoss\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain_with_labeled_pairs(\n\u001b[1;32m     29\u001b[0m         train_anchors\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manchors\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     30\u001b[0m         train_others\u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mothers\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     31\u001b[0m         train_labels\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     32\u001b[0m         validation_anchors\u001b[38;5;241m=\u001b[39mdev_dataset\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manchors\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     33\u001b[0m         validation_others\u001b[38;5;241m=\u001b[39mdev_dataset\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mothers\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     34\u001b[0m         validation_labels\u001b[38;5;241m=\u001b[39mdev_dataset\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     35\u001b[0m         epochs\u001b[38;5;241m=\u001b[39mepochs,    \n\u001b[1;32m     36\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     37\u001b[0m         logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     38\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m     39\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m,\n\u001b[1;32m     40\u001b[0m         early_stopping_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m     41\u001b[0m         early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     42\u001b[0m         accelerator_config\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdispatch_batches\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     44\u001b[0m         },\n\u001b[1;32m     45\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[EpochTrackerCallback()]\n\u001b[1;32m     46\u001b[0m     )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain_with_positive_pairs(\n\u001b[1;32m     49\u001b[0m         train_anchors\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manchors\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     50\u001b[0m         train_positives\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositives\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[EpochTrackerCallback()]\n\u001b[1;32m     64\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datadreamer/trainers/train_sentence_transformer.py:776\u001b[0m, in \u001b[0;36mTrainSentenceTransformer.train_with_labeled_pairs\u001b[0;34m(self, train_anchors, train_others, train_labels, validation_anchors, validation_others, validation_labels, truncate, epochs, batch_size, loss, optim, learning_rate, weight_decay, lr_scheduler_type, warmup_steps, neftune_noise_alpha, seed, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_with_labeled_pairs\u001b[39m(\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    757\u001b[0m     train_anchors: OutputDatasetColumn \u001b[38;5;241m|\u001b[39m OutputIterableDatasetColumn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    775\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainSentenceTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_folder_and_resume(\n\u001b[1;32m    777\u001b[0m         train_anchors\u001b[38;5;241m=\u001b[39mtrain_anchors,\n\u001b[1;32m    778\u001b[0m         train_positives\u001b[38;5;241m=\u001b[39mtrain_others,\n\u001b[1;32m    779\u001b[0m         train_negatives\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    780\u001b[0m         train_labels\u001b[38;5;241m=\u001b[39mtrain_labels,\n\u001b[1;32m    781\u001b[0m         validation_anchors\u001b[38;5;241m=\u001b[39mvalidation_anchors,\n\u001b[1;32m    782\u001b[0m         validation_positives\u001b[38;5;241m=\u001b[39mvalidation_others,\n\u001b[1;32m    783\u001b[0m         validation_negatives\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    784\u001b[0m         validation_labels\u001b[38;5;241m=\u001b[39mvalidation_labels,\n\u001b[1;32m    785\u001b[0m         truncate\u001b[38;5;241m=\u001b[39mtruncate,\n\u001b[1;32m    786\u001b[0m         epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m    787\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    788\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m    789\u001b[0m         optim\u001b[38;5;241m=\u001b[39moptim,\n\u001b[1;32m    790\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[1;32m    791\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    792\u001b[0m         lr_scheduler_type\u001b[38;5;241m=\u001b[39mlr_scheduler_type,\n\u001b[1;32m    793\u001b[0m         warmup_steps\u001b[38;5;241m=\u001b[39mwarmup_steps,\n\u001b[1;32m    794\u001b[0m         neftune_noise_alpha\u001b[38;5;241m=\u001b[39mneftune_noise_alpha,\n\u001b[1;32m    795\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m    796\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    797\u001b[0m     )\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datadreamer/trainers/trainer.py:291\u001b[0m, in \u001b[0;36mTrainer._setup_folder_and_resume\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     DataDreamer\u001b[38;5;241m.\u001b[39m_start_step(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__setup_folder_and_resume(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     DataDreamer\u001b[38;5;241m.\u001b[39m_stop_step()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datadreamer/trainers/trainer.py:271\u001b[0m, in \u001b[0;36mTrainer.__setup_folder_and_resume\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m ignore_training_warnings():\n\u001b[0;32m--> 271\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# TrainingArguments() postinit modifies os.environ, so we restore it\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;66;03m# after running any training procedure\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datadreamer/trainers/train_sentence_transformer.py:643\u001b[0m, in \u001b[0;36mTrainSentenceTransformer._train\u001b[0;34m(self, train_anchors, train_positives, train_negatives, train_labels, validation_anchors, validation_positives, validation_negatives, validation_labels, truncate, margin, epochs, batch_size, loss, optim, learning_rate, weight_decay, lr_scheduler_type, warmup_steps, neftune_noise_alpha, seed, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m trainer\u001b[38;5;241m.\u001b[39mremove_callback(PrinterCallback)\n\u001b[1;32m    642\u001b[0m \u001b[38;5;66;03m# Start the trainer\u001b[39;00m\n\u001b[0;32m--> 643\u001b[0m start_hf_trainer(\u001b[38;5;28mself\u001b[39m, trainer)\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# Save the model to disk\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_model(\n\u001b[1;32m    647\u001b[0m     training_args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m    648\u001b[0m     model\u001b[38;5;241m=\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m     fsdp\u001b[38;5;241m=\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mis_fsdp_enabled,\n\u001b[1;32m    652\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datadreamer/utils/hf_training_utils.py:872\u001b[0m, in \u001b[0;36mstart_hf_trainer\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    870\u001b[0m         trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 872\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DataDreamer\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mhf_log:\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mNOTSET:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datadreamer/utils/hf_training_utils.py:870\u001b[0m, in \u001b[0;36mstart_hf_trainer\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    867\u001b[0m         trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m--> 870\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1939\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1940\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1941\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1942\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1943\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3318\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   3320\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3323\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3324\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datadreamer/utils/hf_training_utils.py:120\u001b[0m, in \u001b[0;36mwrap_trainer_cls.<locals>.WrappedTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compute_loss(model, inputs, return_outputs\u001b[38;5;241m=\u001b[39mreturn_outputs)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompute_loss(\n\u001b[1;32m    121\u001b[0m         model, inputs, return_outputs\u001b[38;5;241m=\u001b[39mreturn_outputs\n\u001b[1;32m    122\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:3363\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3362\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3363\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3365\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datadreamer/trainers/train_sentence_transformer.py:156\u001b[0m, in \u001b[0;36mSentenceTransformerLossWrapper.forward\u001b[0;34m(self, anchor_input_ids, anchor_attention_mask, positive_input_ids, positive_attention_mask, negative_input_ids, negative_attention_mask, labels)\u001b[0m\n\u001b[1;32m    148\u001b[0m     _uniq_ids\u001b[38;5;241m.\u001b[39mappend(uuid4()\u001b[38;5;241m.\u001b[39mhex)\n\u001b[1;32m    149\u001b[0m     sentence_features\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    150\u001b[0m         {\n\u001b[1;32m    151\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_uniq_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: _uniq_ids[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m         }\n\u001b[1;32m    155\u001b[0m     )\n\u001b[0;32m--> 156\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_module(sentence_features\u001b[38;5;241m=\u001b[39msentence_features, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss,\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_for_joint_metric\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss,\n\u001b[1;32m    164\u001b[0m }\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/SupContrastLoss.py:73\u001b[0m, in \u001b[0;36mMultiPosConLoss.forward\u001b[0;34m(self, sentence_features, labels, mask)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence_features, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 73\u001b[0m     reps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(sentence_feature) \u001b[38;5;28;01mfor\u001b[39;00m sentence_feature \u001b[38;5;129;01min\u001b[39;00m sentence_features]\n\u001b[1;32m     74\u001b[0m     feats \u001b[38;5;241m=\u001b[39m reps[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m#feats = outputs['feats']    # feats shape: [B, D]\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m#labels = outputs['labels']    # labels shape: [B]\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/SupContrastLoss.py:73\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence_features, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 73\u001b[0m     reps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(sentence_feature) \u001b[38;5;28;01mfor\u001b[39;00m sentence_feature \u001b[38;5;129;01min\u001b[39;00m sentence_features]\n\u001b[1;32m     74\u001b[0m     feats \u001b[38;5;241m=\u001b[39m reps[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m#feats = outputs['feats']    # feats shape: [B, D]\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m#labels = outputs['labels']    # labels shape: [B]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datadreamer/trainers/train_sentence_transformer.py:76\u001b[0m, in \u001b[0;36mSentenceTransformerWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_return_value(\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     72\u001b[0m     )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Handle PEFT forward\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_return_value(\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     77\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/luar_utils.py:39\u001b[0m, in \u001b[0;36mload_luar_as_sentence_transformer.<locals>.wrap_old_transformer_forward\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m     38\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m st\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/luar_utils.py:50\u001b[0m, in \u001b[0;36mload_luar_as_sentence_transformer.<locals>.<lambda>\u001b[0;34m(return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m: old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)}\n\u001b[0;32m---> 50\u001b[0m st\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mauto_model\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, return_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: wrap_old_forward(\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Return ST model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m st\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/luar_utils.py:49\u001b[0m, in \u001b[0;36mload_luar_as_sentence_transformer.<locals>.wrap_old_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m: old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)}\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/rrivera1849/model.py:217\u001b[0m, in \u001b[0;36mLUAR.forward\u001b[0;34m(self, input_ids, attention_mask, output_attentions, document_batch_size)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, document_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculates a fixed-length feature vector for a batch of episode samples.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_episode_embeddings(input_ids, attention_mask, output_attentions, document_batch_size)\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/rrivera1849/model.py:191\u001b[0m, in \u001b[0;36mLUAR.get_episode_embeddings\u001b[0;34m(self, input_ids, attention_mask, output_attentions, document_batch_size)\u001b[0m\n\u001b[1;32m    189\u001b[0m         outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattentions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([torch\u001b[38;5;241m.\u001b[39mcat([x[i] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattentions\u001b[39m\u001b[38;5;124m\"\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattentions\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]))])\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[1;32m    192\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    193\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    194\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    195\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    196\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    197\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# at this point, we're embedding individual \"comments\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m comment_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_pooling(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m'\u001b[39m], attention_mask)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:832\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    823\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    825\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    826\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    827\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    830\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    831\u001b[0m )\n\u001b[0;32m--> 832\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    833\u001b[0m     embedding_output,\n\u001b[1;32m    834\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m    835\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    836\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m    837\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m    838\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    839\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    840\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    841\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    842\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    843\u001b[0m )\n\u001b[1;32m    844\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    845\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:521\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    510\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    511\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    512\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m         output_attentions,\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 521\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    522\u001b[0m         hidden_states,\n\u001b[1;32m    523\u001b[0m         attention_mask,\n\u001b[1;32m    524\u001b[0m         layer_head_mask,\n\u001b[1;32m    525\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    526\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    527\u001b[0m         past_key_value,\n\u001b[1;32m    528\u001b[0m         output_attentions,\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    531\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:452\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    449\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    450\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 452\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, attention_output\n\u001b[1;32m    454\u001b[0m )\n\u001b[1;32m    455\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/pytorch_utils.py:238\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:464\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 464\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m    465\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:362\u001b[0m, in \u001b[0;36mRobertaIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 362\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    363\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/peft/tuners/lora/layer.py:556\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(lora_A\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dora[active_adapter]:\n\u001b[0;32m--> 556\u001b[0m     result \u001b[38;5;241m=\u001b[39m result \u001b[38;5;241m+\u001b[39m lora_B(lora_A(dropout(x))) \u001b[38;5;241m*\u001b[39m scaling\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m     x \u001b[38;5;241m=\u001b[39m dropout(x)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 530.69 MiB is free. Including non-PyTorch memory, this process has 39.04 GiB memory in use. Of the allocated memory 37.21 GiB is allocated by PyTorch, and 1.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train_datadreamer_ta2(fold, output_path + '/supcon_ta2_model', 'SupConLoss', luar_model_path=luar_model_path, batch_size=128, epochs=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

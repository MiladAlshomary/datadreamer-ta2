{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc1b260-9fd1-4076-a90b-ba63f8b8ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a977db5-ba6a-41ab-986a-368dc42cb39c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-03 10:52:35.174716: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-03 10:52:58.894876: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-03 10:53:01.928140: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-03 10:53:04.918973: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-03 10:53:27.606616: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-03 10:53:31.100330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from trainer import *\n",
    "from SupContrastLoss import SupConLoss\n",
    "from RankingLoss import MultipleNegativesSymmetricRankingLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82ae8da6-a8b7-4ae9-a249-d32c313802eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b8e8a05-a548-4e1b-a76e-ede4b6a65ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47760867328"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7b12fdc-2526-42ec-a8f2-59d6dd7013a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_datadreamer_ta2(fold, output_folder, used_loss):\n",
    "    with DataDreamer(output_folder):\n",
    "        dataset = DataSource(\n",
    "            \"Train Data\",\n",
    "            data=train_data_generator,\n",
    "            total_num_rows=train_num_rows,\n",
    "        )\n",
    "        dev_dataset = DataSource(\n",
    "            \"Dev Data\",\n",
    "            data=dev_data_generator,\n",
    "            total_num_rows=dev_num_rows,\n",
    "        )\n",
    "    \n",
    "        trainer = get_luar_trainer()(\n",
    "            \"LUAR Trainer\",\n",
    "            model_name=\"./rrivera1849/\",\n",
    "            peft_config=LoraConfig(),\n",
    "            trust_remote_code=True,\n",
    "            device='cuda:0',\n",
    "            dtype=\"bfloat16\",\n",
    "            force=True,\n",
    "        )\n",
    "\n",
    "        loss = SupConLoss if used_loss=='SupConLoss' else losses.MultipleNegativesSymmetricRankingLoss\n",
    "\n",
    "        if used_loss == 'SupConLoss':\n",
    "            trainer.train_with_labeled_pairs(\n",
    "                train_anchors=dataset.output[\"anchors\"],\n",
    "                train_others= dataset.output[\"others\"],\n",
    "                train_labels=dataset.output[\"labels\"],\n",
    "                validation_anchors=dev_dataset.output[\"anchors\"],\n",
    "                validation_others=dev_dataset.output[\"others\"],\n",
    "                validation_labels=dev_dataset.output[\"labels\"],\n",
    "                epochs=25,    \n",
    "                batch_size=128,\n",
    "                loss=loss,\n",
    "                learning_rate=0.0005,\n",
    "                early_stopping_threshold=0.001,\n",
    "                early_stopping_patience=5,\n",
    "                accelerator_config={\n",
    "                    \"dispatch_batches\": False,\n",
    "                },\n",
    "                callbacks=[EpochTrackerCallback()]\n",
    "            )\n",
    "        else:\n",
    "            trainer.train_with_positive_pairs(\n",
    "                train_anchors=dataset.output[\"anchors\"],\n",
    "                train_positives=dataset.output[\"positives\"],\n",
    "                validation_anchors=dev_dataset.output[\"anchors\"],\n",
    "                validation_positives=dev_dataset.output[\"positives\"],\n",
    "                epochs=25,    \n",
    "                batch_size=100,\n",
    "                loss=loss,\n",
    "                learning_rate=0.0005,\n",
    "                early_stopping_threshold=0.001,\n",
    "                early_stopping_patience=5,\n",
    "                accelerator_config={\n",
    "                    \"dispatch_batches\": False,\n",
    "                },\n",
    "                callbacks=[EpochTrackerCallback()]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d5eef-275d-4458-a919-5830fe79e46f",
   "metadata": {},
   "source": [
    "### Train original TA2 system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10dd4486-104d-4ecf-852b-5f22e2991c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = \"data/train-test-dev split/{split}/Official Query-candidate format/\"\n",
    "used_loss= \"supervised_contrastive_loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3450f7a9-d3e4-4f0b-8b0f-cf91bb5fe69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: cross_genre_all train None 575 4142\n",
      "Dataset Statistics: cross_genre_all dev None 79 635\n"
     ]
    }
   ],
   "source": [
    "train_num_rows, train_data_generator = get_data_generator(fold, \"cross_genre_all\", \"train\", split_percent=ast.literal_eval(\"None\"))\n",
    "dev_num_rows, dev_data_generator = get_data_generator(fold, \"cross_genre_all\", \"dev\", split_percent=ast.literal_eval(\"None\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6306a9-e13e-4790-b379-1dc425b34707",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Initialized. 🚀 Dreaming to folder: ./original_ta2_model\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'Train Data' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'Train Data' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'Dev Data' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'Dev Data' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Trainer 'LUAR Trainer' was previously run and saved, but was outdated. 😞\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Trainer 'LUAR Trainer' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Train Anchors' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Train Anchors' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Train Positives' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Train Positives' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Validation Anchors' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'Dev Data' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Validation Anchors' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Validation Positives' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Validation Positives' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Loading model...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for ./rrivera1849/ contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/./rrivera1849/.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Finished loading.\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [ 🤗 Accelerate] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Validation Anchors' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Validation Positives' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Eval Epoch: 0.0 -- {'loss': 3.1012659072875977, 'runtime': 6.0931, 'samples_per_second': 25.931, 'steps_per_second': 0.328}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Done. ✨ Results in folder: ./original_ta2_model\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 44.48 GiB of which 339.25 MiB is free. Including non-PyTorch memory, this process has 44.15 GiB memory in use. Of the allocated memory 43.61 GiB is allocated by PyTorch, and 353.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_datadreamer_ta2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./original_ta2_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMultipleNegativesSymmetricRankingLoss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 46\u001b[0m, in \u001b[0;36mtrain_datadreamer_ta2\u001b[0;34m(fold, output_folder, used_loss)\u001b[0m\n\u001b[1;32m     27\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain_with_labeled_pairs(\n\u001b[1;32m     28\u001b[0m         train_anchors\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manchors\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     29\u001b[0m         train_others\u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mothers\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[EpochTrackerCallback()]\n\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_with_positive_pairs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_anchors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manchors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_positives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpositives\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_anchors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manchors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_positives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpositives\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdispatch_batches\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEpochTrackerCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/datadreamer/trainers/train_sentence_transformer.py:730\u001b[0m, in \u001b[0;36mTrainSentenceTransformer.train_with_positive_pairs\u001b[0;34m(self, train_anchors, train_positives, validation_anchors, validation_positives, truncate, epochs, batch_size, loss, optim, learning_rate, weight_decay, lr_scheduler_type, warmup_steps, neftune_noise_alpha, seed, **kwargs)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_with_positive_pairs\u001b[39m(\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    713\u001b[0m     train_anchors: OutputDatasetColumn \u001b[38;5;241m|\u001b[39m OutputIterableDatasetColumn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    729\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainSentenceTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 730\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_folder_and_resume\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_anchors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_anchors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_positives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_positives\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_negatives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_anchors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_anchors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_positives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_positives\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_negatives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmargin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEFAULT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneftune_noise_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneftune_noise_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/datadreamer/trainers/trainer.py:291\u001b[0m, in \u001b[0;36mTrainer._setup_folder_and_resume\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     DataDreamer\u001b[38;5;241m.\u001b[39m_start_step(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__setup_folder_and_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     DataDreamer\u001b[38;5;241m.\u001b[39m_stop_step()\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/datadreamer/trainers/trainer.py:271\u001b[0m, in \u001b[0;36mTrainer.__setup_folder_and_resume\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m ignore_training_warnings():\n\u001b[0;32m--> 271\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# TrainingArguments() postinit modifies os.environ, so we restore it\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;66;03m# after running any training procedure\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/datadreamer/trainers/train_sentence_transformer.py:643\u001b[0m, in \u001b[0;36mTrainSentenceTransformer._train\u001b[0;34m(self, train_anchors, train_positives, train_negatives, train_labels, validation_anchors, validation_positives, validation_negatives, validation_labels, truncate, margin, epochs, batch_size, loss, optim, learning_rate, weight_decay, lr_scheduler_type, warmup_steps, neftune_noise_alpha, seed, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m trainer\u001b[38;5;241m.\u001b[39mremove_callback(PrinterCallback)\n\u001b[1;32m    642\u001b[0m \u001b[38;5;66;03m# Start the trainer\u001b[39;00m\n\u001b[0;32m--> 643\u001b[0m \u001b[43mstart_hf_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# Save the model to disk\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_model(\n\u001b[1;32m    647\u001b[0m     training_args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m    648\u001b[0m     model\u001b[38;5;241m=\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m     fsdp\u001b[38;5;241m=\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mis_fsdp_enabled,\n\u001b[1;32m    652\u001b[0m )\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/datadreamer/utils/hf_training_utils.py:872\u001b[0m, in \u001b[0;36mstart_hf_trainer\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    870\u001b[0m         trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 872\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DataDreamer\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mhf_log:\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mNOTSET:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/datadreamer/utils/hf_training_utils.py:870\u001b[0m, in \u001b[0;36mstart_hf_trainer\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    867\u001b[0m         trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m--> 870\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/transformers/trainer.py:2268\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2268\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2271\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2274\u001b[0m ):\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2276\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/transformers/trainer.py:3307\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3307\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3309\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3311\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/datadreamer/utils/hf_training_utils.py:120\u001b[0m, in \u001b[0;36mwrap_trainer_cls.<locals>.WrappedTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compute_loss(model, inputs, return_outputs\u001b[38;5;241m=\u001b[39mreturn_outputs)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_outputs\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/transformers/trainer.py:3338\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3337\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3338\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3339\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3340\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/datadreamer/trainers/train_sentence_transformer.py:156\u001b[0m, in \u001b[0;36mSentenceTransformerLossWrapper.forward\u001b[0;34m(self, anchor_input_ids, anchor_attention_mask, positive_input_ids, positive_attention_mask, negative_input_ids, negative_attention_mask, labels)\u001b[0m\n\u001b[1;32m    148\u001b[0m     _uniq_ids\u001b[38;5;241m.\u001b[39mappend(uuid4()\u001b[38;5;241m.\u001b[39mhex)\n\u001b[1;32m    149\u001b[0m     sentence_features\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    150\u001b[0m         {\n\u001b[1;32m    151\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_uniq_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: _uniq_ids[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m         }\n\u001b[1;32m    155\u001b[0m     )\n\u001b[0;32m--> 156\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentence_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss,\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_for_joint_metric\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss,\n\u001b[1;32m    164\u001b[0m }\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/sentence_transformers/losses/MultipleNegativesSymmetricRankingLoss.py:73\u001b[0m, in \u001b[0;36mMultipleNegativesSymmetricRankingLoss.forward\u001b[0;34m(self, sentence_features, labels)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence_features: Iterable[Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]], labels: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 73\u001b[0m     reps \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_feature\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence_embedding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence_feature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence_features\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     74\u001b[0m     anchor \u001b[38;5;241m=\u001b[39m reps[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     75\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(reps[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/sentence_transformers/losses/MultipleNegativesSymmetricRankingLoss.py:73\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence_features: Iterable[Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]], labels: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 73\u001b[0m     reps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_feature\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sentence_feature \u001b[38;5;129;01min\u001b[39;00m sentence_features]\n\u001b[1;32m     74\u001b[0m     anchor \u001b[38;5;241m=\u001b[39m reps[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     75\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(reps[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/datadreamer/trainers/train_sentence_transformer.py:76\u001b[0m, in \u001b[0;36mSentenceTransformerWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_return_value(\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     72\u001b[0m     )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Handle PEFT forward\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_return_value(\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     77\u001b[0m     )\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/luar_utils.py:40\u001b[0m, in \u001b[0;36mload_luar_as_sentence_transformer.<locals>.wrap_old_transformer_forward\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m     39\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/luar_utils.py:51\u001b[0m, in \u001b[0;36mload_luar_as_sentence_transformer.<locals>.<lambda>\u001b[0;34m(return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m: old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)}\n\u001b[0;32m---> 51\u001b[0m st\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mauto_model\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, return_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mwrap_old_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Return ST model\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m st\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/luar_utils.py:50\u001b[0m, in \u001b[0;36mload_luar_as_sentence_transformer.<locals>.wrap_old_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m}\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/model.py:217\u001b[0m, in \u001b[0;36mLUAR.forward\u001b[0;34m(self, input_ids, attention_mask, output_attentions, document_batch_size)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, document_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculates a fixed-length feature vector for a batch of episode samples.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_episode_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/model.py:191\u001b[0m, in \u001b[0;36mLUAR.get_episode_embeddings\u001b[0;34m(self, input_ids, attention_mask, output_attentions, document_batch_size)\u001b[0m\n\u001b[1;32m    189\u001b[0m         outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattentions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([torch\u001b[38;5;241m.\u001b[39mcat([x[i] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattentions\u001b[39m\u001b[38;5;124m\"\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattentions\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]))])\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# at this point, we're embedding individual \"comments\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m comment_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_pooling(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m'\u001b[39m], attention_mask)\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:832\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    823\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    825\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    826\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    827\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    830\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    831\u001b[0m )\n\u001b[0;32m--> 832\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    845\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:521\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    510\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    511\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    512\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m         output_attentions,\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 521\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:410\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    400\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    407\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:337\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    329\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    336\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 337\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    347\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:260\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    256\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/burg/dsi/users/ma4608/ajay-ta2-system/training_source/ajay-pausi-venv/lib/python3.11/site-packages/torch/nn/functional.py:1295\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 44.48 GiB of which 339.25 MiB is free. Including non-PyTorch memory, this process has 44.15 GiB memory in use. Of the allocated memory 43.61 GiB is allocated by PyTorch, and 353.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train_datadreamer_ta2(fold, './original_ta2_model', 'MultipleNegativesSymmetricRankingLoss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8914eba7-5840-472c-995c-0f9053075098",
   "metadata": {},
   "source": [
    "### Train original TA2 system with Supervised Contrastive Loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "583024de-15ee-4963-8519-7ab58cb02a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: cross_genre_all train None 575 4142\n",
      "Dataset Statistics: cross_genre_all dev None 79 635\n"
     ]
    }
   ],
   "source": [
    "train_num_rows, train_data_generator = get_data_generator_for_supervised_contrastive_learning(fold, \"cross_genre_all\", \"train\", split_percent=ast.literal_eval(\"None\"))\n",
    "dev_num_rows, dev_data_generator = get_data_generator_for_supervised_contrastive_learning(fold, \"cross_genre_all\", \"dev\", split_percent=ast.literal_eval(\"None\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d20f11b3-3b2f-4fda-8f6f-d513cb7db3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Initialized. 🚀 Dreaming to folder: ./supcon_ta2_model\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'Train Data' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'Train Data' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'Dev Data' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'Dev Data' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Trainer 'LUAR Trainer' was previously run and saved, but was outdated. 😞\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Trainer 'LUAR Trainer' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Train Anchors' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Train Anchors' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Train Others' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Train Others' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Validation Anchors' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'Dev Data' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Validation Anchors' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Validation Others' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Validation Others' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Loading model...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for ./rrivera1849/ contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/./rrivera1849/.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Finished loading.\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [ 🤗 Accelerate] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Validation Anchors' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Validation Others' finished running lazily. 🎉\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(644.0, 4), (903.0, 2), (266.0, 4), (523.0, 3), (1294.0, 2), (529.0, 2), (1301.0, 2), (277.0, 2), (1305.0, 3), (796.0, 3), (416.0, 2), (931.0, 4), (165.0, 3), (39.0, 2), (680.0, 3), (1064.0, 2), (681.0, 2), (812.0, 3), (173.0, 5), (562.0, 2), (312.0, 2), (444.0, 3), (1345.0, 2), (194.0, 6), (580.0, 3), (211.0, 2), (215.0, 6), (88.0, 4), (87.0, 2), (882.0, 2), (117.0, 2), (251.0, 4), (511.0, 4)]\n",
      "[(902.0, 2), (1031.0, 2), (1020.0, 2), (148.0, 3), (1301.0, 4), (278.0, 3), (792.0, 2), (1305.0, 3), (152.0, 2), (157.0, 4), (1309.0, 5), (413.0, 2), (551.0, 4), (681.0, 3), (680.0, 2), (1076.0, 2), (1334.0, 3), (312.0, 4), (444.0, 6), (194.0, 2), (580.0, 3), (1350.0, 2), (202.0, 2), (78.0, 3), (848.0, 2), (81.0, 3), (82.0, 4), (211.0, 3), (215.0, 5), (97.0, 2), (484.0, 3), (1261.0, 2), (1264.0, 2), (116.0, 2), (251.0, 2), (764.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Eval Epoch: 0.0 -- {'loss': 191.56552124023438, 'runtime': 4.5626, 'samples_per_second': 34.629, 'steps_per_second': 0.438}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(645.0, 2), (902.0, 3), (1031.0, 2), (148.0, 2), (277.0, 2), (152.0, 6), (1178.0, 2), (796.0, 3), (1182.0, 2), (416.0, 2), (1192.0, 2), (680.0, 2), (551.0, 2), (173.0, 2), (819.0, 3), (1334.0, 2), (311.0, 2), (312.0, 6), (184.0, 2), (444.0, 5), (194.0, 2), (580.0, 2), (1229.0, 2), (84.0, 2), (212.0, 2), (215.0, 4), (738.0, 2), (484.0, 2), (1261.0, 4), (1264.0, 2), (241.0, 2), (116.0, 2), (1142.0, 2), (251.0, 4), (764.0, 2)]\n",
      "[(8741.0, 2), (8825.0, 2), (1664.0, 2), (653.0, 2), (5775.0, 3), (1717.0, 2), (8426.0, 2), (2300.0, 2), (7990.0, 2), (3396.0, 2), (2916.0, 3), (4968.0, 2), (2459.0, 2), (4601.0, 2), (2045.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 0.1111111111111111 -- {'loss': 39.9524, 'grad_norm': 56.58831024169922, 'learning_rate': 0.0004977777777777778}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8214.0, 2), (6704.0, 2), (5682.0, 2), (5699.0, 2), (3718.0, 2), (138.0, 2), (1748.0, 2), (7450.0, 2), (3886.0, 2), (3904.0, 2), (5460.0, 2), (7070.0, 3), (7071.0, 3), (1023.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 0.2222222222222222 -- {'loss': 28.2605, 'grad_norm': 52.08293151855469, 'learning_rate': 0.0004955555555555556}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2140.0, 2), (3229.0, 2), (8463.0, 2), (3396.0, 3), (4936.0, 2), (4463.0, 2), (924.0, 2), (3527.0, 2), (1521.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 0.3333333333333333 -- {'loss': 26.6055, 'grad_norm': 48.9267463684082, 'learning_rate': 0.0004933333333333334}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(41.0, 2), (4166.0, 3), (3697.0, 3), (8821.0, 2), (5765.0, 2), (5818.0, 2), (1748.0, 2), (4824.0, 2), (7939.0, 2), (782.0, 2), (816.0, 2), (2864.0, 2), (4463.0, 2), (3966.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 0.4444444444444444 -- {'loss': 39.1682, 'grad_norm': 110.13634490966797, 'learning_rate': 0.0004911111111111111}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5643.0, 2), (8741.0, 2), (2624.0, 2), (64.0, 2), (1602.0, 2), (8305.0, 3), (4503.0, 2), (5026.0, 2), (8176.0, 2), (4083.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 0.5555555555555556 -- {'loss': 22.1084, 'grad_norm': 103.12902069091797, 'learning_rate': 0.0004888888888888889}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3589.0, 2), (5223.0, 2), (8299.0, 2), (5112.0, 2), (4246.0, 2), (2805.0, 2), (5444.0, 2), (6999.0, 2), (4463.0, 2), (883.0, 3), (6037.0, 2), (3012.0, 2), (8175.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 0.6666666666666666 -- {'loss': 30.6011, 'grad_norm': 161.0355224609375, 'learning_rate': 0.0004866666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'Train Data' finished running lazily. 🎉\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3589.0, 2), (1566.0, 2), (4133.0, 2), (6704.0, 2), (5699.0, 2), (7820.0, 2), (6386.0, 2), (6443.0, 2), (3928.0, 2), (2916.0, 2), (2444.0, 2), (7070.0, 2), (1468.0, 2), (4051.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 0.7777777777777778 -- {'loss': 29.2287, 'grad_norm': 222.83908081054688, 'learning_rate': 0.00048444444444444446}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Train Anchors' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Train Others' finished running lazily. 🎉\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(526.0, 2), (77.0, 2), (1619.0, 2), (2156.0, 2), (2159.0, 2), (4295.0, 2), (1748.0, 2), (7927.0, 2), (6479.0, 2), (8547.0, 2), (3540.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 0.8888888888888888 -- {'loss': 22.6084, 'grad_norm': 213.4567108154297, 'learning_rate': 0.0004822222222222222}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4098.0, 2), (4133.0, 2), (6208.0, 2), (86.0, 2), (6816.0, 2), (6443.0, 3), (6447.0, 2), (3959.0, 2), (8641.0, 3), (5606.0, 2), (5112.0, 3), (510.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 1.0 -- {'loss': 20.8187, 'grad_norm': 309.58062744140625, 'learning_rate': 0.00048}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5699.0, 2), (8855.0, 2), (5272.0, 2), (6812.0, 2), (1810.0, 2), (5398.0, 2), (3886.0, 2), (7990.0, 2), (6527.0, 2), (2943.0, 2), (6058.0, 2), (963.0, 2), (5112.0, 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 1.1111111111111112 -- {'loss': 18.2955, 'grad_norm': 322.82867431640625, 'learning_rate': 0.0004777777777777778}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4098.0, 2), (6704.0, 2), (64.0, 2), (6237.0, 2), (1125.0, 2), (3697.0, 5), (5784.0, 2), (223.0, 2), (2405.0, 2), (4979.0, 2), (3469.0, 2), (2521.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 1.2222222222222223 -- {'loss': 14.5275, 'grad_norm': 284.42230224609375, 'learning_rate': 0.00047555555555555556}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5140.0, 2), (1063.0, 2), (552.0, 2), (7241.0, 2), (4196.0, 2), (8315.0, 2), (6915.0, 2), (6927.0, 2), (5436.0, 2), (6548.0, 2), (968.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 1.3333333333333333 -- {'loss': 14.0224, 'grad_norm': 103.8862533569336, 'learning_rate': 0.00047333333333333336}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(64.0, 2), (1615.0, 2), (119.0, 2), (5888.0, 2), (7966.0, 2), (6443.0, 2), (843.0, 2), (6992.0, 2), (7530.0, 2), (7040.0, 2), (8100.0, 2), (943.0, 2), (5052.0, 2), (968.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 1.4444444444444444 -- {'loss': 17.2375, 'grad_norm': 186.68727111816406, 'learning_rate': 0.0004711111111111111}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(526.0, 2), (3643.0, 2), (1660.0, 2), (4235.0, 2), (2193.0, 2), (7348.0, 2), (3904.0, 2), (395.0, 2), (8119.0, 2), (8640.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 1.5555555555555556 -- {'loss': 9.5421, 'grad_norm': 96.42599487304688, 'learning_rate': 0.0004688888888888889}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6704.0, 2), (5224.0, 2), (2683.0, 2), (4302.0, 2), (2278.0, 2), (7927.0, 2), (4934.0, 2), (2916.0, 2), (2943.0, 2), (5505.0, 2), (3469.0, 3), (7070.0, 2), (5026.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 1.6666666666666667 -- {'loss': 13.7413, 'grad_norm': 68.90650177001953, 'learning_rate': 0.00046666666666666666}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3697.0, 2), (3213.0, 2), (7852.0, 2), (5815.0, 2), (3786.0, 2), (7383.0, 2), (7911.0, 2), (5368.0, 2), (4356.0, 2), (2331.0, 2), (5425.0, 2), (6992.0, 2), (1916.0, 2), (5026.0, 2), (5579.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 1.7777777777777777 -- {'loss': 16.5284, 'grad_norm': 78.77726745605469, 'learning_rate': 0.00046444444444444446}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4133.0, 2), (1604.0, 2), (77.0, 2), (5728.0, 2), (2146.0, 2), (5223.0, 2), (5751.0, 3), (6264.0, 2), (5765.0, 2), (7829.0, 2), (5330.0, 2), (1748.0, 2), (8497.0, 2), (8035.0, 3), (1916.0, 2), (5052.0, 2), (6085.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 1.8888888888888888 -- {'loss': 17.3549, 'grad_norm': 60.7711296081543, 'learning_rate': 0.0004622222222222222}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7214.0, 2), (628.0, 3), (6340.0, 2), (8426.0, 3), (2795.0, 2), (6901.0, 2), (5425.0, 3), (306.0, 2), (4463.0, 3), (7071.0, 2), (1505.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.0 -- {'loss': 12.0616, 'grad_norm': 55.52666473388672, 'learning_rate': 0.00046}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(30.0, 2), (3643.0, 2), (77.0, 2), (1613.0, 3), (8821.0, 2), (4223.0, 2), (4797.0, 2), (1727.0, 2), (715.0, 2), (2780.0, 2), (8426.0, 2), (5505.0, 2), (1410.0, 2), (4083.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.111111111111111 -- {'loss': 6.8006, 'grad_norm': 46.56003952026367, 'learning_rate': 0.0004577777777777778}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3643.0, 2), (5699.0, 2), (2627.0, 2), (1615.0, 2), (2156.0, 2), (8315.0, 2), (8834.0, 2), (7815.0, 2), (8327.0, 2), (653.0, 2), (1680.0, 2), (7904.0, 2), (4329.0, 2), (782.0, 2), (6443.0, 2), (1464.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.2222222222222223 -- {'loss': 9.9705, 'grad_norm': 46.95590591430664, 'learning_rate': 0.00045555555555555556}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4098.0, 2), (3643.0, 2), (2640.0, 2), (1126.0, 2), (8315.0, 2), (7462.0, 2), (7472.0, 2), (1359.0, 2), (926.0, 2), (5067.0, 2), (8679.0, 2), (6638.0, 2), (7151.0, 2), (4607.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.3333333333333335 -- {'loss': 9.7935, 'grad_norm': 52.65534973144531, 'learning_rate': 0.0004533333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.4444444444444446 -- {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0004511111111111111}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[(644.0, 4), (903.0, 2), (266.0, 4), (523.0, 3), (1294.0, 2), (529.0, 2), (1301.0, 2), (277.0, 2), (1305.0, 3), (796.0, 3), (416.0, 2), (931.0, 4), (165.0, 3), (39.0, 2), (680.0, 3), (1064.0, 2), (681.0, 2), (812.0, 3), (173.0, 5), (562.0, 2), (312.0, 2), (444.0, 3), (1345.0, 2), (194.0, 6), (580.0, 3), (211.0, 2), (215.0, 6), (88.0, 4), (87.0, 2), (882.0, 2), (117.0, 2), (251.0, 4), (511.0, 4)]\n",
      "[(902.0, 2), (1031.0, 2), (1020.0, 2), (148.0, 3), (1301.0, 4), (278.0, 3), (792.0, 2), (1305.0, 3), (152.0, 2), (157.0, 4), (1309.0, 5), (413.0, 2), (551.0, 4), (681.0, 3), (680.0, 2), (1076.0, 2), (1334.0, 3), (312.0, 4), (444.0, 6), (194.0, 2), (580.0, 3), (1350.0, 2), (202.0, 2), (78.0, 3), (848.0, 2), (81.0, 3), (82.0, 4), (211.0, 3), (215.0, 5), (97.0, 2), (484.0, 3), (1261.0, 2), (1264.0, 2), (116.0, 2), (251.0, 2), (764.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Eval Epoch: 2.4444444444444446 -- {'loss': 37.60607147216797, 'runtime': 4.6343, 'samples_per_second': 34.094, 'steps_per_second': 0.432}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(645.0, 2), (902.0, 3), (1031.0, 2), (148.0, 2), (277.0, 2), (152.0, 6), (1178.0, 2), (796.0, 3), (1182.0, 2), (416.0, 2), (1192.0, 2), (680.0, 2), (551.0, 2), (173.0, 2), (819.0, 3), (1334.0, 2), (311.0, 2), (312.0, 6), (184.0, 2), (444.0, 5), (194.0, 2), (580.0, 2), (1229.0, 2), (84.0, 2), (212.0, 2), (215.0, 4), (738.0, 2), (484.0, 2), (1261.0, 4), (1264.0, 2), (241.0, 2), (116.0, 2), (1142.0, 2), (251.0, 4), (764.0, 2)]\n",
      "[(4098.0, 2), (7719.0, 2), (5699.0, 2), (2156.0, 2), (119.0, 2), (1150.0, 2), (8834.0, 2), (3396.0, 2), (7500.0, 2), (4463.0, 2), (4607.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 1.1111111111111112 -- {'loss': 5.7973, 'grad_norm': 37.39731979370117, 'learning_rate': 0.0004488888888888889}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3589.0, 3), (4124.0, 2), (5734.0, 2), (3697.0, 2), (1659.0, 2), (653.0, 2), (1793.0, 2), (1355.0, 2), (7071.0, 2), (1967.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 1.2222222222222223 -- {'loss': 4.3371, 'grad_norm': 47.15622329711914, 'learning_rate': 0.00044666666666666666}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1566.0, 3), (2640.0, 2), (1125.0, 2), (2683.0, 2), (5818.0, 2), (2780.0, 2), (7927.0, 2), (2916.0, 2), (5026.0, 3), (2981.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 1.3333333333333333 -- {'loss': 5.9586, 'grad_norm': 55.73238754272461, 'learning_rate': 0.0004444444444444444}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6695.0, 2), (8305.0, 2), (3697.0, 3), (8315.0, 2), (653.0, 2), (6340.0, 2), (2278.0, 2), (5888.0, 2), (5949.0, 2), (3412.0, 2), (8035.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 1.4444444444444444 -- {'loss': 3.3642, 'grad_norm': 9.3245267868042, 'learning_rate': 0.00044222222222222227}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4133.0, 3), (41.0, 2), (8305.0, 2), (4223.0, 2), (5818.0, 2), (3355.0, 2), (7463.0, 2), (6479.0, 2), (5026.0, 3), (7095.0, 2), (5112.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 1.5555555555555556 -- {'loss': 6.0927, 'grad_norm': 49.53636932373047, 'learning_rate': 0.00044}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6208.0, 3), (5699.0, 3), (8315.0, 2), (5765.0, 3), (6816.0, 2), (1210.0, 2), (3809.0, 2), (2805.0, 2), (3469.0, 3), (8119.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 1.6666666666666665 -- {'loss': 5.184, 'grad_norm': 28.30584144592285, 'learning_rate': 0.00043777777777777776}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(526.0, 2), (5682.0, 2), (3643.0, 2), (5699.0, 2), (7813.0, 2), (8841.0, 2), (3725.0, 2), (1680.0, 2), (4824.0, 2), (2300.0, 2), (5425.0, 2), (7056.0, 2), (8657.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 1.7777777777777777 -- {'loss': 5.7892, 'grad_norm': 31.990524291992188, 'learning_rate': 0.0004355555555555555}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2048.0, 2), (526.0, 2), (6704.0, 2), (5682.0, 2), (3643.0, 2), (64.0, 4), (8821.0, 2), (5759.0, 2), (8340.0, 2), (2795.0, 2), (6428.0, 2), (8100.0, 2), (8640.0, 3), (8679.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 1.8888888888888888 -- {'loss': 4.4927, 'grad_norm': 9.872316360473633, 'learning_rate': 0.00043333333333333337}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1537.0, 2), (6187.0, 2), (3643.0, 2), (5112.0, 2), (2787.0, 2), (4358.0, 2), (6443.0, 2), (5478.0, 2), (963.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.0 -- {'loss': 2.8641, 'grad_norm': 15.479574203491211, 'learning_rate': 0.0004311111111111111}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1566.0, 2), (5699.0, 2), (2770.0, 2), (7939.0, 2), (7472.0, 2), (8497.0, 2), (5986.0, 2), (4463.0, 2), (7070.0, 2), (6604.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.111111111111111 -- {'loss': 4.0168, 'grad_norm': 8.941088676452637, 'learning_rate': 0.00042888888888888886}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1613.0, 3), (77.0, 3), (1120.0, 2), (8305.0, 2), (6793.0, 2), (653.0, 2), (7852.0, 2), (8397.0, 2), (8426.0, 2), (8463.0, 2), (7990.0, 2), (3971.0, 2), (968.0, 2), (4607.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.2222222222222223 -- {'loss': 4.1517, 'grad_norm': 8.645439147949219, 'learning_rate': 0.0004266666666666667}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5653.0, 2), (30.0, 2), (8741.0, 2), (8249.0, 2), (8272.0, 2), (5765.0, 2), (1748.0, 3), (2798.0, 2), (7990.0, 2), (3396.0, 2), (5052.0, 3), (5067.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.333333333333333 -- {'loss': 6.343, 'grad_norm': 11.025206565856934, 'learning_rate': 0.00042444444444444447}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(510.0, 2), (4133.0, 2), (3643.0, 4), (2683.0, 2), (3213.0, 2), (1680.0, 3), (1697.0, 2), (2867.0, 2), (1366.0, 2), (4503.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.4444444444444446 -- {'loss': 5.5574, 'grad_norm': 10.327018737792969, 'learning_rate': 0.0004222222222222222}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3589.0, 2), (3697.0, 2), (1660.0, 3), (4235.0, 2), (8547.0, 2), (2916.0, 2), (4463.0, 2), (2943.0, 2), (395.0, 2), (6051.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.5555555555555554 -- {'loss': 3.101, 'grad_norm': 6.637960910797119, 'learning_rate': 0.00042}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3697.0, 2), (3229.0, 2), (4262.0, 2), (1717.0, 2), (2300.0, 2), (6445.0, 2), (5949.0, 3), (8511.0, 2), (5505.0, 2), (6548.0, 2), (4051.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.666666666666667 -- {'loss': 4.075, 'grad_norm': 8.12839412689209, 'learning_rate': 0.0004177777777777778}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6187.0, 2), (5694.0, 2), (3696.0, 2), (7820.0, 2), (2756.0, 2), (1748.0, 2), (4374.0, 2), (7472.0, 3), (7071.0, 2), (5026.0, 3), (968.0, 2), (4575.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.7777777777777777 -- {'loss': 6.0458, 'grad_norm': 32.644439697265625, 'learning_rate': 0.00041555555555555557}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8769.0, 2), (1615.0, 2), (4738.0, 2), (7815.0, 3), (1717.0, 2), (4797.0, 2), (8426.0, 3), (782.0, 2), (6443.0, 2), (1861.0, 2), (6548.0, 2), (5112.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.888888888888889 -- {'loss': 5.9307, 'grad_norm': 13.420624732971191, 'learning_rate': 0.0004133333333333333}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4098.0, 2), (1023.0, 2), (6704.0, 4), (3228.0, 2), (6303.0, 2), (1748.0, 2), (1763.0, 2), (2805.0, 2), (7424.0, 2), (6443.0, 3), (2916.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.0 -- {'loss': 3.9564, 'grad_norm': 4.000433444976807, 'learning_rate': 0.0004111111111111111}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3589.0, 2), (7203.0, 2), (4133.0, 2), (6704.0, 2), (2146.0, 2), (1664.0, 2), (5255.0, 2), (6443.0, 2), (2916.0, 2), (5505.0, 2), (421.0, 2), (5612.0, 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.111111111111111 -- {'loss': 5.1166, 'grad_norm': 9.79427719116211, 'learning_rate': 0.0004088888888888889}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(41.0, 2), (628.0, 2), (8320.0, 2), (267.0, 2), (5425.0, 2), (7053.0, 2), (8590.0, 2), (3469.0, 2), (421.0, 2), (8149.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.2222222222222223 -- {'loss': 4.281, 'grad_norm': 5.608072280883789, 'learning_rate': 0.00040666666666666667}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8741.0, 2), (7214.0, 2), (8803.0, 2), (2156.0, 2), (628.0, 2), (8827.0, 2), (5765.0, 2), (7813.0, 2), (6363.0, 2), (7070.0, 2), (421.0, 2), (6107.0, 2), (1521.0, 2), (5112.0, 4), (7162.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.3333333333333335 -- {'loss': 2.1342, 'grad_norm': 5.7973151206970215, 'learning_rate': 0.00040444444444444447}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.4444444444444446 -- {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0004022222222222222}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[(644.0, 4), (903.0, 2), (266.0, 4), (523.0, 3), (1294.0, 2), (529.0, 2), (1301.0, 2), (277.0, 2), (1305.0, 3), (796.0, 3), (416.0, 2), (931.0, 4), (165.0, 3), (39.0, 2), (680.0, 3), (1064.0, 2), (681.0, 2), (812.0, 3), (173.0, 5), (562.0, 2), (312.0, 2), (444.0, 3), (1345.0, 2), (194.0, 6), (580.0, 3), (211.0, 2), (215.0, 6), (88.0, 4), (87.0, 2), (882.0, 2), (117.0, 2), (251.0, 4), (511.0, 4)]\n",
      "[(902.0, 2), (1031.0, 2), (1020.0, 2), (148.0, 3), (1301.0, 4), (278.0, 3), (792.0, 2), (1305.0, 3), (152.0, 2), (157.0, 4), (1309.0, 5), (413.0, 2), (551.0, 4), (681.0, 3), (680.0, 2), (1076.0, 2), (1334.0, 3), (312.0, 4), (444.0, 6), (194.0, 2), (580.0, 3), (1350.0, 2), (202.0, 2), (78.0, 3), (848.0, 2), (81.0, 3), (82.0, 4), (211.0, 3), (215.0, 5), (97.0, 2), (484.0, 3), (1261.0, 2), (1264.0, 2), (116.0, 2), (251.0, 2), (764.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Eval Epoch: 3.4444444444444446 -- {'loss': 14.70226764678955, 'runtime': 4.6277, 'samples_per_second': 34.142, 'steps_per_second': 0.432}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(645.0, 2), (902.0, 3), (1031.0, 2), (148.0, 2), (277.0, 2), (152.0, 6), (1178.0, 2), (796.0, 3), (1182.0, 2), (416.0, 2), (1192.0, 2), (680.0, 2), (551.0, 2), (173.0, 2), (819.0, 3), (1334.0, 2), (311.0, 2), (312.0, 6), (184.0, 2), (444.0, 5), (194.0, 2), (580.0, 2), (1229.0, 2), (84.0, 2), (212.0, 2), (215.0, 4), (738.0, 2), (484.0, 2), (1261.0, 4), (1264.0, 2), (241.0, 2), (116.0, 2), (1142.0, 2), (251.0, 4), (764.0, 2)]\n",
      "[(8741.0, 2), (41.0, 2), (2627.0, 2), (2243.0, 2), (6340.0, 2), (8426.0, 2), (4463.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.111111111111111 -- {'loss': 2.6593, 'grad_norm': 3.4353184700012207, 'learning_rate': 0.0004}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3593.0, 2), (5653.0, 2), (1613.0, 2), (5734.0, 2), (7822.0, 2), (2798.0, 2), (7927.0, 2), (267.0, 2), (3942.0, 2), (4463.0, 2), (1468.0, 2), (5579.0, 2), (4083.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.2222222222222223 -- {'loss': 0.9754, 'grad_norm': 4.5801191329956055, 'learning_rate': 0.00039777777777777777}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8214.0, 2), (4133.0, 2), (2627.0, 2), (2119.0, 2), (1615.0, 2), (1125.0, 2), (628.0, 2), (8315.0, 2), (653.0, 2), (1748.0, 3), (3972.0, 2), (8641.0, 2), (985.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.3333333333333335 -- {'loss': 4.3183, 'grad_norm': 7.960028648376465, 'learning_rate': 0.00039555555555555557}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6187.0, 2), (6208.0, 2), (1615.0, 3), (4262.0, 2), (2300.0, 3), (787.0, 2), (6447.0, 2), (2916.0, 3), (395.0, 2), (963.0, 2), (4051.0, 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.4444444444444446 -- {'loss': 5.8501, 'grad_norm': 13.466296195983887, 'learning_rate': 0.0003933333333333333}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4098.0, 2), (5173.0, 2), (5223.0, 2), (5112.0, 2), (5765.0, 2), (1697.0, 2), (2787.0, 2), (7472.0, 2), (7500.0, 2), (2916.0, 2), (1410.0, 2), (5018.0, 2), (2459.0, 2), (7070.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.5555555555555554 -- {'loss': 2.4947, 'grad_norm': 2.936249256134033, 'learning_rate': 0.0003911111111111111}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2560.0, 2), (6246.0, 2), (6264.0, 2), (5624.0, 2), (6816.0, 2), (1717.0, 2), (2832.0, 2), (8470.0, 2), (3355.0, 3), (7990.0, 2), (883.0, 2), (4983.0, 2), (2943.0, 3), (5112.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.6666666666666665 -- {'loss': 5.8457, 'grad_norm': 5.650108814239502, 'learning_rate': 0.0003888888888888889}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1604.0, 2), (8825.0, 2), (5765.0, 2), (6793.0, 2), (4329.0, 2), (773.0, 2), (6443.0, 2), (5425.0, 3), (6037.0, 2), (5052.0, 2), (985.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.7777777777777777 -- {'loss': 3.1555, 'grad_norm': 2.964595079421997, 'learning_rate': 0.00038666666666666667}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6667.0, 2), (3643.0, 2), (77.0, 2), (6816.0, 2), (1717.0, 2), (8426.0, 2), (7019.0, 2), (3469.0, 2), (943.0, 2), (963.0, 2), (2521.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 2.888888888888889 -- {'loss': 4.8579, 'grad_norm': 51.39665985107422, 'learning_rate': 0.0003844444444444444}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4098.0, 2), (6695.0, 2), (5699.0, 2), (3148.0, 2), (2159.0, 2), (3697.0, 2), (4797.0, 2), (4356.0, 2), (1286.0, 2), (1352.0, 2), (6992.0, 2), (3959.0, 2), (8570.0, 2), (7071.0, 2), (7597.0, 2), (3012.0, 2), (8657.0, 2), (2534.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.0 -- {'loss': 5.0695, 'grad_norm': 6.04827356338501, 'learning_rate': 0.0003822222222222223}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2159.0, 2), (8841.0, 2), (4235.0, 2), (2193.0, 2), (3809.0, 2), (5425.0, 3), (6527.0, 2), (3469.0, 2), (6058.0, 3), (8119.0, 2), (4563.0, 2), (1521.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.111111111111111 -- {'loss': 1.8673, 'grad_norm': 3.679696559906006, 'learning_rate': 0.00038}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5682.0, 2), (2641.0, 2), (8315.0, 2), (3725.0, 2), (7472.0, 2), (816.0, 2), (1967.0, 2), (7095.0, 2), (5588.0, 2), (1023.0, 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.2222222222222223 -- {'loss': 2.7399, 'grad_norm': 4.1246538162231445, 'learning_rate': 0.00037777777777777777}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3643.0, 3), (2156.0, 3), (5257.0, 2), (4246.0, 2), (2756.0, 2), (2805.0, 2), (6443.0, 2), (7023.0, 2), (8073.0, 2), (5052.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.333333333333333 -- {'loss': 2.2063, 'grad_norm': 3.3485567569732666, 'learning_rate': 0.0003755555555555555}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7719.0, 2), (2601.0, 2), (3643.0, 2), (77.0, 3), (8305.0, 2), (5112.0, 2), (6786.0, 2), (7822.0, 2), (2805.0, 2), (2300.0, 2), (5384.0, 2), (7463.0, 2), (8547.0, 2), (2916.0, 3), (963.0, 2), (5624.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.4444444444444446 -- {'loss': 8.1318, 'grad_norm': 9.44621467590332, 'learning_rate': 0.0003733333333333334}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1566.0, 3), (8741.0, 2), (5699.0, 2), (4196.0, 2), (3697.0, 2), (8821.0, 2), (7820.0, 2), (7348.0, 3), (7973.0, 2), (6992.0, 2), (863.0, 2), (4463.0, 3), (8657.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.5555555555555554 -- {'loss': 6.0769, 'grad_norm': 5.973217487335205, 'learning_rate': 0.0003711111111111111}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3589.0, 3), (6208.0, 3), (5202.0, 2), (5734.0, 2), (4223.0, 2), (7820.0, 2), (6340.0, 2), (5363.0, 2), (2299.0, 2), (780.0, 2), (782.0, 2), (816.0, 2), (7472.0, 2), (7990.0, 2), (5444.0, 2), (4463.0, 2), (8607.0, 2), (4575.0, 2), (5606.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.666666666666667 -- {'loss': 8.2846, 'grad_norm': 7.186977386474609, 'learning_rate': 0.00036888888888888887}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4133.0, 2), (6704.0, 3), (86.0, 2), (2156.0, 2), (119.0, 2), (2795.0, 2), (2799.0, 2), (8450.0, 2), (6915.0, 2), (844.0, 2), (6999.0, 2), (5505.0, 2), (7071.0, 2), (5026.0, 2), (3017.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.7777777777777777 -- {'loss': 2.9873, 'grad_norm': 3.572148084640503, 'learning_rate': 0.00036666666666666667}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1537.0, 3), (6704.0, 2), (1604.0, 4), (7263.0, 2), (3697.0, 2), (2683.0, 2), (7813.0, 2), (7815.0, 2), (3778.0, 2), (4936.0, 2), (7071.0, 2), (2521.0, 2), (5112.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.888888888888889 -- {'loss': 1.5165, 'grad_norm': 3.359375476837158, 'learning_rate': 0.00036444444444444447}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3589.0, 2), (6667.0, 2), (6704.0, 3), (3697.0, 3), (628.0, 2), (3229.0, 2), (3352.0, 2), (2943.0, 2), (7623.0, 2), (7652.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.0 -- {'loss': 4.3929, 'grad_norm': 3.0973122119903564, 'learning_rate': 0.0003622222222222222}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8718.0, 2), (8821.0, 2), (1664.0, 2), (1680.0, 3), (1717.0, 2), (3353.0, 2), (843.0, 2), (2410.0, 2), (2504.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.111111111111111 -- {'loss': 2.5296, 'grad_norm': 3.1563193798065186, 'learning_rate': 0.00035999999999999997}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4098.0, 2), (8718.0, 2), (8305.0, 2), (1660.0, 2), (6288.0, 2), (7852.0, 2), (3778.0, 2), (5330.0, 2), (1748.0, 2), (4356.0, 2), (6443.0, 2), (924.0, 2), (5027.0, 2), (2981.0, 2), (8641.0, 2), (8130.0, 2), (4607.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.222222222222222 -- {'loss': 4.8319, 'grad_norm': 3.430270195007324, 'learning_rate': 0.00035777777777777777}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5653.0, 2), (8249.0, 2), (3643.0, 2), (578.0, 2), (628.0, 2), (8327.0, 2), (3213.0, 2), (7939.0, 2), (5425.0, 2), (3904.0, 2), (8083.0, 2), (2045.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.333333333333334 -- {'loss': 3.6677, 'grad_norm': 3.356282949447632, 'learning_rate': 0.00035555555555555557}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.444444444444445 -- {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0003533333333333333}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[(644.0, 4), (903.0, 2), (266.0, 4), (523.0, 3), (1294.0, 2), (529.0, 2), (1301.0, 2), (277.0, 2), (1305.0, 3), (796.0, 3), (416.0, 2), (931.0, 4), (165.0, 3), (39.0, 2), (680.0, 3), (1064.0, 2), (681.0, 2), (812.0, 3), (173.0, 5), (562.0, 2), (312.0, 2), (444.0, 3), (1345.0, 2), (194.0, 6), (580.0, 3), (211.0, 2), (215.0, 6), (88.0, 4), (87.0, 2), (882.0, 2), (117.0, 2), (251.0, 4), (511.0, 4)]\n",
      "[(902.0, 2), (1031.0, 2), (1020.0, 2), (148.0, 3), (1301.0, 4), (278.0, 3), (792.0, 2), (1305.0, 3), (152.0, 2), (157.0, 4), (1309.0, 5), (413.0, 2), (551.0, 4), (681.0, 3), (680.0, 2), (1076.0, 2), (1334.0, 3), (312.0, 4), (444.0, 6), (194.0, 2), (580.0, 3), (1350.0, 2), (202.0, 2), (78.0, 3), (848.0, 2), (81.0, 3), (82.0, 4), (211.0, 3), (215.0, 5), (97.0, 2), (484.0, 3), (1261.0, 2), (1264.0, 2), (116.0, 2), (251.0, 2), (764.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Eval Epoch: 4.444444444444445 -- {'loss': 8.228592872619629, 'runtime': 4.6156, 'samples_per_second': 34.232, 'steps_per_second': 0.433}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(645.0, 2), (902.0, 3), (1031.0, 2), (148.0, 2), (277.0, 2), (152.0, 6), (1178.0, 2), (796.0, 3), (1182.0, 2), (416.0, 2), (1192.0, 2), (680.0, 2), (551.0, 2), (173.0, 2), (819.0, 3), (1334.0, 2), (311.0, 2), (312.0, 6), (184.0, 2), (444.0, 5), (194.0, 2), (580.0, 2), (1229.0, 2), (84.0, 2), (212.0, 2), (215.0, 4), (738.0, 2), (484.0, 2), (1261.0, 4), (1264.0, 2), (241.0, 2), (116.0, 2), (1142.0, 2), (251.0, 4), (764.0, 2)]\n",
      "[(6208.0, 2), (77.0, 2), (703.0, 2), (787.0, 2), (3398.0, 2), (421.0, 2), (8130.0, 2), (5588.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.111111111111111 -- {'loss': 3.7351, 'grad_norm': 4.333193778991699, 'learning_rate': 0.0003511111111111111}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4098.0, 2), (2566.0, 3), (4124.0, 2), (41.0, 3), (77.0, 2), (3697.0, 2), (8821.0, 2), (2805.0, 2), (7927.0, 2), (5368.0, 2), (8511.0, 2), (7070.0, 2), (2981.0, 2), (1967.0, 2), (2521.0, 2), (5112.0, 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.2222222222222223 -- {'loss': 7.9307, 'grad_norm': 6.292405128479004, 'learning_rate': 0.0003488888888888889}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8718.0, 2), (8741.0, 2), (4133.0, 2), (6246.0, 2), (5765.0, 2), (7852.0, 2), (3809.0, 2), (2787.0, 2), (4358.0, 2), (816.0, 2), (6992.0, 2), (8034.0, 2), (883.0, 3), (5052.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.3333333333333335 -- {'loss': 5.1027, 'grad_norm': 6.481539249420166, 'learning_rate': 0.00034666666666666667}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6150.0, 2), (1023.0, 2), (6704.0, 2), (8834.0, 2), (3213.0, 2), (653.0, 2), (1810.0, 2), (7472.0, 2), (3469.0, 2), (7071.0, 2), (1468.0, 2), (8131.0, 2), (5579.0, 2), (8175.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.4444444444444446 -- {'loss': 1.0573, 'grad_norm': 4.788677215576172, 'learning_rate': 0.0003444444444444445}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3643.0, 2), (2627.0, 2), (5699.0, 2), (119.0, 2), (1748.0, 2), (2805.0, 2), (6903.0, 2), (6915.0, 2), (2867.0, 2), (1359.0, 2), (4463.0, 2), (5505.0, 2), (5026.0, 2), (958.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.5555555555555554 -- {'loss': 1.5382, 'grad_norm': 3.6739587783813477, 'learning_rate': 0.0003422222222222222}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4124.0, 2), (5699.0, 3), (6751.0, 2), (7813.0, 2), (1697.0, 3), (2780.0, 2), (773.0, 2), (5949.0, 2), (5952.0, 2), (843.0, 3), (6992.0, 2), (3959.0, 2), (2975.0, 2), (5579.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.6666666666666665 -- {'loss': 1.8096, 'grad_norm': 4.806834697723389, 'learning_rate': 0.00034}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2048.0, 2), (8315.0, 2), (1659.0, 2), (4737.0, 2), (1680.0, 2), (6816.0, 2), (6386.0, 2), (7463.0, 2), (1468.0, 2), (8641.0, 2), (963.0, 2), (7623.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.7777777777777777 -- {'loss': 5.1124, 'grad_norm': 5.201562881469727, 'learning_rate': 0.00033777777777777777}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4098.0, 2), (1023.0, 2), (1063.0, 2), (64.0, 2), (5699.0, 2), (2627.0, 2), (6264.0, 2), (3718.0, 2), (1717.0, 2), (6340.0, 2), (6992.0, 2), (4463.0, 2), (1410.0, 3), (7070.0, 2), (7071.0, 3), (4575.0, 2), (8679.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 3.888888888888889 -- {'loss': 5.2795, 'grad_norm': 4.193465709686279, 'learning_rate': 0.0003355555555555556}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3587.0, 2), (1613.0, 2), (5203.0, 2), (605.0, 2), (2156.0, 2), (5778.0, 2), (2770.0, 2), (1748.0, 3), (6901.0, 2), (1793.0, 2), (5425.0, 2), (6548.0, 2), (7597.0, 2), (6629.0, 2), (8679.0, 2), (1521.0, 2), (7162.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.0 -- {'loss': 7.3279, 'grad_norm': 5.867407321929932, 'learning_rate': 0.0003333333333333333}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5643.0, 2), (6704.0, 3), (3643.0, 2), (4196.0, 2), (5734.0, 3), (2159.0, 2), (5765.0, 2), (8841.0, 3), (3229.0, 2), (3804.0, 2), (2805.0, 2), (7463.0, 2), (3886.0, 2), (8083.0, 2), (7095.0, 2), (3017.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.111111111111111 -- {'loss': 7.5671, 'grad_norm': 5.226157188415527, 'learning_rate': 0.0003311111111111111}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(526.0, 2), (7203.0, 2), (2624.0, 2), (3697.0, 2), (8426.0, 2), (8470.0, 2), (5425.0, 3), (7990.0, 2), (7053.0, 2), (3469.0, 2), (7071.0, 2), (5112.0, 3), (2045.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.222222222222222 -- {'loss': 4.5059, 'grad_norm': 3.833479881286621, 'learning_rate': 0.0003288888888888889}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2560.0, 2), (33.0, 2), (8741.0, 3), (7719.0, 2), (5699.0, 2), (5202.0, 2), (612.0, 2), (3696.0, 2), (1680.0, 2), (1697.0, 2), (1748.0, 3), (1763.0, 2), (2787.0, 2), (8426.0, 2), (1366.0, 2), (1367.0, 2), (4446.0, 2), (7023.0, 2), (3465.0, 3), (5027.0, 2), (1023.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.333333333333333 -- {'loss': 10.0523, 'grad_norm': 11.915170669555664, 'learning_rate': 0.0003266666666666667}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6704.0, 2), (64.0, 2), (5224.0, 3), (6762.0, 2), (3697.0, 4), (5624.0, 3), (5818.0, 2), (5425.0, 2), (2916.0, 2), (2943.0, 2), (4607.0, 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.444444444444445 -- {'loss': 5.3228, 'grad_norm': 4.195725440979004, 'learning_rate': 0.0003244444444444444}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3589.0, 2), (2601.0, 2), (1602.0, 2), (2140.0, 2), (5815.0, 2), (2798.0, 2), (796.0, 3), (6443.0, 2), (2916.0, 2), (6021.0, 2), (395.0, 3), (7071.0, 2), (5588.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.555555555555555 -- {'loss': 5.7476, 'grad_norm': 5.692105293273926, 'learning_rate': 0.0003222222222222222}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4098.0, 2), (14.0, 2), (3643.0, 2), (578.0, 2), (4196.0, 2), (628.0, 2), (7820.0, 2), (8335.0, 2), (8397.0, 2), (7424.0, 2), (2331.0, 2), (6445.0, 2), (8504.0, 2), (1410.0, 2), (8083.0, 2), (8657.0, 2), (8663.0, 2), (4607.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.666666666666667 -- {'loss': 7.4529, 'grad_norm': 5.17855978012085, 'learning_rate': 0.00032}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4133.0, 2), (6704.0, 2), (4166.0, 2), (5734.0, 3), (1126.0, 2), (6428.0, 2), (5478.0, 2), (2943.0, 2), (6037.0, 2), (1967.0, 2), (8119.0, 2), (5052.0, 2), (968.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.777777777777778 -- {'loss': 5.1053, 'grad_norm': 5.2316789627075195, 'learning_rate': 0.0003177777777777778}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(526.0, 2), (4133.0, 3), (2640.0, 2), (3696.0, 2), (7383.0, 2), (2294.0, 2), (2864.0, 2), (2916.0, 2), (7530.0, 2), (4463.0, 2), (7040.0, 2), (8607.0, 2), (2521.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.888888888888889 -- {'loss': 4.5616, 'grad_norm': 4.800128936767578, 'learning_rate': 0.0003155555555555555}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3593.0, 2), (5699.0, 2), (2146.0, 2), (1150.0, 2), (653.0, 3), (3213.0, 2), (2206.0, 2), (238.0, 2), (6999.0, 2), (8035.0, 2), (2415.0, 2), (924.0, 2), (968.0, 2), (6638.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.0 -- {'loss': 1.7815, 'grad_norm': 3.2720754146575928, 'learning_rate': 0.0003133333333333334}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6667.0, 2), (1063.0, 2), (41.0, 2), (6720.0, 2), (1102.0, 2), (5247.0, 2), (4223.0, 2), (7463.0, 2), (5942.0, 2), (6037.0, 2), (4503.0, 2), (8641.0, 2), (5606.0, 3), (5112.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.111111111111111 -- {'loss': 5.2078, 'grad_norm': 4.16691255569458, 'learning_rate': 0.0003111111111111111}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4098.0, 2), (3589.0, 2), (8249.0, 2), (1604.0, 2), (628.0, 2), (7815.0, 2), (703.0, 2), (7904.0, 2), (2300.0, 2), (4356.0, 2), (2821.0, 2), (2832.0, 2), (3396.0, 2), (2916.0, 2), (2943.0, 2), (6021.0, 2), (5612.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.222222222222222 -- {'loss': 1.2832, 'grad_norm': 4.769179344177246, 'learning_rate': 0.0003088888888888889}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4098.0, 4), (3643.0, 2), (2683.0, 2), (7927.0, 2), (3878.0, 2), (4463.0, 4), (4083.0, 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.333333333333334 -- {'loss': 1.3371, 'grad_norm': 3.3540267944335938, 'learning_rate': 0.0003066666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.444444444444445 -- {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0003044444444444445}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[(644.0, 4), (903.0, 2), (266.0, 4), (523.0, 3), (1294.0, 2), (529.0, 2), (1301.0, 2), (277.0, 2), (1305.0, 3), (796.0, 3), (416.0, 2), (931.0, 4), (165.0, 3), (39.0, 2), (680.0, 3), (1064.0, 2), (681.0, 2), (812.0, 3), (173.0, 5), (562.0, 2), (312.0, 2), (444.0, 3), (1345.0, 2), (194.0, 6), (580.0, 3), (211.0, 2), (215.0, 6), (88.0, 4), (87.0, 2), (882.0, 2), (117.0, 2), (251.0, 4), (511.0, 4)]\n",
      "[(902.0, 2), (1031.0, 2), (1020.0, 2), (148.0, 3), (1301.0, 4), (278.0, 3), (792.0, 2), (1305.0, 3), (152.0, 2), (157.0, 4), (1309.0, 5), (413.0, 2), (551.0, 4), (681.0, 3), (680.0, 2), (1076.0, 2), (1334.0, 3), (312.0, 4), (444.0, 6), (194.0, 2), (580.0, 3), (1350.0, 2), (202.0, 2), (78.0, 3), (848.0, 2), (81.0, 3), (82.0, 4), (211.0, 3), (215.0, 5), (97.0, 2), (484.0, 3), (1261.0, 2), (1264.0, 2), (116.0, 2), (251.0, 2), (764.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Eval Epoch: 5.444444444444445 -- {'loss': 6.793605804443359, 'runtime': 4.6152, 'samples_per_second': 34.235, 'steps_per_second': 0.433}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(645.0, 2), (902.0, 3), (1031.0, 2), (148.0, 2), (277.0, 2), (152.0, 6), (1178.0, 2), (796.0, 3), (1182.0, 2), (416.0, 2), (1192.0, 2), (680.0, 2), (551.0, 2), (173.0, 2), (819.0, 3), (1334.0, 2), (311.0, 2), (312.0, 6), (184.0, 2), (444.0, 5), (194.0, 2), (580.0, 2), (1229.0, 2), (84.0, 2), (212.0, 2), (215.0, 4), (738.0, 2), (484.0, 2), (1261.0, 4), (1264.0, 2), (241.0, 2), (116.0, 2), (1142.0, 2), (251.0, 4), (764.0, 2)]\n",
      "[(6704.0, 2), (6208.0, 3), (4166.0, 2), (605.0, 2), (7815.0, 2), (3804.0, 2), (3809.0, 2), (6992.0, 2), (1363.0, 2), (5478.0, 2), (3469.0, 2), (7654.0, 2), (3046.0, 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.111111111111111 -- {'loss': 6.8652, 'grad_norm': 6.8708343505859375, 'learning_rate': 0.0003022222222222222}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2627.0, 2), (4738.0, 2), (1680.0, 2), (2805.0, 3), (2864.0, 2), (883.0, 2), (3971.0, 2), (1929.0, 2), (7053.0, 2), (7095.0, 2), (8175.0, 2), (1023.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.222222222222222 -- {'loss': 6.0899, 'grad_norm': 3.8183462619781494, 'learning_rate': 0.0003}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2566.0, 2), (3643.0, 2), (5694.0, 3), (7829.0, 2), (5818.0, 2), (2756.0, 2), (7463.0, 2), (8083.0, 3), (7070.0, 2), (7623.0, 2), (8658.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.333333333333333 -- {'loss': 4.6678, 'grad_norm': 5.258097171783447, 'learning_rate': 0.0002977777777777778}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6667.0, 2), (7719.0, 2), (6704.0, 2), (6208.0, 2), (1102.0, 2), (2156.0, 3), (4737.0, 2), (1748.0, 2), (6915.0, 2), (1367.0, 2), (1379.0, 2), (4463.0, 2), (4979.0, 2), (5579.0, 2), (4565.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.444444444444445 -- {'loss': 1.1728, 'grad_norm': 5.301900386810303, 'learning_rate': 0.0002955555555555556}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(526.0, 2), (4284.0, 2), (2795.0, 2), (1352.0, 3), (7070.0, 2), (5026.0, 2), (5052.0, 2), (968.0, 2), (7162.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.555555555555555 -- {'loss': 0.7191, 'grad_norm': 3.8338537216186523, 'learning_rate': 0.0002933333333333333}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(30.0, 2), (1566.0, 2), (1065.0, 2), (3643.0, 4), (3696.0, 2), (8315.0, 2), (691.0, 2), (6340.0, 2), (8426.0, 3), (3886.0, 2), (3904.0, 2), (6992.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.666666666666667 -- {'loss': 1.903, 'grad_norm': 2.7145516872406006, 'learning_rate': 0.00029111111111111113}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3589.0, 3), (5176.0, 3), (5699.0, 2), (2119.0, 2), (77.0, 2), (8803.0, 2), (2683.0, 2), (4260.0, 2), (1717.0, 2), (306.0, 3), (5437.0, 2), (395.0, 2), (8663.0, 2), (6107.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.777777777777778 -- {'loss': 8.2569, 'grad_norm': 7.178400993347168, 'learning_rate': 0.0002888888888888889}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4133.0, 2), (6720.0, 2), (6762.0, 2), (3697.0, 2), (1150.0, 2), (7820.0, 2), (8855.0, 2), (800.0, 2), (5434.0, 2), (2405.0, 2), (7040.0, 2), (395.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 4.888888888888889 -- {'loss': 2.6903, 'grad_norm': 3.091766834259033, 'learning_rate': 0.0002866666666666667}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3587.0, 2), (3148.0, 2), (3672.0, 2), (3697.0, 2), (1717.0, 3), (7462.0, 2), (5952.0, 2), (3396.0, 2), (3412.0, 2), (1940.0, 2), (2981.0, 2), (1464.0, 2), (5052.0, 2), (963.0, 2), (5112.0, 2), (4607.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.0 -- {'loss': 4.7822, 'grad_norm': 2.8234665393829346, 'learning_rate': 0.0002844444444444444}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6704.0, 3), (8841.0, 2), (6340.0, 2), (3786.0, 2), (2300.0, 2), (787.0, 2), (7990.0, 2), (2943.0, 2), (8100.0, 2), (5579.0, 2), (8657.0, 2), (4575.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.111111111111111 -- {'loss': 1.8582, 'grad_norm': 2.777223825454712, 'learning_rate': 0.00028222222222222223}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3085.0, 2), (5699.0, 2), (3141.0, 2), (3697.0, 2), (8305.0, 2), (8821.0, 3), (5112.0, 2), (8834.0, 2), (5765.0, 2), (1748.0, 2), (7904.0, 2), (8470.0, 2), (4463.0, 2), (8570.0, 2), (2943.0, 2), (392.0, 2), (8119.0, 2), (8640.0, 2), (4563.0, 2), (1521.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.222222222222222 -- {'loss': 3.6353, 'grad_norm': 5.483209609985352, 'learning_rate': 0.00028000000000000003}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4098.0, 2), (8741.0, 3), (64.0, 3), (4166.0, 2), (77.0, 3), (86.0, 2), (653.0, 2), (6340.0, 2), (768.0, 2), (8547.0, 2), (2975.0, 2), (7071.0, 3), (8657.0, 2), (5606.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.333333333333333 -- {'loss': 8.175, 'grad_norm': 14.084636688232422, 'learning_rate': 0.0002777777777777778}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2624.0, 2), (578.0, 2), (2640.0, 2), (8299.0, 2), (5112.0, 2), (5765.0, 3), (7813.0, 2), (7840.0, 2), (4262.0, 2), (6901.0, 2), (2300.0, 2), (3352.0, 2), (816.0, 2), (7472.0, 2), (4463.0, 2), (5505.0, 2), (985.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.444444444444445 -- {'loss': 8.0414, 'grad_norm': 10.086591720581055, 'learning_rate': 0.0002755555555555556}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6187.0, 2), (3643.0, 2), (8315.0, 2), (5765.0, 2), (653.0, 2), (4246.0, 3), (6812.0, 2), (2805.0, 2), (6992.0, 2), (1967.0, 2), (1479.0, 2), (5067.0, 2), (2045.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.555555555555555 -- {'loss': 5.81, 'grad_norm': 6.711795330047607, 'learning_rate': 0.00027333333333333333}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5653.0, 2), (4133.0, 2), (3697.0, 2), (6264.0, 2), (5775.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.666666666666667 -- {'loss': 2.1765, 'grad_norm': 2.668764114379883, 'learning_rate': 0.00027111111111111113}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8718.0, 2), (7203.0, 2), (7719.0, 2), (5699.0, 2), (77.0, 2), (119.0, 2), (7288.0, 2), (7822.0, 2), (7463.0, 2), (3886.0, 2), (6992.0, 2), (2916.0, 4), (7070.0, 2), (5026.0, 2), (8100.0, 2), (421.0, 2), (4051.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.777777777777778 -- {'loss': 6.1081, 'grad_norm': 8.878589630126953, 'learning_rate': 0.00026888888888888893}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(605.0, 2), (8305.0, 2), (653.0, 2), (3229.0, 2), (2206.0, 2), (6368.0, 2), (4329.0, 2), (7927.0, 2), (8450.0, 2), (3355.0, 2), (7990.0, 2), (6523.0, 2), (924.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.888888888888889 -- {'loss': 4.8903, 'grad_norm': 10.760701179504395, 'learning_rate': 0.0002666666666666667}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4098.0, 2), (6762.0, 2), (8299.0, 2), (1680.0, 2), (3229.0, 2), (1210.0, 2), (1748.0, 2), (8463.0, 2), (5425.0, 2), (8035.0, 3), (7071.0, 2), (932.0, 2), (968.0, 2), (2045.0, 3), (4607.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 6.0 -- {'loss': 4.9903, 'grad_norm': 5.824221134185791, 'learning_rate': 0.00026444444444444443}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4098.0, 2), (7.0, 2), (4133.0, 2), (5682.0, 2), (4196.0, 2), (628.0, 2), (7820.0, 2), (1748.0, 2), (6443.0, 2), (7472.0, 2), (5425.0, 2), (5478.0, 2), (3465.0, 2), (4503.0, 2), (5026.0, 2), (8119.0, 2), (8640.0, 2), (8654.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 6.111111111111111 -- {'loss': 8.29, 'grad_norm': 5.556473255157471, 'learning_rate': 0.00026222222222222223}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6704.0, 2), (5699.0, 3), (6246.0, 2), (5759.0, 2), (1748.0, 2), (3804.0, 2), (8034.0, 2), (7071.0, 2), (5026.0, 2), (5052.0, 2), (2514.0, 2), (2521.0, 3), (4607.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 6.222222222222222 -- {'loss': 5.0206, 'grad_norm': 2.960458278656006, 'learning_rate': 0.00026000000000000003}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4246.0, 2), (4295.0, 2), (6386.0, 2), (2294.0, 2), (1359.0, 2), (2916.0, 2), (3469.0, 2), (7056.0, 2), (3514.0, 2), (5052.0, 2), (2521.0, 3), (5112.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 6.333333333333334 -- {'loss': 5.6046, 'grad_norm': 7.184483528137207, 'learning_rate': 0.0002577777777777778}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 6.444444444444445 -- {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.00025555555555555553}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[(644.0, 4), (903.0, 2), (266.0, 4), (523.0, 3), (1294.0, 2), (529.0, 2), (1301.0, 2), (277.0, 2), (1305.0, 3), (796.0, 3), (416.0, 2), (931.0, 4), (165.0, 3), (39.0, 2), (680.0, 3), (1064.0, 2), (681.0, 2), (812.0, 3), (173.0, 5), (562.0, 2), (312.0, 2), (444.0, 3), (1345.0, 2), (194.0, 6), (580.0, 3), (211.0, 2), (215.0, 6), (88.0, 4), (87.0, 2), (882.0, 2), (117.0, 2), (251.0, 4), (511.0, 4)]\n",
      "[(902.0, 2), (1031.0, 2), (1020.0, 2), (148.0, 3), (1301.0, 4), (278.0, 3), (792.0, 2), (1305.0, 3), (152.0, 2), (157.0, 4), (1309.0, 5), (413.0, 2), (551.0, 4), (681.0, 3), (680.0, 2), (1076.0, 2), (1334.0, 3), (312.0, 4), (444.0, 6), (194.0, 2), (580.0, 3), (1350.0, 2), (202.0, 2), (78.0, 3), (848.0, 2), (81.0, 3), (82.0, 4), (211.0, 3), (215.0, 5), (97.0, 2), (484.0, 3), (1261.0, 2), (1264.0, 2), (116.0, 2), (251.0, 2), (764.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Eval Epoch: 6.444444444444445 -- {'loss': 9.297707557678223, 'runtime': 4.6134, 'samples_per_second': 34.248, 'steps_per_second': 0.434}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(645.0, 2), (902.0, 3), (1031.0, 2), (148.0, 2), (277.0, 2), (152.0, 6), (1178.0, 2), (796.0, 3), (1182.0, 2), (416.0, 2), (1192.0, 2), (680.0, 2), (551.0, 2), (173.0, 2), (819.0, 3), (1334.0, 2), (311.0, 2), (312.0, 6), (184.0, 2), (444.0, 5), (194.0, 2), (580.0, 2), (1229.0, 2), (84.0, 2), (212.0, 2), (215.0, 4), (738.0, 2), (484.0, 2), (1261.0, 4), (1264.0, 2), (241.0, 2), (116.0, 2), (1142.0, 2), (251.0, 4), (764.0, 2)]\n",
      "[(4133.0, 2), (3697.0, 2), (5765.0, 2), (1748.0, 4), (6927.0, 2), (7472.0, 2), (7019.0, 2), (1929.0, 2), (5018.0, 2), (963.0, 2), (5077.0, 2), (5112.0, 2), (7162.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.111111111111111 -- {'loss': 5.1169, 'grad_norm': 3.858614206314087, 'learning_rate': 0.0002533333333333334}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3643.0, 2), (6208.0, 2), (3697.0, 2), (5272.0, 2), (3280.0, 2), (5363.0, 3), (6901.0, 2), (2805.0, 2), (1793.0, 2), (4356.0, 2), (8470.0, 2), (3514.0, 2), (5052.0, 2), (963.0, 2), (991.0, 2), (5624.0, 2), (1023.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.222222222222222 -- {'loss': 2.9043, 'grad_norm': 4.745729446411133, 'learning_rate': 0.00025111111111111113}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(526.0, 2), (41.0, 2), (8320.0, 2), (7820.0, 2), (1727.0, 2), (2787.0, 2), (2795.0, 2), (6443.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.333333333333333 -- {'loss': 3.9048, 'grad_norm': 5.50202751159668, 'learning_rate': 0.0002488888888888889}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3188.0, 2), (4738.0, 2), (8834.0, 2), (5765.0, 2), (6340.0, 2), (4295.0, 2), (2805.0, 2), (2300.0, 2), (773.0, 2), (6992.0, 2), (7040.0, 2), (7056.0, 2), (3017.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.444444444444445 -- {'loss': 1.9406, 'grad_norm': 2.9937052726745605, 'learning_rate': 0.0002466666666666667}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3589.0, 2), (6704.0, 2), (3643.0, 2), (3672.0, 2), (3697.0, 2), (653.0, 3), (6915.0, 2), (5384.0, 2), (8497.0, 3), (1861.0, 2), (4503.0, 2), (7070.0, 2), (8679.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.555555555555555 -- {'loss': 2.91, 'grad_norm': 3.2851850986480713, 'learning_rate': 0.00024444444444444443}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3643.0, 2), (2640.0, 2), (2156.0, 2), (2159.0, 3), (8315.0, 2), (1150.0, 2), (3469.0, 2), (421.0, 2), (1468.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.666666666666667 -- {'loss': 1.4055, 'grad_norm': 2.556844472885132, 'learning_rate': 0.00024222222222222223}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6699.0, 2), (3643.0, 3), (8299.0, 2), (8821.0, 2), (2193.0, 2), (6812.0, 2), (1748.0, 2), (796.0, 2), (3412.0, 2), (8547.0, 2), (5505.0, 2), (4487.0, 2), (5052.0, 2), (963.0, 2), (2521.0, 2), (7652.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.777777777777778 -- {'loss': 6.7164, 'grad_norm': 47.54352569580078, 'learning_rate': 0.00024}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7203.0, 3), (6704.0, 2), (8827.0, 2), (6816.0, 2), (796.0, 2), (4463.0, 4), (8607.0, 2), (7071.0, 2), (8640.0, 2), (5606.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 5.888888888888889 -- {'loss': 1.5377, 'grad_norm': 2.636791467666626, 'learning_rate': 0.00023777777777777778}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(510.0, 2), (1023.0, 2), (3643.0, 2), (77.0, 2), (7813.0, 2), (5765.0, 2), (7348.0, 2), (5818.0, 2), (1359.0, 2), (5052.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 6.0 -- {'loss': 4.2833, 'grad_norm': 3.435574531555176, 'learning_rate': 0.00023555555555555556}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1604.0, 2), (4696.0, 2), (5759.0, 2), (4329.0, 2), (2864.0, 2), (3904.0, 2), (1359.0, 2), (1879.0, 2), (395.0, 4), (3469.0, 2), (5026.0, 2), (7654.0, 2), (5607.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 6.111111111111111 -- {'loss': 4.1523, 'grad_norm': 5.996998310089111, 'learning_rate': 0.00023333333333333333}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1537.0, 2), (4133.0, 2), (77.0, 2), (5223.0, 3), (8821.0, 2), (6793.0, 2), (4607.0, 2), (4302.0, 2), (5356.0, 2), (7990.0, 2), (8073.0, 2), (5067.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 6.222222222222222 -- {'loss': 4.4402, 'grad_norm': 5.368873119354248, 'learning_rate': 0.0002311111111111111}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(64.0, 2), (8305.0, 2), (5257.0, 2), (653.0, 2), (1680.0, 2), (6341.0, 2), (3809.0, 2), (7463.0, 2), (2916.0, 2), (6021.0, 2), (392.0, 2), (5112.0, 2), (510.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 6.333333333333333 -- {'loss': 2.6917, 'grad_norm': 3.902343273162842, 'learning_rate': 0.0002288888888888889}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4098.0, 2), (5699.0, 2), (1664.0, 2), (6340.0, 2), (6386.0, 2), (2864.0, 2), (5425.0, 2), (843.0, 2), (2444.0, 2), (7070.0, 2), (6051.0, 2), (421.0, 2), (5606.0, 2), (5612.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 6.444444444444445 -- {'loss': 4.5896, 'grad_norm': 5.962998867034912, 'learning_rate': 0.00022666666666666666}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(526.0, 2), (7719.0, 2), (5734.0, 2), (3697.0, 2), (7288.0, 2), (2799.0, 2), (3886.0, 2), (5952.0, 2), (1926.0, 2), (1956.0, 2), (7095.0, 2), (4051.0, 2), (4563.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 6.555555555555555 -- {'loss': 1.6684, 'grad_norm': 2.564931631088257, 'learning_rate': 0.00022444444444444446}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1537.0, 2), (8741.0, 2), (4166.0, 2), (8825.0, 2), (5784.0, 2), (6303.0, 2), (2790.0, 2), (8463.0, 2), (6443.0, 2), (5425.0, 2), (5942.0, 2), (8536.0, 2), (2916.0, 2), (1410.0, 2), (4503.0, 2), (4607.0, 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 6.666666666666667 -- {'loss': 3.5573, 'grad_norm': 3.7217512130737305, 'learning_rate': 0.0002222222222222222}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6697.0, 2), (5699.0, 4), (628.0, 2), (6812.0, 2), (3229.0, 2), (3355.0, 2), (6443.0, 2), (4446.0, 2), (2916.0, 3), (2459.0, 2), (1468.0, 2), (5052.0, 2), (2521.0, 3), (5606.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 6.777777777777778 -- {'loss': 6.4464, 'grad_norm': 6.036905288696289, 'learning_rate': 0.00022}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8718.0, 2), (2119.0, 2), (8835.0, 2), (5815.0, 2), (4295.0, 2), (1763.0, 2), (8486.0, 2), (1356.0, 2), (5986.0, 2), (4463.0, 2), (883.0, 2), (2943.0, 3), (2960.0, 2), (924.0, 2), (6604.0, 2), (8679.0, 2), (5112.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 6.888888888888889 -- {'loss': 1.9747, 'grad_norm': 5.000075340270996, 'learning_rate': 0.00021777777777777776}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6667.0, 2), (8841.0, 3), (4797.0, 2), (1748.0, 2), (8426.0, 2), (7939.0, 2), (2331.0, 2), (7990.0, 2), (2916.0, 2), (1967.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 7.0 -- {'loss': 5.3917, 'grad_norm': 4.20573091506958, 'learning_rate': 0.00021555555555555556}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(77.0, 2), (5734.0, 2), (8305.0, 2), (7820.0, 2), (7904.0, 2), (8426.0, 2), (1793.0, 2), (7939.0, 3), (5425.0, 2), (916.0, 2), (3046.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 7.111111111111111 -- {'loss': 3.7098, 'grad_norm': 3.6639294624328613, 'learning_rate': 0.00021333333333333336}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4098.0, 2), (6661.0, 2), (6667.0, 2), (1063.0, 2), (576.0, 2), (5224.0, 2), (2156.0, 2), (6791.0, 2), (1717.0, 2), (4463.0, 2), (5112.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 7.222222222222222 -- {'loss': 2.7946, 'grad_norm': 5.056323051452637, 'learning_rate': 0.0002111111111111111}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6704.0, 2), (5682.0, 3), (6208.0, 2), (6762.0, 2), (3697.0, 3), (8315.0, 2), (5765.0, 2), (5368.0, 2), (7424.0, 2), (1366.0, 2), (6505.0, 2), (3465.0, 2), (7071.0, 2), (8130.0, 2), (5112.0, 4)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 7.333333333333334 -- {'loss': 4.9611, 'grad_norm': 3.102872371673584, 'learning_rate': 0.0002088888888888889}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 7.444444444444445 -- {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.00020666666666666666}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[(644.0, 4), (903.0, 2), (266.0, 4), (523.0, 3), (1294.0, 2), (529.0, 2), (1301.0, 2), (277.0, 2), (1305.0, 3), (796.0, 3), (416.0, 2), (931.0, 4), (165.0, 3), (39.0, 2), (680.0, 3), (1064.0, 2), (681.0, 2), (812.0, 3), (173.0, 5), (562.0, 2), (312.0, 2), (444.0, 3), (1345.0, 2), (194.0, 6), (580.0, 3), (211.0, 2), (215.0, 6), (88.0, 4), (87.0, 2), (882.0, 2), (117.0, 2), (251.0, 4), (511.0, 4)]\n",
      "[(902.0, 2), (1031.0, 2), (1020.0, 2), (148.0, 3), (1301.0, 4), (278.0, 3), (792.0, 2), (1305.0, 3), (152.0, 2), (157.0, 4), (1309.0, 5), (413.0, 2), (551.0, 4), (681.0, 3), (680.0, 2), (1076.0, 2), (1334.0, 3), (312.0, 4), (444.0, 6), (194.0, 2), (580.0, 3), (1350.0, 2), (202.0, 2), (78.0, 3), (848.0, 2), (81.0, 3), (82.0, 4), (211.0, 3), (215.0, 5), (97.0, 2), (484.0, 3), (1261.0, 2), (1264.0, 2), (116.0, 2), (251.0, 2), (764.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Eval Epoch: 7.444444444444445 -- {'loss': 11.764237403869629, 'runtime': 4.6172, 'samples_per_second': 34.22, 'steps_per_second': 0.433}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Train Epoch: 7.444444444444445 -- {'runtime': 399.3811, 'samples_per_second': 71.986, 'steps_per_second': 0.563, 'loss': 6.666211252862757}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(645.0, 2), (902.0, 3), (1031.0, 2), (148.0, 2), (277.0, 2), (152.0, 6), (1178.0, 2), (796.0, 3), (1182.0, 2), (416.0, 2), (1192.0, 2), (680.0, 2), (551.0, 2), (173.0, 2), (819.0, 3), (1334.0, 2), (311.0, 2), (312.0, 6), (184.0, 2), (444.0, 5), (194.0, 2), (580.0, 2), (1229.0, 2), (84.0, 2), (212.0, 2), (215.0, 4), (738.0, 2), (484.0, 2), (1261.0, 4), (1264.0, 2), (241.0, 2), (116.0, 2), (1142.0, 2), (251.0, 4), (764.0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Loading trained model from disk...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for ./rrivera1849/ contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/./rrivera1849/.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (./rrivera1849/)] Finished loading.\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Trainer 'LUAR Trainer' finished and is saved to disk. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Done. ✨ Results in folder: ./supcon_ta2_model\n"
     ]
    }
   ],
   "source": [
    "train_datadreamer_ta2(fold, './supcon_ta2_model', 'SupConLoss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2cb8d-c013-4245-a9e8-41dfa071294a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ajay-pausi-venv",
   "language": "python",
   "name": "ajay-pausi-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

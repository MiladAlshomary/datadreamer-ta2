{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc1b260-9fd1-4076-a90b-ba63f8b8ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac0f8f8-c3fb-4016-bfff-211fdf261c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a977db5-ba6a-41ab-986a-368dc42cb39c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datadreamer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mSupContrastLoss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SupConLoss\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mRankingLoss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultipleNegativesSymmetricRankingLoss\n",
      "File \u001b[0;32m~/datadreamer-ta2/training_source/trainer.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatadreamer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataDreamer\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatadreamer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msteps\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataSource\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainerCallback\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datadreamer'"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from trainer import *\n",
    "from SupContrastLoss import SupConLoss\n",
    "from RankingLoss import MultipleNegativesSymmetricRankingLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7b12fdc-2526-42ec-a8f2-59d6dd7013a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_datadreamer_ta2(fold, output_folder, used_loss, luar_model_path='./rrivera1849'):\n",
    "    with DataDreamer(output_folder):\n",
    "        dataset = DataSource(\n",
    "            \"Train Data\",\n",
    "            data=train_data_generator,\n",
    "            total_num_rows=train_num_rows,\n",
    "        )\n",
    "        dev_dataset = DataSource(\n",
    "            \"Dev Data\",\n",
    "            data=dev_data_generator,\n",
    "            total_num_rows=dev_num_rows,\n",
    "        )\n",
    "    \n",
    "        trainer = get_luar_trainer()(\n",
    "            \"LUAR Trainer\",\n",
    "            model_name=luar_model_path,\n",
    "            peft_config=LoraConfig(),\n",
    "            trust_remote_code=True,\n",
    "            device='cuda:0',\n",
    "            dtype=\"bfloat16\",\n",
    "            force=True,\n",
    "        )\n",
    "\n",
    "        loss = SupConLoss if used_loss=='SupConLoss' else losses.MultipleNegativesSymmetricRankingLoss\n",
    "\n",
    "        if used_loss == 'SupConLoss':\n",
    "            trainer.train_with_labeled_pairs(\n",
    "                train_anchors=dataset.output[\"anchors\"],\n",
    "                train_others= dataset.output[\"others\"],\n",
    "                train_labels=dataset.output[\"labels\"],\n",
    "                validation_anchors=dev_dataset.output[\"anchors\"],\n",
    "                validation_others=dev_dataset.output[\"others\"],\n",
    "                validation_labels=dev_dataset.output[\"labels\"],\n",
    "                epochs=25,    \n",
    "                batch_size=128,\n",
    "                loss=loss,\n",
    "                learning_rate=0.0005,\n",
    "                early_stopping_threshold=0.001,\n",
    "                early_stopping_patience=5,\n",
    "                accelerator_config={\n",
    "                    \"dispatch_batches\": False,\n",
    "                },\n",
    "                callbacks=[EpochTrackerCallback()]\n",
    "            )\n",
    "        else:\n",
    "            trainer.train_with_positive_pairs(\n",
    "                train_anchors=dataset.output[\"anchors\"],\n",
    "                train_positives=dataset.output[\"positives\"],\n",
    "                validation_anchors=dev_dataset.output[\"anchors\"],\n",
    "                validation_positives=dev_dataset.output[\"positives\"],\n",
    "                epochs=25,    \n",
    "                batch_size=100,\n",
    "                loss=loss,\n",
    "                learning_rate=0.0005,\n",
    "                early_stopping_threshold=0.001,\n",
    "                early_stopping_patience=5,\n",
    "                accelerator_config={\n",
    "                    \"dispatch_batches\": False,\n",
    "                },\n",
    "                callbacks=[EpochTrackerCallback()]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d5eef-275d-4458-a919-5830fe79e46f",
   "metadata": {},
   "source": [
    "### Train original TA2 system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe4397e-c25b-4306-b335-177771705eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/mnt/swordfish-pool2/milad/datadreamer-ta2/'\n",
    "luar_model_path = '/mnt/swordfish-pool2/milad/rrivera1849'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10dd4486-104d-4ecf-852b-5f22e2991c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = \"/mnt/swordfish-pool2/milad/hiatus-data/train-test-dev split/{split}/Official Query-candidate format/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3450f7a9-d3e4-4f0b-8b0f-cf91bb5fe69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: cross_genre_all train None 575 4142\n",
      "Dataset Statistics: cross_genre_all dev None 79 635\n"
     ]
    }
   ],
   "source": [
    "train_num_rows, train_data_generator = get_data_generator(fold, \"cross_genre_all\", \"train\", split_percent=ast.literal_eval(\"None\"))\n",
    "dev_num_rows, dev_data_generator = get_data_generator(fold, \"cross_genre_all\", \"dev\", split_percent=ast.literal_eval(\"None\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc6306a9-e13e-4790-b379-1dc425b34707",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Initialized. 🚀 Dreaming to folder: /mnt/swordfish-pool2/milad/datadreamer-ta2//original_ta2_model\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'Train Data' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'Train Data' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'Dev Data' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'Dev Data' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Trainer 'LUAR Trainer' was previously run and saved, but was outdated. 😞\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Trainer 'LUAR Trainer' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Train Anchors' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Train Anchors' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Train Positives' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Train Positives' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Validation Anchors' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'Dev Data' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Validation Anchors' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Validation Positives' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Validation Positives' will run lazily. 🥱\n",
      "/home/ma4608/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [ 🤗 Accelerate] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Validation Anchors' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Validation Positives' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Eval Epoch: 0.0 -- {'loss': 2.9185125827789307, 'runtime': 2.0459, 'samples_per_second': 77.226, 'steps_per_second': 0.978}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 0.08333333333333333 -- {'loss': 2.9531, 'grad_norm': 0.59765625, 'learning_rate': 0.0004983333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 0.16666666666666666 -- {'loss': 3.125, 'grad_norm': 0.640625, 'learning_rate': 0.0004966666666666666}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 0.25 -- {'loss': 3.0938, 'grad_norm': 0.828125, 'learning_rate': 0.000495}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 0.3333333333333333 -- {'loss': 2.9062, 'grad_norm': 0.67578125, 'learning_rate': 0.0004933333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 0.4166666666666667 -- {'loss': 2.8438, 'grad_norm': 0.7421875, 'learning_rate': 0.0004916666666666666}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 0.5 -- {'loss': 2.7656, 'grad_norm': 0.71875, 'learning_rate': 0.00049}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 0.5833333333333334 -- {'loss': 2.5938, 'grad_norm': 0.6171875, 'learning_rate': 0.0004883333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 0.6666666666666666 -- {'loss': 2.4375, 'grad_norm': 0.77734375, 'learning_rate': 0.0004866666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 0.75 -- {'loss': 2.4531, 'grad_norm': 0.90234375, 'learning_rate': 0.00048499999999999997}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'Train Data' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 0.8333333333333334 -- {'loss': 2.3125, 'grad_norm': 0.71875, 'learning_rate': 0.00048333333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Train Anchors' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'LUAR Trainer / Tokenize Train Positives' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 0.9166666666666666 -- {'loss': 2.5312, 'grad_norm': 0.7890625, 'learning_rate': 0.0004816666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 1.0 -- {'loss': 2.0, 'grad_norm': 1.0625, 'learning_rate': 0.00048}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Eval Epoch: 1.0 -- {'loss': 2.438291072845459, 'runtime': 1.3157, 'samples_per_second': 120.092, 'steps_per_second': 1.52}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 1.0833333333333333 -- {'loss': 2.25, 'grad_norm': 0.9609375, 'learning_rate': 0.0004783333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 1.1666666666666667 -- {'loss': 2.2812, 'grad_norm': 1.0703125, 'learning_rate': 0.0004766666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 1.25 -- {'loss': 2.2812, 'grad_norm': 0.91015625, 'learning_rate': 0.000475}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 1.3333333333333333 -- {'loss': 2.2656, 'grad_norm': 0.90625, 'learning_rate': 0.00047333333333333336}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 1.4166666666666667 -- {'loss': 2.2344, 'grad_norm': 0.97265625, 'learning_rate': 0.0004716666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 1.5 -- {'loss': 2.25, 'grad_norm': 1.0859375, 'learning_rate': 0.00047}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 1.5833333333333335 -- {'loss': 2.2969, 'grad_norm': 0.95703125, 'learning_rate': 0.00046833333333333335}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 1.6666666666666665 -- {'loss': 2.3125, 'grad_norm': 1.0390625, 'learning_rate': 0.00046666666666666666}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 1.75 -- {'loss': 2.4219, 'grad_norm': 1.1171875, 'learning_rate': 0.000465}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 1.8333333333333335 -- {'loss': 1.7109, 'grad_norm': 1.2265625, 'learning_rate': 0.00046333333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 1.9166666666666665 -- {'loss': 2.0938, 'grad_norm': 1.3046875, 'learning_rate': 0.0004616666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 2.0 -- {'loss': 1.7031, 'grad_norm': 1.359375, 'learning_rate': 0.00046}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Eval Epoch: 2.0 -- {'loss': 2.275019884109497, 'runtime': 1.3777, 'samples_per_second': 114.687, 'steps_per_second': 1.452}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 2.0833333333333335 -- {'loss': 1.7656, 'grad_norm': 0.94921875, 'learning_rate': 0.0004583333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 2.1666666666666665 -- {'loss': 2.1406, 'grad_norm': 1.078125, 'learning_rate': 0.0004566666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 2.25 -- {'loss': 2.125, 'grad_norm': 1.0703125, 'learning_rate': 0.000455}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 2.3333333333333335 -- {'loss': 2.2031, 'grad_norm': 1.15625, 'learning_rate': 0.0004533333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 2.4166666666666665 -- {'loss': 2.1875, 'grad_norm': 1.078125, 'learning_rate': 0.0004516666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 2.5 -- {'loss': 2.125, 'grad_norm': 1.0546875, 'learning_rate': 0.00045000000000000004}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 2.5833333333333335 -- {'loss': 2.3438, 'grad_norm': 1.0546875, 'learning_rate': 0.0004483333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 2.6666666666666665 -- {'loss': 2.25, 'grad_norm': 0.98046875, 'learning_rate': 0.00044666666666666666}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 2.75 -- {'loss': 2.2031, 'grad_norm': 1.109375, 'learning_rate': 0.00044500000000000003}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 2.8333333333333335 -- {'loss': 2.0938, 'grad_norm': 1.03125, 'learning_rate': 0.00044333333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 2.9166666666666665 -- {'loss': 2.2812, 'grad_norm': 0.9921875, 'learning_rate': 0.00044166666666666665}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 3.0 -- {'loss': 1.4688, 'grad_norm': 1.171875, 'learning_rate': 0.00044}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Eval Epoch: 3.0 -- {'loss': 2.1799840927124023, 'runtime': 1.3759, 'samples_per_second': 114.836, 'steps_per_second': 1.454}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 3.0833333333333335 -- {'loss': 2.0625, 'grad_norm': 1.0546875, 'learning_rate': 0.0004383333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 3.1666666666666665 -- {'loss': 2.0469, 'grad_norm': 0.90234375, 'learning_rate': 0.00043666666666666664}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 3.25 -- {'loss': 1.9375, 'grad_norm': 0.96484375, 'learning_rate': 0.000435}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 3.3333333333333335 -- {'loss': 2.4375, 'grad_norm': 1.015625, 'learning_rate': 0.00043333333333333337}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 3.4166666666666665 -- {'loss': 2.0156, 'grad_norm': 1.0078125, 'learning_rate': 0.0004316666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 3.5 -- {'loss': 1.7812, 'grad_norm': 0.9765625, 'learning_rate': 0.00043}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 3.5833333333333335 -- {'loss': 1.9531, 'grad_norm': 0.98046875, 'learning_rate': 0.00042833333333333335}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 3.6666666666666665 -- {'loss': 1.8125, 'grad_norm': 1.015625, 'learning_rate': 0.0004266666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 3.75 -- {'loss': 2.0938, 'grad_norm': 0.99609375, 'learning_rate': 0.000425}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 3.8333333333333335 -- {'loss': 2.0625, 'grad_norm': 0.984375, 'learning_rate': 0.00042333333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 3.9166666666666665 -- {'loss': 1.9844, 'grad_norm': 1.0703125, 'learning_rate': 0.0004216666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 4.0 -- {'loss': 1.2422, 'grad_norm': 1.25, 'learning_rate': 0.00042}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Eval Epoch: 4.0 -- {'loss': 2.125791072845459, 'runtime': 1.3852, 'samples_per_second': 114.064, 'steps_per_second': 1.444}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 4.083333333333333 -- {'loss': 1.7188, 'grad_norm': 1.1484375, 'learning_rate': 0.00041833333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 4.166666666666667 -- {'loss': 1.6719, 'grad_norm': 1.0, 'learning_rate': 0.0004166666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 4.25 -- {'loss': 1.875, 'grad_norm': 1.171875, 'learning_rate': 0.000415}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 4.333333333333333 -- {'loss': 2.1562, 'grad_norm': 1.25, 'learning_rate': 0.0004133333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 4.416666666666667 -- {'loss': 1.9062, 'grad_norm': 1.1953125, 'learning_rate': 0.0004116666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 4.5 -- {'loss': 2.0312, 'grad_norm': 1.25, 'learning_rate': 0.00041}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 4.583333333333333 -- {'loss': 1.6094, 'grad_norm': 1.234375, 'learning_rate': 0.00040833333333333336}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 4.666666666666667 -- {'loss': 1.9219, 'grad_norm': 1.1328125, 'learning_rate': 0.00040666666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 4.75 -- {'loss': 1.875, 'grad_norm': 1.2109375, 'learning_rate': 0.00040500000000000003}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 4.833333333333333 -- {'loss': 1.7031, 'grad_norm': 1.109375, 'learning_rate': 0.00040333333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 4.916666666666667 -- {'loss': 1.9531, 'grad_norm': 1.265625, 'learning_rate': 0.00040166666666666665}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 5.0 -- {'loss': 1.8281, 'grad_norm': 1.6875, 'learning_rate': 0.0004}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Eval Epoch: 5.0 -- {'loss': 2.0919699668884277, 'runtime': 1.3798, 'samples_per_second': 114.507, 'steps_per_second': 1.449}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 5.083333333333333 -- {'loss': 1.7969, 'grad_norm': 1.21875, 'learning_rate': 0.00039833333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 5.166666666666667 -- {'loss': 1.8359, 'grad_norm': 1.3203125, 'learning_rate': 0.0003966666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 5.25 -- {'loss': 2.0625, 'grad_norm': 1.234375, 'learning_rate': 0.000395}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 5.333333333333333 -- {'loss': 1.9375, 'grad_norm': 1.2734375, 'learning_rate': 0.0003933333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 5.416666666666667 -- {'loss': 1.5312, 'grad_norm': 1.171875, 'learning_rate': 0.0003916666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 5.5 -- {'loss': 1.6406, 'grad_norm': 1.234375, 'learning_rate': 0.00039000000000000005}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 5.583333333333333 -- {'loss': 1.6953, 'grad_norm': 1.1796875, 'learning_rate': 0.0003883333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 5.666666666666667 -- {'loss': 2.1562, 'grad_norm': 1.3828125, 'learning_rate': 0.00038666666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 5.75 -- {'loss': 1.7188, 'grad_norm': 1.2265625, 'learning_rate': 0.00038500000000000003}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 5.833333333333333 -- {'loss': 1.7188, 'grad_norm': 1.28125, 'learning_rate': 0.00038333333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 5.916666666666667 -- {'loss': 2.25, 'grad_norm': 1.53125, 'learning_rate': 0.00038166666666666666}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 6.0 -- {'loss': 1.3594, 'grad_norm': 1.6484375, 'learning_rate': 0.00038}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Eval Epoch: 6.0 -- {'loss': 2.0294699668884277, 'runtime': 1.3716, 'samples_per_second': 115.195, 'steps_per_second': 1.458}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 6.083333333333333 -- {'loss': 2.0156, 'grad_norm': 1.2734375, 'learning_rate': 0.0003783333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 6.166666666666667 -- {'loss': 1.875, 'grad_norm': 1.3515625, 'learning_rate': 0.00037666666666666664}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 6.25 -- {'loss': 1.7969, 'grad_norm': 1.3671875, 'learning_rate': 0.000375}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 6.333333333333333 -- {'loss': 1.7656, 'grad_norm': 1.359375, 'learning_rate': 0.0003733333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 6.416666666666667 -- {'loss': 1.5938, 'grad_norm': 1.3515625, 'learning_rate': 0.00037166666666666663}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 6.5 -- {'loss': 1.6094, 'grad_norm': 1.4765625, 'learning_rate': 0.00037}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 6.583333333333333 -- {'loss': 1.9922, 'grad_norm': 1.5625, 'learning_rate': 0.00036833333333333336}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 6.666666666666667 -- {'loss': 1.8984, 'grad_norm': 1.4921875, 'learning_rate': 0.00036666666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 6.75 -- {'loss': 1.9531, 'grad_norm': 1.5703125, 'learning_rate': 0.000365}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 6.833333333333333 -- {'loss': 1.7656, 'grad_norm': 1.5546875, 'learning_rate': 0.00036333333333333335}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 6.916666666666667 -- {'loss': 1.8203, 'grad_norm': 1.484375, 'learning_rate': 0.0003616666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 7.0 -- {'loss': 1.1328, 'grad_norm': 1.6328125, 'learning_rate': 0.00035999999999999997}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Eval Epoch: 7.0 -- {'loss': 2.0167129039764404, 'runtime': 1.3357, 'samples_per_second': 118.289, 'steps_per_second': 1.497}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 7.083333333333333 -- {'loss': 1.7109, 'grad_norm': 1.3828125, 'learning_rate': 0.00035833333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 7.166666666666667 -- {'loss': 1.9453, 'grad_norm': 1.578125, 'learning_rate': 0.0003566666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 7.25 -- {'loss': 2.0469, 'grad_norm': 1.5546875, 'learning_rate': 0.000355}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 7.333333333333333 -- {'loss': 1.8516, 'grad_norm': 1.578125, 'learning_rate': 0.0003533333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 7.416666666666667 -- {'loss': 1.6094, 'grad_norm': 1.4765625, 'learning_rate': 0.0003516666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 7.5 -- {'loss': 1.7031, 'grad_norm': 1.5234375, 'learning_rate': 0.00035}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 7.583333333333333 -- {'loss': 2.0625, 'grad_norm': 1.578125, 'learning_rate': 0.00034833333333333336}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 7.666666666666667 -- {'loss': 1.7344, 'grad_norm': 1.4375, 'learning_rate': 0.00034666666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 7.75 -- {'loss': 1.7266, 'grad_norm': 1.53125, 'learning_rate': 0.000345}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 7.833333333333333 -- {'loss': 2.0, 'grad_norm': 1.6953125, 'learning_rate': 0.00034333333333333335}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 7.916666666666667 -- {'loss': 1.7188, 'grad_norm': 1.4296875, 'learning_rate': 0.00034166666666666666}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 8.0 -- {'loss': 1.2656, 'grad_norm': 1.953125, 'learning_rate': 0.00034}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Eval Epoch: 8.0 -- {'loss': 1.9982199668884277, 'runtime': 1.3942, 'samples_per_second': 113.328, 'steps_per_second': 1.435}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 8.083333333333334 -- {'loss': 2.0312, 'grad_norm': 1.625, 'learning_rate': 0.00033833333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 8.166666666666666 -- {'loss': 1.6016, 'grad_norm': 1.484375, 'learning_rate': 0.0003366666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 8.25 -- {'loss': 1.8438, 'grad_norm': 1.5234375, 'learning_rate': 0.000335}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 8.333333333333334 -- {'loss': 1.5781, 'grad_norm': 1.390625, 'learning_rate': 0.0003333333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 8.416666666666666 -- {'loss': 1.6953, 'grad_norm': 1.4296875, 'learning_rate': 0.0003316666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 8.5 -- {'loss': 1.6875, 'grad_norm': 1.40625, 'learning_rate': 0.00033}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 8.583333333333334 -- {'loss': 1.7344, 'grad_norm': 1.5078125, 'learning_rate': 0.0003283333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 8.666666666666666 -- {'loss': 1.7578, 'grad_norm': 1.6015625, 'learning_rate': 0.0003266666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 8.75 -- {'loss': 1.7266, 'grad_norm': 1.4921875, 'learning_rate': 0.00032500000000000004}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 8.833333333333334 -- {'loss': 1.75, 'grad_norm': 1.5703125, 'learning_rate': 0.0003233333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 8.916666666666666 -- {'loss': 1.6875, 'grad_norm': 1.5546875, 'learning_rate': 0.00032166666666666666}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 9.0 -- {'loss': 1.125, 'grad_norm': 1.7890625, 'learning_rate': 0.00032}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Eval Epoch: 9.0 -- {'loss': 1.961234211921692, 'runtime': 1.4405, 'samples_per_second': 109.681, 'steps_per_second': 1.388}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 9.083333333333334 -- {'loss': 1.5625, 'grad_norm': 1.5625, 'learning_rate': 0.00031833333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 9.166666666666666 -- {'loss': 1.8281, 'grad_norm': 1.6953125, 'learning_rate': 0.00031666666666666665}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 9.25 -- {'loss': 1.4688, 'grad_norm': 1.3984375, 'learning_rate': 0.000315}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 9.333333333333334 -- {'loss': 1.5938, 'grad_norm': 1.6015625, 'learning_rate': 0.0003133333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 9.416666666666666 -- {'loss': 1.6562, 'grad_norm': 1.5546875, 'learning_rate': 0.00031166666666666663}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 9.5 -- {'loss': 1.8438, 'grad_norm': 1.6640625, 'learning_rate': 0.00031}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 9.583333333333334 -- {'loss': 1.7656, 'grad_norm': 1.5390625, 'learning_rate': 0.00030833333333333337}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 9.666666666666666 -- {'loss': 1.6562, 'grad_norm': 1.4921875, 'learning_rate': 0.0003066666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 9.75 -- {'loss': 1.3594, 'grad_norm': 1.40625, 'learning_rate': 0.000305}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 9.833333333333334 -- {'loss': 1.375, 'grad_norm': 1.65625, 'learning_rate': 0.00030333333333333335}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 9.916666666666666 -- {'loss': 1.6016, 'grad_norm': 1.5703125, 'learning_rate': 0.0003016666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 10.0 -- {'loss': 1.2188, 'grad_norm': 1.953125, 'learning_rate': 0.0003}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Eval Epoch: 10.0 -- {'loss': 1.961234211921692, 'runtime': 1.3961, 'samples_per_second': 113.172, 'steps_per_second': 1.433}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 10.083333333333334 -- {'loss': 1.7031, 'grad_norm': 1.5703125, 'learning_rate': 0.00029833333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 10.166666666666666 -- {'loss': 1.6719, 'grad_norm': 1.734375, 'learning_rate': 0.0002966666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 10.25 -- {'loss': 1.8125, 'grad_norm': 1.7578125, 'learning_rate': 0.000295}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 10.333333333333334 -- {'loss': 1.6719, 'grad_norm': 1.84375, 'learning_rate': 0.0002933333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 10.416666666666666 -- {'loss': 1.7031, 'grad_norm': 1.78125, 'learning_rate': 0.0002916666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 10.5 -- {'loss': 1.5547, 'grad_norm': 1.671875, 'learning_rate': 0.00029}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 10.583333333333334 -- {'loss': 1.625, 'grad_norm': 1.796875, 'learning_rate': 0.0002883333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 10.666666666666666 -- {'loss': 1.5312, 'grad_norm': 1.6875, 'learning_rate': 0.0002866666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 10.75 -- {'loss': 1.2656, 'grad_norm': 1.5703125, 'learning_rate': 0.000285}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 10.833333333333334 -- {'loss': 1.4609, 'grad_norm': 1.5625, 'learning_rate': 0.00028333333333333335}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 10.916666666666666 -- {'loss': 1.4375, 'grad_norm': 1.6875, 'learning_rate': 0.00028166666666666666}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 11.0 -- {'loss': 1.4531, 'grad_norm': 2.53125, 'learning_rate': 0.00028000000000000003}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Eval Epoch: 11.0 -- {'loss': 1.9810127019882202, 'runtime': 1.4174, 'samples_per_second': 111.473, 'steps_per_second': 1.411}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 11.083333333333334 -- {'loss': 1.6094, 'grad_norm': 1.8828125, 'learning_rate': 0.00027833333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 11.166666666666666 -- {'loss': 1.4141, 'grad_norm': 1.6796875, 'learning_rate': 0.00027666666666666665}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 11.25 -- {'loss': 1.4766, 'grad_norm': 1.75, 'learning_rate': 0.000275}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 11.333333333333334 -- {'loss': 1.7656, 'grad_norm': 1.7890625, 'learning_rate': 0.00027333333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 11.416666666666666 -- {'loss': 1.6328, 'grad_norm': 1.859375, 'learning_rate': 0.0002716666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 11.5 -- {'loss': 1.6328, 'grad_norm': 1.7578125, 'learning_rate': 0.00027}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 11.583333333333334 -- {'loss': 1.6172, 'grad_norm': 1.8203125, 'learning_rate': 0.0002683333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 11.666666666666666 -- {'loss': 1.4375, 'grad_norm': 1.8203125, 'learning_rate': 0.0002666666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 11.75 -- {'loss': 1.3594, 'grad_norm': 1.796875, 'learning_rate': 0.00026500000000000004}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 11.833333333333334 -- {'loss': 1.4688, 'grad_norm': 1.6875, 'learning_rate': 0.0002633333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 11.916666666666666 -- {'loss': 1.5938, 'grad_norm': 1.7734375, 'learning_rate': 0.00026166666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 12.0 -- {'loss': 1.0938, 'grad_norm': 2.375, 'learning_rate': 0.00026000000000000003}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Eval Epoch: 12.0 -- {'loss': 1.9794304370880127, 'runtime': 1.4071, 'samples_per_second': 112.289, 'steps_per_second': 1.421}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 12.083333333333334 -- {'loss': 1.6719, 'grad_norm': 1.9453125, 'learning_rate': 0.00025833333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 12.166666666666666 -- {'loss': 1.5859, 'grad_norm': 1.8046875, 'learning_rate': 0.00025666666666666665}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 12.25 -- {'loss': 1.7422, 'grad_norm': 1.90625, 'learning_rate': 0.000255}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 12.333333333333334 -- {'loss': 1.3828, 'grad_norm': 1.71875, 'learning_rate': 0.0002533333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 12.416666666666666 -- {'loss': 1.6016, 'grad_norm': 1.71875, 'learning_rate': 0.00025166666666666664}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 12.5 -- {'loss': 1.5, 'grad_norm': 1.8984375, 'learning_rate': 0.00025}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 12.583333333333334 -- {'loss': 1.8125, 'grad_norm': 1.84375, 'learning_rate': 0.0002483333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 12.666666666666666 -- {'loss': 1.6328, 'grad_norm': 1.7734375, 'learning_rate': 0.0002466666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 12.75 -- {'loss': 1.9141, 'grad_norm': 2.0, 'learning_rate': 0.000245}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 12.833333333333334 -- {'loss': 1.2031, 'grad_norm': 1.625, 'learning_rate': 0.00024333333333333336}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 12.916666666666666 -- {'loss': 1.8047, 'grad_norm': 2.046875, 'learning_rate': 0.00024166666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 13.0 -- {'loss': 1.1094, 'grad_norm': 2.328125, 'learning_rate': 0.00024}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Eval Epoch: 13.0 -- {'loss': 1.9724090099334717, 'runtime': 1.3874, 'samples_per_second': 113.886, 'steps_per_second': 1.442}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 13.083333333333334 -- {'loss': 1.3984, 'grad_norm': 1.6796875, 'learning_rate': 0.00023833333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 13.166666666666666 -- {'loss': 1.6094, 'grad_norm': 1.734375, 'learning_rate': 0.00023666666666666668}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 13.25 -- {'loss': 1.5938, 'grad_norm': 1.953125, 'learning_rate': 0.000235}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 13.333333333333334 -- {'loss': 1.2969, 'grad_norm': 1.671875, 'learning_rate': 0.00023333333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 13.416666666666666 -- {'loss': 1.2812, 'grad_norm': 1.5859375, 'learning_rate': 0.00023166666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 13.5 -- {'loss': 1.6797, 'grad_norm': 1.8984375, 'learning_rate': 0.00023}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 13.583333333333334 -- {'loss': 1.4375, 'grad_norm': 1.828125, 'learning_rate': 0.00022833333333333334}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 13.666666666666666 -- {'loss': 1.7344, 'grad_norm': 1.921875, 'learning_rate': 0.00022666666666666666}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 13.75 -- {'loss': 1.4844, 'grad_norm': 1.8671875, 'learning_rate': 0.00022500000000000002}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 13.833333333333334 -- {'loss': 1.6484, 'grad_norm': 1.859375, 'learning_rate': 0.00022333333333333333}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 13.916666666666666 -- {'loss': 1.4844, 'grad_norm': 1.921875, 'learning_rate': 0.00022166666666666667}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 14.0 -- {'loss': 0.7891, 'grad_norm': 2.09375, 'learning_rate': 0.00022}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Eval Epoch: 14.0 -- {'loss': 1.9810127019882202, 'runtime': 1.4116, 'samples_per_second': 111.931, 'steps_per_second': 1.417}\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] [LUAR Trainer (/mnt/swordfish-pool2/milad/rrivera1849)] Train Epoch: 14.0 -- {'runtime': 203.247, 'samples_per_second': 141.454, 'steps_per_second': 1.476, 'loss': 1.8203590029761905}\n",
      "/home/ma4608/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Trainer 'LUAR Trainer' finished and is saved to disk. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Done. ✨ Results in folder: /mnt/swordfish-pool2/milad/datadreamer-ta2//original_ta2_model\n"
     ]
    }
   ],
   "source": [
    "train_datadreamer_ta2(fold, output_path + '/original_ta2_model', 'MultipleNegativesSymmetricRankingLoss', luar_model_path=luar_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8914eba7-5840-472c-995c-0f9053075098",
   "metadata": {},
   "source": [
    "### Train original TA2 system with Supervised Contrastive Loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583024de-15ee-4963-8519-7ab58cb02a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num_rows, train_data_generator = get_data_generator_for_supervised_contrastive_learning(fold, \"cross_genre_all\", \"train\", split_percent=ast.literal_eval(\"None\"))\n",
    "dev_num_rows, dev_data_generator = get_data_generator_for_supervised_contrastive_learning(fold, \"cross_genre_all\", \"dev\", split_percent=ast.literal_eval(\"None\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f11b3-3b2f-4fda-8f6f-d513cb7db3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_datadreamer_ta2(fold, output_path + '/supcon_ta2_model', 'SupConLoss', luar_model_path=luar_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2cb8d-c013-4245-a9e8-41dfa071294a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
